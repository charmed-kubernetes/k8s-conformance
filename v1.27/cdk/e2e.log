  I0729 12:12:26.112988      18 e2e.go:117] Starting e2e run "b54c8ec5-3416-4458-8e59-a1ab1dbd9640" on Ginkgo node 1
  Jul 29 12:12:26.137: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1690632746 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jul 29 12:12:26.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:12:26.344: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jul 29 12:12:26.372: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jul 29 12:12:26.376: INFO: e2e test version: v1.27.4
  Jul 29 12:12:26.378: INFO: kube-apiserver version: v1.27.4
  Jul 29 12:12:26.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:12:26.381: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 07/29/23 12:12:26.773
  Jul 29 12:12:26.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 12:12:26.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:12:26.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:12:26.798
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 12:12:26.805
  STEP: Saw pod success @ 07/29/23 12:12:48.871
  Jul 29 12:12:48.874: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-a31ff8ef-ad88-4462-b27d-047d7f985182 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 12:12:48.893
  Jul 29 12:12:48.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3969" for this suite. @ 07/29/23 12:12:48.915
• [22.148 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 07/29/23 12:12:48.922
  Jul 29 12:12:48.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename secrets @ 07/29/23 12:12:48.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:12:48.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:12:48.946
  STEP: Creating secret with name secret-test-1cda9f2b-0473-4e2b-9c9b-3bde31a73ae0 @ 07/29/23 12:12:48.948
  STEP: Creating a pod to test consume secrets @ 07/29/23 12:12:48.956
  STEP: Saw pod success @ 07/29/23 12:12:52.975
  Jul 29 12:12:52.979: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-secrets-36db6ccf-ff54-4ef8-baa6-ec37a7a960b0 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 12:12:52.986
  Jul 29 12:12:53.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4997" for this suite. @ 07/29/23 12:12:53.006
• [4.090 seconds]
------------------------------
S
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 07/29/23 12:12:53.013
  Jul 29 12:12:53.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 12:12:53.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:12:53.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:12:53.037
  STEP: Creating configMap configmap-964/configmap-test-947744fc-b9be-48af-b9f2-cf5d9aa254fc @ 07/29/23 12:12:53.04
  STEP: Creating a pod to test consume configMaps @ 07/29/23 12:12:53.044
  STEP: Saw pod success @ 07/29/23 12:12:57.065
  Jul 29 12:12:57.069: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-configmaps-fa837eab-b337-4385-9323-af9b829446ff container env-test: <nil>
  STEP: delete the pod @ 07/29/23 12:12:57.077
  Jul 29 12:12:57.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-964" for this suite. @ 07/29/23 12:12:57.1
• [4.094 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 07/29/23 12:12:57.107
  Jul 29 12:12:57.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 12:12:57.107
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:12:57.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:12:57.127
  Jul 29 12:12:59.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1197" for this suite. @ 07/29/23 12:12:59.164
• [2.064 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 07/29/23 12:12:59.17
  Jul 29 12:12:59.170: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 12:12:59.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:12:59.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:12:59.195
  STEP: Updating Namespace "namespaces-2874" @ 07/29/23 12:12:59.21
  Jul 29 12:12:59.220: INFO: Namespace "namespaces-2874" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"b54c8ec5-3416-4458-8e59-a1ab1dbd9640", "kubernetes.io/metadata.name":"namespaces-2874", "namespaces-2874":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jul 29 12:12:59.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2874" for this suite. @ 07/29/23 12:12:59.225
• [0.065 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 07/29/23 12:12:59.236
  Jul 29 12:12:59.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 12:12:59.237
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:12:59.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:12:59.258
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 07/29/23 12:12:59.261
  STEP: When a replication controller with a matching selector is created @ 07/29/23 12:13:05.291
  STEP: Then the orphan pod is adopted @ 07/29/23 12:13:05.296
  Jul 29 12:13:06.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8457" for this suite. @ 07/29/23 12:13:06.306
• [7.077 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 07/29/23 12:13:06.313
  Jul 29 12:13:06.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 12:13:06.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:13:06.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:13:06.339
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 12:13:06.342
  STEP: Saw pod success @ 07/29/23 12:13:12.367
  Jul 29 12:13:12.370: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-0d99e6b6-68bb-4e4a-b7a6-a47a4a4c97fd container client-container: <nil>
  STEP: delete the pod @ 07/29/23 12:13:12.376
  Jul 29 12:13:12.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5074" for this suite. @ 07/29/23 12:13:12.4
• [6.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 07/29/23 12:13:12.415
  Jul 29 12:13:12.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 12:13:12.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:13:12.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:13:12.437
  STEP: create the container @ 07/29/23 12:13:12.44
  W0729 12:13:12.449811      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 07/29/23 12:13:12.45
  STEP: get the container status @ 07/29/23 12:13:15.466
  STEP: the container should be terminated @ 07/29/23 12:13:15.47
  STEP: the termination message should be set @ 07/29/23 12:13:15.47
  Jul 29 12:13:15.470: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/29/23 12:13:15.47
  Jul 29 12:13:15.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1610" for this suite. @ 07/29/23 12:13:15.488
• [3.078 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 07/29/23 12:13:15.495
  Jul 29 12:13:15.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename secrets @ 07/29/23 12:13:15.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:13:15.515
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:13:15.517
  Jul 29 12:13:15.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8244" for this suite. @ 07/29/23 12:13:15.56
• [0.071 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 07/29/23 12:13:15.567
  Jul 29 12:13:15.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename gc @ 07/29/23 12:13:15.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:13:15.589
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:13:15.592
  STEP: create the deployment @ 07/29/23 12:13:15.594
  W0729 12:13:15.600482      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/29/23 12:13:15.6
  STEP: delete the deployment @ 07/29/23 12:13:16.108
  STEP: wait for all rs to be garbage collected @ 07/29/23 12:13:16.114
  STEP: expected 0 rs, got 1 rs @ 07/29/23 12:13:16.121
  STEP: expected 0 pods, got 2 pods @ 07/29/23 12:13:16.126
  STEP: Gathering metrics @ 07/29/23 12:13:16.635
  W0729 12:13:16.639076      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 29 12:13:16.639: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 12:13:16.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-849" for this suite. @ 07/29/23 12:13:16.642
• [1.081 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 07/29/23 12:13:16.649
  Jul 29 12:13:16.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename crd-webhook @ 07/29/23 12:13:16.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:13:16.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:13:16.666
  STEP: Setting up server cert @ 07/29/23 12:13:16.669
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/29/23 12:13:17.149
  STEP: Deploying the custom resource conversion webhook pod @ 07/29/23 12:13:17.158
  STEP: Wait for the deployment to be ready @ 07/29/23 12:13:17.173
  Jul 29 12:13:17.179: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 12:13:19.191
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 12:13:19.201
  Jul 29 12:13:20.201: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul 29 12:13:20.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Creating a v1 custom resource @ 07/29/23 12:13:22.78
  STEP: v2 custom resource should be converted @ 07/29/23 12:13:22.785
  Jul 29 12:13:22.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-543" for this suite. @ 07/29/23 12:13:23.35
• [6.707 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 07/29/23 12:13:23.356
  Jul 29 12:13:23.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 12:13:23.357
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:13:23.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:13:23.382
  STEP: Creating ReplicationController "e2e-rc-4svq4" @ 07/29/23 12:13:23.385
  Jul 29 12:13:23.390: INFO: Get Replication Controller "e2e-rc-4svq4" to confirm replicas
  Jul 29 12:13:24.393: INFO: Get Replication Controller "e2e-rc-4svq4" to confirm replicas
  Jul 29 12:13:24.397: INFO: Found 1 replicas for "e2e-rc-4svq4" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-4svq4" @ 07/29/23 12:13:24.397
  STEP: Updating a scale subresource @ 07/29/23 12:13:24.4
  STEP: Verifying replicas where modified for replication controller "e2e-rc-4svq4" @ 07/29/23 12:13:24.407
  Jul 29 12:13:24.407: INFO: Get Replication Controller "e2e-rc-4svq4" to confirm replicas
  Jul 29 12:13:25.409: INFO: Get Replication Controller "e2e-rc-4svq4" to confirm replicas
  Jul 29 12:13:25.413: INFO: Found 2 replicas for "e2e-rc-4svq4" replication controller
  Jul 29 12:13:25.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9587" for this suite. @ 07/29/23 12:13:25.417
• [2.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 07/29/23 12:13:25.425
  Jul 29 12:13:25.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 12:13:25.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:13:25.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:13:25.453
  Jul 29 12:13:25.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:13:31.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8932" for this suite. @ 07/29/23 12:13:31.658
• [6.239 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 07/29/23 12:13:31.665
  Jul 29 12:13:31.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 12:13:31.666
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:13:31.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:13:31.686
  STEP: creating service nodeport-test with type=NodePort in namespace services-6855 @ 07/29/23 12:13:31.689
  STEP: creating replication controller nodeport-test in namespace services-6855 @ 07/29/23 12:13:31.707
  I0729 12:13:31.716610      18 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-6855, replica count: 2
  I0729 12:13:34.768395      18 runners.go:194] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0729 12:13:37.769156      18 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 12:13:37.769: INFO: Creating new exec pod
  Jul 29 12:13:40.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6855 exec execpodwrx2n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jul 29 12:13:40.927: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul 29 12:13:40.927: INFO: stdout: "nodeport-test-bkxcd"
  Jul 29 12:13:40.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6855 exec execpodwrx2n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.58 80'
  Jul 29 12:13:41.055: INFO: stderr: "+ nc -v -t -w 2 10.152.183.58 80\n+ echo hostName\nConnection to 10.152.183.58 80 port [tcp/http] succeeded!\n"
  Jul 29 12:13:41.055: INFO: stdout: "nodeport-test-bkxcd"
  Jul 29 12:13:41.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6855 exec execpodwrx2n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.19.67 32228'
  Jul 29 12:13:41.181: INFO: stderr: "+ nc -v -t -w 2 172.31.19.67 32228\n+ echo hostName\nConnection to 172.31.19.67 32228 port [tcp/*] succeeded!\n"
  Jul 29 12:13:41.181: INFO: stdout: "nodeport-test-bkxcd"
  Jul 29 12:13:41.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6855 exec execpodwrx2n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.5.66 32228'
  Jul 29 12:13:41.302: INFO: stderr: "+ nc -v -t -w 2 172.31.5.66 32228\n+ echo hostName\nConnection to 172.31.5.66 32228 port [tcp/*] succeeded!\n"
  Jul 29 12:13:41.302: INFO: stdout: ""
  Jul 29 12:13:42.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6855 exec execpodwrx2n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.5.66 32228'
  Jul 29 12:13:42.426: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.5.66 32228\nConnection to 172.31.5.66 32228 port [tcp/*] succeeded!\n"
  Jul 29 12:13:42.426: INFO: stdout: "nodeport-test-mgkh6"
  Jul 29 12:13:42.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6855" for this suite. @ 07/29/23 12:13:42.43
• [10.773 seconds]
------------------------------
SS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 07/29/23 12:13:42.438
  Jul 29 12:13:42.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename sysctl @ 07/29/23 12:13:42.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:13:42.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:13:42.458
  STEP: Creating a pod with one valid and two invalid sysctls @ 07/29/23 12:13:42.461
  Jul 29 12:13:42.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-1980" for this suite. @ 07/29/23 12:13:42.47
• [0.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 07/29/23 12:13:42.477
  Jul 29 12:13:42.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pods @ 07/29/23 12:13:42.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:13:42.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:13:42.499
  STEP: Create a pod @ 07/29/23 12:13:42.506
  STEP: patching /status @ 07/29/23 12:13:44.522
  Jul 29 12:13:44.531: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jul 29 12:13:44.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8509" for this suite. @ 07/29/23 12:13:44.535
• [2.066 seconds]
------------------------------
SS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 07/29/23 12:13:44.544
  Jul 29 12:13:44.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename controllerrevisions @ 07/29/23 12:13:44.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:13:44.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:13:44.565
  STEP: Creating DaemonSet "e2e-nglzf-daemon-set" @ 07/29/23 12:13:44.586
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 12:13:44.594
  Jul 29 12:13:44.597: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:13:44.597: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:13:44.600: INFO: Number of nodes with available pods controlled by daemonset e2e-nglzf-daemon-set: 0
  Jul 29 12:13:44.600: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  Jul 29 12:13:45.606: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:13:45.606: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:13:45.609: INFO: Number of nodes with available pods controlled by daemonset e2e-nglzf-daemon-set: 1
  Jul 29 12:13:45.609: INFO: Node ip-172-31-33-37 is running 0 daemon pod, expected 1
  Jul 29 12:13:46.604: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:13:46.605: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:13:46.607: INFO: Number of nodes with available pods controlled by daemonset e2e-nglzf-daemon-set: 2
  Jul 29 12:13:46.607: INFO: Node ip-172-31-5-66 is running 0 daemon pod, expected 1
  Jul 29 12:13:47.604: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:13:47.604: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:13:47.608: INFO: Number of nodes with available pods controlled by daemonset e2e-nglzf-daemon-set: 2
  Jul 29 12:13:47.608: INFO: Node ip-172-31-5-66 is running 0 daemon pod, expected 1
  Jul 29 12:13:48.604: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:13:48.604: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:13:48.607: INFO: Number of nodes with available pods controlled by daemonset e2e-nglzf-daemon-set: 3
  Jul 29 12:13:48.607: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-nglzf-daemon-set
  STEP: Confirm DaemonSet "e2e-nglzf-daemon-set" successfully created with "daemonset-name=e2e-nglzf-daemon-set" label @ 07/29/23 12:13:48.61
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-nglzf-daemon-set" @ 07/29/23 12:13:48.617
  Jul 29 12:13:48.621: INFO: Located ControllerRevision: "e2e-nglzf-daemon-set-84fbb4fdb4"
  STEP: Patching ControllerRevision "e2e-nglzf-daemon-set-84fbb4fdb4" @ 07/29/23 12:13:48.623
  Jul 29 12:13:48.630: INFO: e2e-nglzf-daemon-set-84fbb4fdb4 has been patched
  STEP: Create a new ControllerRevision @ 07/29/23 12:13:48.63
  Jul 29 12:13:48.636: INFO: Created ControllerRevision: e2e-nglzf-daemon-set-656d6f9d6b
  STEP: Confirm that there are two ControllerRevisions @ 07/29/23 12:13:48.636
  Jul 29 12:13:48.637: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 12:13:48.639: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-nglzf-daemon-set-84fbb4fdb4" @ 07/29/23 12:13:48.639
  STEP: Confirm that there is only one ControllerRevision @ 07/29/23 12:13:48.645
  Jul 29 12:13:48.646: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 12:13:48.648: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-nglzf-daemon-set-656d6f9d6b" @ 07/29/23 12:13:48.651
  Jul 29 12:13:48.660: INFO: e2e-nglzf-daemon-set-656d6f9d6b has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 07/29/23 12:13:48.66
  W0729 12:13:48.668857      18 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 07/29/23 12:13:48.668
  Jul 29 12:13:48.668: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 12:13:49.672: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 12:13:49.676: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-nglzf-daemon-set-656d6f9d6b=updated" @ 07/29/23 12:13:49.676
  STEP: Confirm that there is only one ControllerRevision @ 07/29/23 12:13:49.684
  Jul 29 12:13:49.684: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 12:13:49.687: INFO: Found 1 ControllerRevisions
  Jul 29 12:13:49.690: INFO: ControllerRevision "e2e-nglzf-daemon-set-bfcb5645f" has revision 3
  STEP: Deleting DaemonSet "e2e-nglzf-daemon-set" @ 07/29/23 12:13:49.693
  STEP: deleting DaemonSet.extensions e2e-nglzf-daemon-set in namespace controllerrevisions-2060, will wait for the garbage collector to delete the pods @ 07/29/23 12:13:49.693
  Jul 29 12:13:49.753: INFO: Deleting DaemonSet.extensions e2e-nglzf-daemon-set took: 5.136338ms
  Jul 29 12:13:49.853: INFO: Terminating DaemonSet.extensions e2e-nglzf-daemon-set pods took: 100.533289ms
  Jul 29 12:13:52.658: INFO: Number of nodes with available pods controlled by daemonset e2e-nglzf-daemon-set: 0
  Jul 29 12:13:52.658: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-nglzf-daemon-set
  Jul 29 12:13:52.664: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"3899"},"items":null}

  Jul 29 12:13:52.667: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"3899"},"items":null}

  Jul 29 12:13:52.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-2060" for this suite. @ 07/29/23 12:13:52.684
• [8.146 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 07/29/23 12:13:52.691
  Jul 29 12:13:52.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 12:13:52.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:13:52.708
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:13:52.711
  STEP: Creating a ResourceQuota with terminating scope @ 07/29/23 12:13:52.714
  STEP: Ensuring ResourceQuota status is calculated @ 07/29/23 12:13:52.72
  STEP: Creating a ResourceQuota with not terminating scope @ 07/29/23 12:13:54.725
  STEP: Ensuring ResourceQuota status is calculated @ 07/29/23 12:13:54.729
  STEP: Creating a long running pod @ 07/29/23 12:13:56.733
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 07/29/23 12:13:56.746
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 07/29/23 12:13:58.749
  STEP: Deleting the pod @ 07/29/23 12:14:00.753
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 12:14:00.767
  STEP: Creating a terminating pod @ 07/29/23 12:14:02.772
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 07/29/23 12:14:02.782
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 07/29/23 12:14:04.787
  STEP: Deleting the pod @ 07/29/23 12:14:06.792
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 12:14:06.803
  Jul 29 12:14:08.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2635" for this suite. @ 07/29/23 12:14:08.813
• [16.129 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 07/29/23 12:14:08.821
  Jul 29 12:14:08.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pod-network-test @ 07/29/23 12:14:08.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:14:08.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:14:08.843
  STEP: Performing setup for networking test in namespace pod-network-test-1016 @ 07/29/23 12:14:08.845
  STEP: creating a selector @ 07/29/23 12:14:08.846
  STEP: Creating the service pods in kubernetes @ 07/29/23 12:14:08.846
  Jul 29 12:14:08.846: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 07/29/23 12:14:30.958
  Jul 29 12:14:32.978: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 29 12:14:32.978: INFO: Breadth first check of 192.168.10.6 on host 172.31.19.67...
  Jul 29 12:14:32.981: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.81:9080/dial?request=hostname&protocol=udp&host=192.168.10.6&port=8081&tries=1'] Namespace:pod-network-test-1016 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:14:32.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:14:32.981: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:14:32.981: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1016/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.129.81%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.10.6%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 12:14:33.046: INFO: Waiting for responses: map[]
  Jul 29 12:14:33.046: INFO: reached 192.168.10.6 after 0/1 tries
  Jul 29 12:14:33.046: INFO: Breadth first check of 192.168.129.80 on host 172.31.33.37...
  Jul 29 12:14:33.050: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.81:9080/dial?request=hostname&protocol=udp&host=192.168.129.80&port=8081&tries=1'] Namespace:pod-network-test-1016 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:14:33.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:14:33.051: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:14:33.051: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1016/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.129.81%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.129.80%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 12:14:33.113: INFO: Waiting for responses: map[]
  Jul 29 12:14:33.113: INFO: reached 192.168.129.80 after 0/1 tries
  Jul 29 12:14:33.113: INFO: Breadth first check of 192.168.8.137 on host 172.31.5.66...
  Jul 29 12:14:33.116: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.81:9080/dial?request=hostname&protocol=udp&host=192.168.8.137&port=8081&tries=1'] Namespace:pod-network-test-1016 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:14:33.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:14:33.117: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:14:33.117: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-1016/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.129.81%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.8.137%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 12:14:33.185: INFO: Waiting for responses: map[]
  Jul 29 12:14:33.185: INFO: reached 192.168.8.137 after 0/1 tries
  Jul 29 12:14:33.185: INFO: Going to retry 0 out of 3 pods....
  Jul 29 12:14:33.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1016" for this suite. @ 07/29/23 12:14:33.191
• [24.380 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 07/29/23 12:14:33.202
  Jul 29 12:14:33.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/29/23 12:14:33.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:14:33.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:14:33.228
  STEP: create the container to handle the HTTPGet hook request. @ 07/29/23 12:14:33.235
  STEP: create the pod with lifecycle hook @ 07/29/23 12:14:35.257
  STEP: delete the pod with lifecycle hook @ 07/29/23 12:14:37.276
  STEP: check prestop hook @ 07/29/23 12:14:39.293
  Jul 29 12:14:39.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6801" for this suite. @ 07/29/23 12:14:39.317
• [6.122 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 07/29/23 12:14:39.324
  Jul 29 12:14:39.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 12:14:39.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:14:39.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:14:39.349
  STEP: Creating configMap with name configmap-test-volume-map-79b76524-c066-4d47-b39a-9fbff203cdf3 @ 07/29/23 12:14:39.352
  STEP: Creating a pod to test consume configMaps @ 07/29/23 12:14:39.356
  STEP: Saw pod success @ 07/29/23 12:14:43.377
  Jul 29 12:14:43.381: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-configmaps-33cfa821-2036-4dd6-b2fc-df56c93df78c container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 12:14:43.395
  Jul 29 12:14:43.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3265" for this suite. @ 07/29/23 12:14:43.417
• [4.099 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 07/29/23 12:14:43.423
  Jul 29 12:14:43.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 12:14:43.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:14:43.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:14:43.445
  STEP: Creating a cronjob @ 07/29/23 12:14:43.448
  STEP: creating @ 07/29/23 12:14:43.448
  STEP: getting @ 07/29/23 12:14:43.454
  STEP: listing @ 07/29/23 12:14:43.458
  STEP: watching @ 07/29/23 12:14:43.461
  Jul 29 12:14:43.461: INFO: starting watch
  STEP: cluster-wide listing @ 07/29/23 12:14:43.462
  STEP: cluster-wide watching @ 07/29/23 12:14:43.464
  Jul 29 12:14:43.464: INFO: starting watch
  STEP: patching @ 07/29/23 12:14:43.465
  STEP: updating @ 07/29/23 12:14:43.472
  Jul 29 12:14:43.481: INFO: waiting for watch events with expected annotations
  Jul 29 12:14:43.481: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/29/23 12:14:43.481
  STEP: updating /status @ 07/29/23 12:14:43.488
  STEP: get /status @ 07/29/23 12:14:43.494
  STEP: deleting @ 07/29/23 12:14:43.498
  STEP: deleting a collection @ 07/29/23 12:14:43.512
  Jul 29 12:14:43.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2693" for this suite. @ 07/29/23 12:14:43.526
• [0.113 seconds]
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 07/29/23 12:14:43.537
  Jul 29 12:14:43.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 12:14:43.537
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:14:43.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:14:43.558
  STEP: Deleting RuntimeClass runtimeclass-2696-delete-me @ 07/29/23 12:14:43.565
  STEP: Waiting for the RuntimeClass to disappear @ 07/29/23 12:14:43.571
  Jul 29 12:14:43.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2696" for this suite. @ 07/29/23 12:14:43.586
• [0.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 07/29/23 12:14:43.594
  Jul 29 12:14:43.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename dns @ 07/29/23 12:14:43.595
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:14:43.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:14:43.616
  STEP: Creating a test headless service @ 07/29/23 12:14:43.62
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-935 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-935;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-935 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-935;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-935.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-935.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-935.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-935.svc;check="$$(dig +notcp +noall +answer +search 34.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.34_tcp@PTR;sleep 1; done
   @ 07/29/23 12:14:43.638
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-935 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-935;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-935 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-935;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-935.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-935.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-935.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-935.svc;check="$$(dig +notcp +noall +answer +search 34.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.34_tcp@PTR;sleep 1; done
   @ 07/29/23 12:14:43.638
  STEP: creating a pod to probe DNS @ 07/29/23 12:14:43.638
  STEP: submitting the pod to kubernetes @ 07/29/23 12:14:43.638
  STEP: retrieving the pod @ 07/29/23 12:14:51.74
  STEP: looking for the results for each expected name from probers @ 07/29/23 12:14:51.744
  Jul 29 12:14:51.750: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.754: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.757: INFO: Unable to read wheezy_udp@dns-test-service.dns-935 from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.763: INFO: Unable to read wheezy_tcp@dns-test-service.dns-935 from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.766: INFO: Unable to read wheezy_udp@dns-test-service.dns-935.svc from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.770: INFO: Unable to read wheezy_tcp@dns-test-service.dns-935.svc from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.775: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-935.svc from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.778: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-935.svc from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.799: INFO: Unable to read jessie_udp@dns-test-service from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.802: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.806: INFO: Unable to read jessie_udp@dns-test-service.dns-935 from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.811: INFO: Unable to read jessie_tcp@dns-test-service.dns-935 from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.814: INFO: Unable to read jessie_udp@dns-test-service.dns-935.svc from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.818: INFO: Unable to read jessie_tcp@dns-test-service.dns-935.svc from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.823: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-935.svc from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.826: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-935.svc from pod dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00: the server could not find the requested resource (get pods dns-test-0bd9fba3-87c0-411e-8831-826f920eda00)
  Jul 29 12:14:51.842: INFO: Lookups using dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-935 wheezy_tcp@dns-test-service.dns-935 wheezy_udp@dns-test-service.dns-935.svc wheezy_tcp@dns-test-service.dns-935.svc wheezy_udp@_http._tcp.dns-test-service.dns-935.svc wheezy_tcp@_http._tcp.dns-test-service.dns-935.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-935 jessie_tcp@dns-test-service.dns-935 jessie_udp@dns-test-service.dns-935.svc jessie_tcp@dns-test-service.dns-935.svc jessie_udp@_http._tcp.dns-test-service.dns-935.svc jessie_tcp@_http._tcp.dns-test-service.dns-935.svc]

  Jul 29 12:14:56.936: INFO: DNS probes using dns-935/dns-test-0bd9fba3-87c0-411e-8831-826f920eda00 succeeded

  Jul 29 12:14:56.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 12:14:56.939
  STEP: deleting the test service @ 07/29/23 12:14:56.953
  STEP: deleting the test headless service @ 07/29/23 12:14:56.972
  STEP: Destroying namespace "dns-935" for this suite. @ 07/29/23 12:14:56.983
• [13.400 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 07/29/23 12:14:56.996
  Jul 29 12:14:56.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 12:14:56.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:14:57.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:14:57.018
  STEP: Create a Replicaset @ 07/29/23 12:14:57.023
  STEP: Verify that the required pods have come up. @ 07/29/23 12:14:57.028
  Jul 29 12:14:57.031: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul 29 12:15:02.035: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 12:15:02.035
  STEP: Getting /status @ 07/29/23 12:15:06.047
  Jul 29 12:15:06.051: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 07/29/23 12:15:06.051
  Jul 29 12:15:06.060: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 07/29/23 12:15:06.06
  Jul 29 12:15:06.062: INFO: Observed &ReplicaSet event: ADDED
  Jul 29 12:15:06.062: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 12:15:06.063: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 12:15:06.063: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 12:15:06.063: INFO: Found replicaset test-rs in namespace replicaset-5069 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 29 12:15:06.063: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 07/29/23 12:15:06.063
  Jul 29 12:15:06.063: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 29 12:15:06.071: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 07/29/23 12:15:06.071
  Jul 29 12:15:06.072: INFO: Observed &ReplicaSet event: ADDED
  Jul 29 12:15:06.072: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 12:15:06.072: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 12:15:06.073: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 12:15:06.073: INFO: Observed replicaset test-rs in namespace replicaset-5069 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 12:15:06.073: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 12:15:06.073: INFO: Found replicaset test-rs in namespace replicaset-5069 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jul 29 12:15:06.073: INFO: Replicaset test-rs has a patched status
  Jul 29 12:15:06.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5069" for this suite. @ 07/29/23 12:15:06.078
• [9.090 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 07/29/23 12:15:06.086
  Jul 29 12:15:06.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename events @ 07/29/23 12:15:06.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:15:06.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:15:06.109
  STEP: creating a test event @ 07/29/23 12:15:06.113
  STEP: listing events in all namespaces @ 07/29/23 12:15:06.123
  STEP: listing events in test namespace @ 07/29/23 12:15:06.129
  STEP: listing events with field selection filtering on source @ 07/29/23 12:15:06.132
  STEP: listing events with field selection filtering on reportingController @ 07/29/23 12:15:06.135
  STEP: getting the test event @ 07/29/23 12:15:06.139
  STEP: patching the test event @ 07/29/23 12:15:06.147
  STEP: getting the test event @ 07/29/23 12:15:06.156
  STEP: updating the test event @ 07/29/23 12:15:06.159
  STEP: getting the test event @ 07/29/23 12:15:06.167
  STEP: deleting the test event @ 07/29/23 12:15:06.17
  STEP: listing events in all namespaces @ 07/29/23 12:15:06.177
  STEP: listing events in test namespace @ 07/29/23 12:15:06.183
  Jul 29 12:15:06.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8934" for this suite. @ 07/29/23 12:15:06.19
• [0.110 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 07/29/23 12:15:06.196
  Jul 29 12:15:06.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 12:15:06.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:15:06.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:15:06.222
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-3376 @ 07/29/23 12:15:06.226
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/29/23 12:15:06.245
  STEP: creating service externalsvc in namespace services-3376 @ 07/29/23 12:15:06.246
  STEP: creating replication controller externalsvc in namespace services-3376 @ 07/29/23 12:15:06.256
  I0729 12:15:06.265619      18 runners.go:194] Created replication controller with name: externalsvc, namespace: services-3376, replica count: 2
  I0729 12:15:09.316990      18 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 07/29/23 12:15:09.321
  Jul 29 12:15:09.339: INFO: Creating new exec pod
  Jul 29 12:15:11.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-3376 exec execpodd4dvb -- /bin/sh -x -c nslookup nodeport-service.services-3376.svc.cluster.local'
  Jul 29 12:15:11.567: INFO: stderr: "+ nslookup nodeport-service.services-3376.svc.cluster.local\n"
  Jul 29 12:15:11.567: INFO: stdout: "Server:\t\t10.152.183.124\nAddress:\t10.152.183.124#53\n\nnodeport-service.services-3376.svc.cluster.local\tcanonical name = externalsvc.services-3376.svc.cluster.local.\nName:\texternalsvc.services-3376.svc.cluster.local\nAddress: 10.152.183.55\n\n"
  Jul 29 12:15:11.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-3376, will wait for the garbage collector to delete the pods @ 07/29/23 12:15:11.572
  Jul 29 12:15:11.634: INFO: Deleting ReplicationController externalsvc took: 7.503339ms
  Jul 29 12:15:11.734: INFO: Terminating ReplicationController externalsvc pods took: 100.734919ms
  Jul 29 12:15:13.752: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-3376" for this suite. @ 07/29/23 12:15:13.765
• [7.574 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 07/29/23 12:15:13.771
  Jul 29 12:15:13.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 12:15:13.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:15:13.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:15:13.792
  STEP: Creating a pod to test substitution in container's args @ 07/29/23 12:15:13.795
  STEP: Saw pod success @ 07/29/23 12:15:17.819
  Jul 29 12:15:17.823: INFO: Trying to get logs from node ip-172-31-33-37 pod var-expansion-888aabe3-ce88-4701-8e35-e52cf012be75 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 12:15:17.83
  Jul 29 12:15:17.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2208" for this suite. @ 07/29/23 12:15:17.857
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 07/29/23 12:15:17.865
  Jul 29 12:15:17.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 12:15:17.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:15:17.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:15:17.891
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-7309 @ 07/29/23 12:15:17.895
  STEP: changing the ExternalName service to type=ClusterIP @ 07/29/23 12:15:17.899
  STEP: creating replication controller externalname-service in namespace services-7309 @ 07/29/23 12:15:17.914
  I0729 12:15:17.920743      18 runners.go:194] Created replication controller with name: externalname-service, namespace: services-7309, replica count: 2
  I0729 12:15:20.971668      18 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 12:15:20.971: INFO: Creating new exec pod
  Jul 29 12:15:23.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-7309 exec execpodcwx5r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul 29 12:15:24.119: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jul 29 12:15:24.119: INFO: stdout: "externalname-service-jrs99"
  Jul 29 12:15:24.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-7309 exec execpodcwx5r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.91 80'
  Jul 29 12:15:24.258: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.91 80\nConnection to 10.152.183.91 80 port [tcp/http] succeeded!\n"
  Jul 29 12:15:24.258: INFO: stdout: ""
  Jul 29 12:15:25.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-7309 exec execpodcwx5r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.91 80'
  Jul 29 12:15:25.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.91 80\nConnection to 10.152.183.91 80 port [tcp/http] succeeded!\n"
  Jul 29 12:15:25.377: INFO: stdout: ""
  Jul 29 12:15:26.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-7309 exec execpodcwx5r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.91 80'
  Jul 29 12:15:26.399: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.91 80\nConnection to 10.152.183.91 80 port [tcp/http] succeeded!\n"
  Jul 29 12:15:26.400: INFO: stdout: ""
  Jul 29 12:15:27.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-7309 exec execpodcwx5r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.91 80'
  Jul 29 12:15:27.388: INFO: stderr: "+ nc -v -t -w 2 10.152.183.91 80\nConnection to 10.152.183.91 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jul 29 12:15:27.388: INFO: stdout: ""
  Jul 29 12:15:28.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-7309 exec execpodcwx5r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.91 80'
  Jul 29 12:15:28.392: INFO: stderr: "+ nc -v -t -w 2 10.152.183.91 80\nConnection to 10.152.183.91 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  Jul 29 12:15:28.392: INFO: stdout: "externalname-service-jrs99"
  Jul 29 12:15:28.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 12:15:28.396: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-7309" for this suite. @ 07/29/23 12:15:28.423
• [10.567 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 07/29/23 12:15:28.434
  Jul 29 12:15:28.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pod-network-test @ 07/29/23 12:15:28.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:15:28.454
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:15:28.463
  STEP: Performing setup for networking test in namespace pod-network-test-6007 @ 07/29/23 12:15:28.468
  STEP: creating a selector @ 07/29/23 12:15:28.468
  STEP: Creating the service pods in kubernetes @ 07/29/23 12:15:28.468
  Jul 29 12:15:28.468: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 07/29/23 12:15:50.577
  Jul 29 12:15:52.614: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 29 12:15:52.614: INFO: Going to poll 192.168.10.10 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 12:15:52.616: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.10.10:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6007 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:15:52.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:15:52.617: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:15:52.617: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6007/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.10.10%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 12:15:52.678: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul 29 12:15:52.678: INFO: Going to poll 192.168.129.91 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 12:15:52.681: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.129.91:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6007 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:15:52.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:15:52.682: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:15:52.682: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6007/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.129.91%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 12:15:52.745: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul 29 12:15:52.745: INFO: Going to poll 192.168.8.138 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 12:15:52.750: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.8.138:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6007 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:15:52.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:15:52.750: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:15:52.750: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-6007/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.8.138%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 12:15:52.822: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul 29 12:15:52.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6007" for this suite. @ 07/29/23 12:15:52.827
• [24.400 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 07/29/23 12:15:52.835
  Jul 29 12:15:52.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename dns @ 07/29/23 12:15:52.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:15:52.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:15:52.862
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/29/23 12:15:52.865
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/29/23 12:15:52.865
  STEP: creating a pod to probe DNS @ 07/29/23 12:15:52.865
  STEP: submitting the pod to kubernetes @ 07/29/23 12:15:52.865
  STEP: retrieving the pod @ 07/29/23 12:15:54.879
  STEP: looking for the results for each expected name from probers @ 07/29/23 12:15:54.884
  Jul 29 12:15:54.900: INFO: DNS probes using dns-2604/dns-test-f472f266-9223-49a1-bf2a-bcf5c872fd53 succeeded

  Jul 29 12:15:54.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 12:15:54.905
  STEP: Destroying namespace "dns-2604" for this suite. @ 07/29/23 12:15:54.917
• [2.089 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 07/29/23 12:15:54.924
  Jul 29 12:15:54.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 12:15:54.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:15:54.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:15:54.944
  STEP: Discovering how many secrets are in namespace by default @ 07/29/23 12:15:54.947
  STEP: Counting existing ResourceQuota @ 07/29/23 12:15:59.951
  STEP: Creating a ResourceQuota @ 07/29/23 12:16:04.956
  STEP: Ensuring resource quota status is calculated @ 07/29/23 12:16:04.961
  STEP: Creating a Secret @ 07/29/23 12:16:06.964
  STEP: Ensuring resource quota status captures secret creation @ 07/29/23 12:16:06.978
  STEP: Deleting a secret @ 07/29/23 12:16:08.982
  STEP: Ensuring resource quota status released usage @ 07/29/23 12:16:08.987
  Jul 29 12:16:10.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6625" for this suite. @ 07/29/23 12:16:10.995
• [16.079 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 07/29/23 12:16:11.003
  Jul 29 12:16:11.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 12:16:11.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:16:11.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:16:11.027
  STEP: Creating configMap with name projected-configmap-test-volume-f9c6b2ac-5ba6-4769-adbe-8d0e3ec31e95 @ 07/29/23 12:16:11.034
  STEP: Creating a pod to test consume configMaps @ 07/29/23 12:16:11.044
  STEP: Saw pod success @ 07/29/23 12:16:15.072
  Jul 29 12:16:15.075: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-projected-configmaps-a89228a8-e3b2-47d4-b9ab-9f12196105fb container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 12:16:15.081
  Jul 29 12:16:15.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9557" for this suite. @ 07/29/23 12:16:15.101
• [4.105 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 07/29/23 12:16:15.109
  Jul 29 12:16:15.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename security-context @ 07/29/23 12:16:15.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:16:15.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:16:15.131
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/29/23 12:16:15.133
  STEP: Saw pod success @ 07/29/23 12:16:19.154
  Jul 29 12:16:19.157: INFO: Trying to get logs from node ip-172-31-33-37 pod security-context-4d5e5c71-cf1d-4c67-8ea5-543979b91dfe container test-container: <nil>
  STEP: delete the pod @ 07/29/23 12:16:19.164
  Jul 29 12:16:19.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-278" for this suite. @ 07/29/23 12:16:19.192
• [4.090 seconds]
------------------------------
SSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 07/29/23 12:16:19.2
  Jul 29 12:16:19.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename events @ 07/29/23 12:16:19.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:16:19.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:16:19.232
  STEP: Create set of events @ 07/29/23 12:16:19.236
  Jul 29 12:16:19.242: INFO: created test-event-1
  Jul 29 12:16:19.247: INFO: created test-event-2
  Jul 29 12:16:19.251: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 07/29/23 12:16:19.251
  STEP: delete collection of events @ 07/29/23 12:16:19.254
  Jul 29 12:16:19.254: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/29/23 12:16:19.276
  Jul 29 12:16:19.276: INFO: requesting list of events to confirm quantity
  Jul 29 12:16:19.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4332" for this suite. @ 07/29/23 12:16:19.283
• [0.090 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 07/29/23 12:16:19.29
  Jul 29 12:16:19.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 12:16:19.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:16:19.309
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:16:19.313
  STEP: Creating a pod to test downward api env vars @ 07/29/23 12:16:19.317
  STEP: Saw pod success @ 07/29/23 12:16:23.336
  Jul 29 12:16:23.339: INFO: Trying to get logs from node ip-172-31-33-37 pod downward-api-0d7198af-2192-47dd-a946-b1076e1d335c container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 12:16:23.348
  Jul 29 12:16:23.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5610" for this suite. @ 07/29/23 12:16:23.366
• [4.084 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 07/29/23 12:16:23.375
  Jul 29 12:16:23.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 12:16:23.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:16:23.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:16:23.395
  STEP: creating the pod @ 07/29/23 12:16:23.398
  STEP: waiting for pod running @ 07/29/23 12:16:23.407
  STEP: creating a file in subpath @ 07/29/23 12:16:25.416
  Jul 29 12:16:25.418: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4065 PodName:var-expansion-972b7847-7db8-48bb-8aa4-55cf5555a238 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:16:25.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:16:25.419: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:16:25.419: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-4065/pods/var-expansion-972b7847-7db8-48bb-8aa4-55cf5555a238/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 07/29/23 12:16:25.476
  Jul 29 12:16:25.479: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4065 PodName:var-expansion-972b7847-7db8-48bb-8aa4-55cf5555a238 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:16:25.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:16:25.480: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:16:25.480: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-4065/pods/var-expansion-972b7847-7db8-48bb-8aa4-55cf5555a238/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 07/29/23 12:16:25.555
  Jul 29 12:16:26.068: INFO: Successfully updated pod "var-expansion-972b7847-7db8-48bb-8aa4-55cf5555a238"
  STEP: waiting for annotated pod running @ 07/29/23 12:16:26.068
  STEP: deleting the pod gracefully @ 07/29/23 12:16:26.071
  Jul 29 12:16:26.071: INFO: Deleting pod "var-expansion-972b7847-7db8-48bb-8aa4-55cf5555a238" in namespace "var-expansion-4065"
  Jul 29 12:16:26.079: INFO: Wait up to 5m0s for pod "var-expansion-972b7847-7db8-48bb-8aa4-55cf5555a238" to be fully deleted
  Jul 29 12:16:58.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4065" for this suite. @ 07/29/23 12:16:58.154
• [34.787 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 07/29/23 12:16:58.162
  Jul 29 12:16:58.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename taint-single-pod @ 07/29/23 12:16:58.163
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:16:58.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:16:58.183
  Jul 29 12:16:58.186: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 12:17:58.203: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 12:17:58.206: INFO: Starting informer...
  STEP: Starting pod... @ 07/29/23 12:17:58.206
  Jul 29 12:17:58.420: INFO: Pod is running on ip-172-31-33-37. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/29/23 12:17:58.42
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/29/23 12:17:58.43
  STEP: Waiting short time to make sure Pod is queued for deletion @ 07/29/23 12:17:58.433
  Jul 29 12:17:58.433: INFO: Pod wasn't evicted. Proceeding
  Jul 29 12:17:58.433: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/29/23 12:17:58.444
  STEP: Waiting some time to make sure that toleration time passed. @ 07/29/23 12:17:58.447
  Jul 29 12:19:13.448: INFO: Pod wasn't evicted. Test successful
  Jul 29 12:19:13.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-1718" for this suite. @ 07/29/23 12:19:13.452
• [135.297 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 07/29/23 12:19:13.46
  Jul 29 12:19:13.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename sched-preemption @ 07/29/23 12:19:13.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:19:13.479
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:19:13.482
  Jul 29 12:19:13.502: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 12:20:13.519: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/29/23 12:20:13.523
  Jul 29 12:20:13.546: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul 29 12:20:13.556: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul 29 12:20:13.594: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul 29 12:20:13.602: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul 29 12:20:13.622: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul 29 12:20:13.704: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/29/23 12:20:13.705
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 07/29/23 12:20:17.741
  Jul 29 12:20:21.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1464" for this suite. @ 07/29/23 12:20:21.821
• [68.368 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 07/29/23 12:20:21.831
  Jul 29 12:20:21.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 12:20:21.832
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:21.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:21.854
  STEP: validating api versions @ 07/29/23 12:20:21.858
  Jul 29 12:20:21.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6440 api-versions'
  Jul 29 12:20:21.966: INFO: stderr: ""
  Jul 29 12:20:21.966: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Jul 29 12:20:21.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6440" for this suite. @ 07/29/23 12:20:21.971
• [0.147 seconds]
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 07/29/23 12:20:21.978
  Jul 29 12:20:21.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename disruption @ 07/29/23 12:20:21.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:22.004
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:22.007
  STEP: Waiting for the pdb to be processed @ 07/29/23 12:20:22.016
  STEP: Waiting for all pods to be running @ 07/29/23 12:20:24.044
  Jul 29 12:20:24.048: INFO: running pods: 0 < 3
  Jul 29 12:20:26.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-1399" for this suite. @ 07/29/23 12:20:26.064
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 07/29/23 12:20:26.072
  Jul 29 12:20:26.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 12:20:26.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:26.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:26.093
  STEP: Creating the pod @ 07/29/23 12:20:26.097
  Jul 29 12:20:28.652: INFO: Successfully updated pod "annotationupdate45e3e0bb-8549-4de9-9f03-a5d751e8a67e"
  Jul 29 12:20:32.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1242" for this suite. @ 07/29/23 12:20:32.68
• [6.615 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 07/29/23 12:20:32.689
  Jul 29 12:20:32.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 12:20:32.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:32.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:32.71
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/29/23 12:20:32.713
  STEP: Saw pod success @ 07/29/23 12:20:36.73
  Jul 29 12:20:36.733: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-1ace787a-b8ad-443a-80e1-91b8221c4bad container test-container: <nil>
  STEP: delete the pod @ 07/29/23 12:20:36.741
  Jul 29 12:20:36.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7071" for this suite. @ 07/29/23 12:20:36.761
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 07/29/23 12:20:36.769
  Jul 29 12:20:36.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 12:20:36.77
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:36.788
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:36.792
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/29/23 12:20:36.796
  Jul 29 12:20:36.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-3918 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul 29 12:20:36.865: INFO: stderr: ""
  Jul 29 12:20:36.865: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 07/29/23 12:20:36.865
  Jul 29 12:20:36.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-3918 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jul 29 12:20:36.931: INFO: stderr: ""
  Jul 29 12:20:36.931: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/29/23 12:20:36.932
  Jul 29 12:20:36.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-3918 delete pods e2e-test-httpd-pod'
  Jul 29 12:20:38.890: INFO: stderr: ""
  Jul 29 12:20:38.890: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 29 12:20:38.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3918" for this suite. @ 07/29/23 12:20:38.895
• [2.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 07/29/23 12:20:38.902
  Jul 29 12:20:38.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 12:20:38.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:38.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:38.921
  STEP: Creating a pod to test downward api env vars @ 07/29/23 12:20:38.923
  STEP: Saw pod success @ 07/29/23 12:20:42.947
  Jul 29 12:20:42.950: INFO: Trying to get logs from node ip-172-31-33-37 pod downward-api-dfe84d15-9f7e-45a6-8b6e-20178bf4fc80 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 12:20:42.958
  Jul 29 12:20:42.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7491" for this suite. @ 07/29/23 12:20:42.977
• [4.083 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 07/29/23 12:20:42.986
  Jul 29 12:20:42.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 12:20:42.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:43.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:43.006
  STEP: Creating namespace "e2e-ns-km5pw" @ 07/29/23 12:20:43.009
  Jul 29 12:20:43.030: INFO: Namespace "e2e-ns-km5pw-2675" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-km5pw-2675" @ 07/29/23 12:20:43.03
  Jul 29 12:20:43.039: INFO: Namespace "e2e-ns-km5pw-2675" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-km5pw-2675" @ 07/29/23 12:20:43.039
  Jul 29 12:20:43.047: INFO: Namespace "e2e-ns-km5pw-2675" has []v1.FinalizerName{"kubernetes"}
  Jul 29 12:20:43.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2165" for this suite. @ 07/29/23 12:20:43.051
  STEP: Destroying namespace "e2e-ns-km5pw-2675" for this suite. @ 07/29/23 12:20:43.056
• [0.078 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 07/29/23 12:20:43.064
  Jul 29 12:20:43.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 12:20:43.065
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:43.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:43.084
  Jul 29 12:20:43.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3323" for this suite. @ 07/29/23 12:20:43.118
• [0.060 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 07/29/23 12:20:43.126
  Jul 29 12:20:43.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 12:20:43.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:43.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:43.147
  STEP: Creating configMap with name configmap-test-volume-map-700c8317-b5d9-4be1-88d9-32655fdcaf23 @ 07/29/23 12:20:43.15
  STEP: Creating a pod to test consume configMaps @ 07/29/23 12:20:43.156
  STEP: Saw pod success @ 07/29/23 12:20:47.175
  Jul 29 12:20:47.179: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-configmaps-e3adface-97c7-43db-86c2-325b6ad038aa container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 12:20:47.187
  Jul 29 12:20:47.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4724" for this suite. @ 07/29/23 12:20:47.208
• [4.090 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 07/29/23 12:20:47.216
  Jul 29 12:20:47.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 12:20:47.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:47.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:47.237
  STEP: Setting up server cert @ 07/29/23 12:20:47.267
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 12:20:48.05
  STEP: Deploying the webhook pod @ 07/29/23 12:20:48.059
  STEP: Wait for the deployment to be ready @ 07/29/23 12:20:48.073
  Jul 29 12:20:48.082: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 12:20:50.091
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 12:20:50.102
  Jul 29 12:20:51.102: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 07/29/23 12:20:51.105
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 12:20:51.122
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 07/29/23 12:20:51.131
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 12:20:51.142
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 07/29/23 12:20:51.153
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 12:20:51.16
  Jul 29 12:20:51.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3621" for this suite. @ 07/29/23 12:20:51.209
  STEP: Destroying namespace "webhook-markers-5744" for this suite. @ 07/29/23 12:20:51.215
• [4.007 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 07/29/23 12:20:51.224
  Jul 29 12:20:51.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 12:20:51.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:51.245
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:51.247
  Jul 29 12:20:53.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9025" for this suite. @ 07/29/23 12:20:53.284
• [2.067 seconds]
------------------------------
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 07/29/23 12:20:53.291
  Jul 29 12:20:53.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename certificates @ 07/29/23 12:20:53.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:53.309
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:53.312
  STEP: getting /apis @ 07/29/23 12:20:53.998
  STEP: getting /apis/certificates.k8s.io @ 07/29/23 12:20:54.002
  STEP: getting /apis/certificates.k8s.io/v1 @ 07/29/23 12:20:54.003
  STEP: creating @ 07/29/23 12:20:54.004
  STEP: getting @ 07/29/23 12:20:54.022
  STEP: listing @ 07/29/23 12:20:54.024
  STEP: watching @ 07/29/23 12:20:54.028
  Jul 29 12:20:54.028: INFO: starting watch
  STEP: patching @ 07/29/23 12:20:54.029
  STEP: updating @ 07/29/23 12:20:54.037
  Jul 29 12:20:54.042: INFO: waiting for watch events with expected annotations
  Jul 29 12:20:54.042: INFO: saw patched and updated annotations
  STEP: getting /approval @ 07/29/23 12:20:54.043
  STEP: patching /approval @ 07/29/23 12:20:54.046
  STEP: updating /approval @ 07/29/23 12:20:54.053
  STEP: getting /status @ 07/29/23 12:20:54.059
  STEP: patching /status @ 07/29/23 12:20:54.061
  STEP: updating /status @ 07/29/23 12:20:54.07
  STEP: deleting @ 07/29/23 12:20:54.077
  STEP: deleting a collection @ 07/29/23 12:20:54.089
  Jul 29 12:20:54.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-2254" for this suite. @ 07/29/23 12:20:54.109
• [0.824 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 07/29/23 12:20:54.115
  Jul 29 12:20:54.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename disruption @ 07/29/23 12:20:54.116
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:54.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:54.136
  STEP: Waiting for the pdb to be processed @ 07/29/23 12:20:54.144
  STEP: Updating PodDisruptionBudget status @ 07/29/23 12:20:56.152
  STEP: Waiting for all pods to be running @ 07/29/23 12:20:56.161
  Jul 29 12:20:56.164: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 07/29/23 12:20:58.168
  STEP: Waiting for the pdb to be processed @ 07/29/23 12:20:58.179
  STEP: Patching PodDisruptionBudget status @ 07/29/23 12:20:58.186
  STEP: Waiting for the pdb to be processed @ 07/29/23 12:20:58.197
  Jul 29 12:20:58.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-265" for this suite. @ 07/29/23 12:20:58.205
• [4.099 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 07/29/23 12:20:58.215
  Jul 29 12:20:58.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 12:20:58.216
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:20:58.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:20:58.237
  Jul 29 12:20:58.240: INFO: Creating ReplicaSet my-hostname-basic-7904f1ab-1568-4a0f-9fd1-3d6d5eabc534
  Jul 29 12:20:58.249: INFO: Pod name my-hostname-basic-7904f1ab-1568-4a0f-9fd1-3d6d5eabc534: Found 0 pods out of 1
  Jul 29 12:21:03.255: INFO: Pod name my-hostname-basic-7904f1ab-1568-4a0f-9fd1-3d6d5eabc534: Found 1 pods out of 1
  Jul 29 12:21:03.255: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-7904f1ab-1568-4a0f-9fd1-3d6d5eabc534" is running
  Jul 29 12:21:03.258: INFO: Pod "my-hostname-basic-7904f1ab-1568-4a0f-9fd1-3d6d5eabc534-vdq5k" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 12:20:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 12:20:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 12:20:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 12:20:58 +0000 UTC Reason: Message:}])
  Jul 29 12:21:03.258: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/29/23 12:21:03.258
  Jul 29 12:21:03.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1768" for this suite. @ 07/29/23 12:21:03.278
• [5.069 seconds]
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 07/29/23 12:21:03.285
  Jul 29 12:21:03.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 12:21:03.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:21:03.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:21:03.307
  STEP: Creating Pod @ 07/29/23 12:21:03.315
  STEP: Reading file content from the nginx-container @ 07/29/23 12:21:05.336
  Jul 29 12:21:05.336: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6956 PodName:pod-sharedvolume-68931942-0e4f-43b3-9f98-f11ffaa004e6 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:21:05.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:21:05.337: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:21:05.337: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-6956/pods/pod-sharedvolume-68931942-0e4f-43b3-9f98-f11ffaa004e6/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Jul 29 12:21:05.389: INFO: Exec stderr: ""
  Jul 29 12:21:05.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6956" for this suite. @ 07/29/23 12:21:05.393
• [2.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 07/29/23 12:21:05.402
  Jul 29 12:21:05.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 12:21:05.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:21:05.421
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:21:05.424
  STEP: Creating a ResourceQuota with best effort scope @ 07/29/23 12:21:05.427
  STEP: Ensuring ResourceQuota status is calculated @ 07/29/23 12:21:05.431
  STEP: Creating a ResourceQuota with not best effort scope @ 07/29/23 12:21:07.435
  STEP: Ensuring ResourceQuota status is calculated @ 07/29/23 12:21:07.441
  STEP: Creating a best-effort pod @ 07/29/23 12:21:09.444
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 07/29/23 12:21:09.458
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 07/29/23 12:21:11.464
  STEP: Deleting the pod @ 07/29/23 12:21:13.468
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 12:21:13.479
  STEP: Creating a not best-effort pod @ 07/29/23 12:21:15.483
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 07/29/23 12:21:15.494
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 07/29/23 12:21:17.498
  STEP: Deleting the pod @ 07/29/23 12:21:19.501
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 12:21:19.512
  Jul 29 12:21:21.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2917" for this suite. @ 07/29/23 12:21:21.52
• [16.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 07/29/23 12:21:21.529
  Jul 29 12:21:21.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename limitrange @ 07/29/23 12:21:21.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:21:21.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:21:21.55
  STEP: Creating LimitRange "e2e-limitrange-wgcdn" in namespace "limitrange-6927" @ 07/29/23 12:21:21.553
  STEP: Creating another limitRange in another namespace @ 07/29/23 12:21:21.559
  Jul 29 12:21:21.575: INFO: Namespace "e2e-limitrange-wgcdn-7249" created
  Jul 29 12:21:21.575: INFO: Creating LimitRange "e2e-limitrange-wgcdn" in namespace "e2e-limitrange-wgcdn-7249"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-wgcdn" @ 07/29/23 12:21:21.58
  Jul 29 12:21:21.589: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-wgcdn" in "limitrange-6927" namespace @ 07/29/23 12:21:21.589
  Jul 29 12:21:21.595: INFO: LimitRange "e2e-limitrange-wgcdn" has been patched
  STEP: Delete LimitRange "e2e-limitrange-wgcdn" by Collection with labelSelector: "e2e-limitrange-wgcdn=patched" @ 07/29/23 12:21:21.595
  STEP: Confirm that the limitRange "e2e-limitrange-wgcdn" has been deleted @ 07/29/23 12:21:21.603
  Jul 29 12:21:21.603: INFO: Requesting list of LimitRange to confirm quantity
  Jul 29 12:21:21.605: INFO: Found 0 LimitRange with label "e2e-limitrange-wgcdn=patched"
  Jul 29 12:21:21.605: INFO: LimitRange "e2e-limitrange-wgcdn" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-wgcdn" @ 07/29/23 12:21:21.605
  Jul 29 12:21:21.609: INFO: Found 1 limitRange
  Jul 29 12:21:21.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-6927" for this suite. @ 07/29/23 12:21:21.612
  STEP: Destroying namespace "e2e-limitrange-wgcdn-7249" for this suite. @ 07/29/23 12:21:21.618
• [0.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 07/29/23 12:21:21.627
  Jul 29 12:21:21.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 12:21:21.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:21:21.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:21:21.65
  STEP: Creating a pod to test downward api env vars @ 07/29/23 12:21:21.653
  STEP: Saw pod success @ 07/29/23 12:21:25.677
  Jul 29 12:21:25.681: INFO: Trying to get logs from node ip-172-31-33-37 pod downward-api-ead9e7cf-27c5-456c-8c5c-cf2017170541 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 12:21:25.687
  Jul 29 12:21:25.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1625" for this suite. @ 07/29/23 12:21:25.704
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 07/29/23 12:21:25.712
  Jul 29 12:21:25.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 12:21:25.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:21:25.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:21:25.733
  STEP: creating Agnhost RC @ 07/29/23 12:21:25.737
  Jul 29 12:21:25.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8875 create -f -'
  Jul 29 12:21:26.397: INFO: stderr: ""
  Jul 29 12:21:26.397: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/29/23 12:21:26.397
  Jul 29 12:21:27.401: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 12:21:27.401: INFO: Found 0 / 1
  Jul 29 12:21:28.401: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 12:21:28.401: INFO: Found 1 / 1
  Jul 29 12:21:28.401: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 07/29/23 12:21:28.401
  Jul 29 12:21:28.406: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 12:21:28.406: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 29 12:21:28.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8875 patch pod agnhost-primary-8w74l -p {"metadata":{"annotations":{"x":"y"}}}'
  Jul 29 12:21:28.481: INFO: stderr: ""
  Jul 29 12:21:28.481: INFO: stdout: "pod/agnhost-primary-8w74l patched\n"
  STEP: checking annotations @ 07/29/23 12:21:28.481
  Jul 29 12:21:28.484: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 12:21:28.484: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 29 12:21:28.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8875" for this suite. @ 07/29/23 12:21:28.488
• [2.782 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 07/29/23 12:21:28.494
  Jul 29 12:21:28.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 12:21:28.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:21:28.517
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:21:28.52
  Jul 29 12:21:28.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:21:29.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8705" for this suite. @ 07/29/23 12:21:29.07
• [0.583 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 07/29/23 12:21:29.078
  Jul 29 12:21:29.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pods @ 07/29/23 12:21:29.079
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:21:29.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:21:29.1
  Jul 29 12:21:29.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: creating the pod @ 07/29/23 12:21:29.104
  STEP: submitting the pod to kubernetes @ 07/29/23 12:21:29.104
  Jul 29 12:21:31.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6136" for this suite. @ 07/29/23 12:21:31.207
• [2.135 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 07/29/23 12:21:31.214
  Jul 29 12:21:31.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename deployment @ 07/29/23 12:21:31.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:21:31.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:21:31.236
  Jul 29 12:21:31.239: INFO: Creating deployment "test-recreate-deployment"
  Jul 29 12:21:31.244: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jul 29 12:21:31.252: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  Jul 29 12:21:33.257: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jul 29 12:21:33.260: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jul 29 12:21:33.270: INFO: Updating deployment test-recreate-deployment
  Jul 29 12:21:33.270: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jul 29 12:21:33.360: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1123  b17748f1-bf39-4219-b1cb-c7763014a96b 6903 2 2023-07-29 12:21:31 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-29 12:21:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 12:21:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e1bb68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-29 12:21:33 +0000 UTC,LastTransitionTime:2023-07-29 12:21:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-07-29 12:21:33 +0000 UTC,LastTransitionTime:2023-07-29 12:21:31 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jul 29 12:21:33.368: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-1123  4d89cb2e-8717-487d-b53b-2c490336579c 6900 1 2023-07-29 12:21:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b17748f1-bf39-4219-b1cb-c7763014a96b 0xc003ddfbd7 0xc003ddfbd8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 12:21:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17748f1-bf39-4219-b1cb-c7763014a96b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 12:21:33 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ddfc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 12:21:33.368: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jul 29 12:21:33.368: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-1123  9bc8948e-f314-436f-aace-7e7db6e88501 6891 2 2023-07-29 12:21:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b17748f1-bf39-4219-b1cb-c7763014a96b 0xc003ddfcd7 0xc003ddfcd8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 12:21:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b17748f1-bf39-4219-b1cb-c7763014a96b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 12:21:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ddfd88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 12:21:33.372: INFO: Pod "test-recreate-deployment-54757ffd6c-dsfg4" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-dsfg4 test-recreate-deployment-54757ffd6c- deployment-1123  7507b3d9-ad1f-4782-bbea-9f1511b49c53 6902 0 2023-07-29 12:21:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 4d89cb2e-8717-487d-b53b-2c490336579c 0xc003bd81d7 0xc003bd81d8}] [] [{kube-controller-manager Update v1 2023-07-29 12:21:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d89cb2e-8717-487d-b53b-2c490336579c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:21:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jkmww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jkmww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:21:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:21:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:21:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:21:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:,StartTime:2023-07-29 12:21:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:21:33.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1123" for this suite. @ 07/29/23 12:21:33.377
• [2.168 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 07/29/23 12:21:33.383
  Jul 29 12:21:33.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename security-context-test @ 07/29/23 12:21:33.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:21:33.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:21:33.403
  Jul 29 12:21:37.430: INFO: Got logs for pod "busybox-privileged-false-42034ea9-6683-4503-a557-3f79ae45be82": "ip: RTNETLINK answers: Operation not permitted\n"
  Jul 29 12:21:37.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5754" for this suite. @ 07/29/23 12:21:37.434
• [4.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 07/29/23 12:21:37.446
  Jul 29 12:21:37.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename aggregator @ 07/29/23 12:21:37.447
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:21:37.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:21:37.469
  Jul 29 12:21:37.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Registering the sample API server. @ 07/29/23 12:21:37.473
  Jul 29 12:21:37.943: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jul 29 12:21:37.968: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
  Jul 29 12:21:40.013: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 12:21:42.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 12:21:44.018: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 12:21:46.018: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 12:21:48.018: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 12:21:50.019: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 12:21:52.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 12:21:54.016: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 12:21:56.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 12:21:58.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 12:22:00.017: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 12, 21, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 21, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 12:22:02.150: INFO: Waited 124.275819ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 07/29/23 12:22:02.195
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 07/29/23 12:22:02.206
  STEP: List APIServices @ 07/29/23 12:22:02.217
  Jul 29 12:22:02.223: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 07/29/23 12:22:02.223
  Jul 29 12:22:02.236: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 07/29/23 12:22:02.236
  Jul 29 12:22:02.248: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.July, 29, 12, 22, 2, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 07/29/23 12:22:02.248
  Jul 29 12:22:02.251: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-07-29 12:22:02 +0000 UTC Passed all checks passed}
  Jul 29 12:22:02.251: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 12:22:02.251: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 07/29/23 12:22:02.251
  Jul 29 12:22:02.263: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-678543034" @ 07/29/23 12:22:02.263
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 07/29/23 12:22:02.275
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 07/29/23 12:22:02.282
  STEP: Patch APIService Status @ 07/29/23 12:22:02.286
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 07/29/23 12:22:02.294
  Jul 29 12:22:02.297: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-07-29 12:22:02 +0000 UTC Passed all checks passed}
  Jul 29 12:22:02.297: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 12:22:02.297: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jul 29 12:22:02.297: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 07/29/23 12:22:02.297
  STEP: Confirm that the generated APIService has been deleted @ 07/29/23 12:22:02.301
  Jul 29 12:22:02.301: INFO: Requesting list of APIServices to confirm quantity
  Jul 29 12:22:02.307: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jul 29 12:22:02.307: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jul 29 12:22:02.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-8559" for this suite. @ 07/29/23 12:22:02.423
• [24.984 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 07/29/23 12:22:02.431
  Jul 29 12:22:02.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename subpath @ 07/29/23 12:22:02.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:22:02.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:22:02.463
  STEP: Setting up data @ 07/29/23 12:22:02.466
  STEP: Creating pod pod-subpath-test-secret-fb2c @ 07/29/23 12:22:02.476
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 12:22:02.476
  STEP: Saw pod success @ 07/29/23 12:22:26.543
  Jul 29 12:22:26.546: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-subpath-test-secret-fb2c container test-container-subpath-secret-fb2c: <nil>
  STEP: delete the pod @ 07/29/23 12:22:26.553
  STEP: Deleting pod pod-subpath-test-secret-fb2c @ 07/29/23 12:22:26.57
  Jul 29 12:22:26.570: INFO: Deleting pod "pod-subpath-test-secret-fb2c" in namespace "subpath-6986"
  Jul 29 12:22:26.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6986" for this suite. @ 07/29/23 12:22:26.576
• [24.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 07/29/23 12:22:26.585
  Jul 29 12:22:26.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 12:22:26.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:22:26.605
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:22:26.608
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 12:22:26.611
  STEP: Saw pod success @ 07/29/23 12:22:30.631
  Jul 29 12:22:30.634: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-fe67a538-4665-493a-9641-926164323ff9 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 12:22:30.64
  Jul 29 12:22:30.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9666" for this suite. @ 07/29/23 12:22:30.661
• [4.084 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 07/29/23 12:22:30.67
  Jul 29 12:22:30.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 12:22:30.671
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:22:30.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:22:30.693
  Jul 29 12:22:30.720: INFO: created pod pod-service-account-defaultsa
  Jul 29 12:22:30.720: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jul 29 12:22:30.727: INFO: created pod pod-service-account-mountsa
  Jul 29 12:22:30.727: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jul 29 12:22:30.736: INFO: created pod pod-service-account-nomountsa
  Jul 29 12:22:30.736: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jul 29 12:22:30.741: INFO: created pod pod-service-account-defaultsa-mountspec
  Jul 29 12:22:30.741: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jul 29 12:22:30.748: INFO: created pod pod-service-account-mountsa-mountspec
  Jul 29 12:22:30.748: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jul 29 12:22:30.756: INFO: created pod pod-service-account-nomountsa-mountspec
  Jul 29 12:22:30.756: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jul 29 12:22:30.763: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jul 29 12:22:30.763: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jul 29 12:22:30.770: INFO: created pod pod-service-account-mountsa-nomountspec
  Jul 29 12:22:30.771: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jul 29 12:22:30.778: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jul 29 12:22:30.778: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jul 29 12:22:30.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4785" for this suite. @ 07/29/23 12:22:30.786
• [0.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 07/29/23 12:22:30.809
  Jul 29 12:22:30.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pods @ 07/29/23 12:22:30.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:22:30.831
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:22:30.834
  STEP: creating the pod @ 07/29/23 12:22:30.838
  STEP: setting up watch @ 07/29/23 12:22:30.838
  STEP: submitting the pod to kubernetes @ 07/29/23 12:22:30.942
  STEP: verifying the pod is in kubernetes @ 07/29/23 12:22:30.957
  STEP: verifying pod creation was observed @ 07/29/23 12:22:30.963
  STEP: deleting the pod gracefully @ 07/29/23 12:22:32.976
  STEP: verifying pod deletion was observed @ 07/29/23 12:22:32.983
  Jul 29 12:22:34.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5607" for this suite. @ 07/29/23 12:22:34.201
• [3.399 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 07/29/23 12:22:34.21
  Jul 29 12:22:34.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 12:22:34.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:22:34.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:22:34.231
  STEP: creating Agnhost RC @ 07/29/23 12:22:34.234
  Jul 29 12:22:34.235: INFO: namespace kubectl-1499
  Jul 29 12:22:34.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-1499 create -f -'
  Jul 29 12:22:34.711: INFO: stderr: ""
  Jul 29 12:22:34.711: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/29/23 12:22:34.711
  Jul 29 12:22:35.716: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 12:22:35.716: INFO: Found 0 / 1
  Jul 29 12:22:36.714: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 12:22:36.714: INFO: Found 1 / 1
  Jul 29 12:22:36.714: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul 29 12:22:36.717: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 12:22:36.717: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 29 12:22:36.717: INFO: wait on agnhost-primary startup in kubectl-1499 
  Jul 29 12:22:36.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-1499 logs agnhost-primary-q8zjv agnhost-primary'
  Jul 29 12:22:36.905: INFO: stderr: ""
  Jul 29 12:22:36.905: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 07/29/23 12:22:36.905
  Jul 29 12:22:36.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-1499 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jul 29 12:22:36.998: INFO: stderr: ""
  Jul 29 12:22:36.998: INFO: stdout: "service/rm2 exposed\n"
  Jul 29 12:22:37.003: INFO: Service rm2 in namespace kubectl-1499 found.
  STEP: exposing service @ 07/29/23 12:22:39.011
  Jul 29 12:22:39.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-1499 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Jul 29 12:22:39.084: INFO: stderr: ""
  Jul 29 12:22:39.084: INFO: stdout: "service/rm3 exposed\n"
  Jul 29 12:22:39.088: INFO: Service rm3 in namespace kubectl-1499 found.
  Jul 29 12:22:41.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1499" for this suite. @ 07/29/23 12:22:41.1
• [6.897 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 07/29/23 12:22:41.108
  Jul 29 12:22:41.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 12:22:41.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:22:41.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:22:41.135
  STEP: creating service endpoint-test2 in namespace services-6857 @ 07/29/23 12:22:41.138
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6857 to expose endpoints map[] @ 07/29/23 12:22:41.148
  Jul 29 12:22:41.153: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  Jul 29 12:22:42.161: INFO: successfully validated that service endpoint-test2 in namespace services-6857 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-6857 @ 07/29/23 12:22:42.162
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6857 to expose endpoints map[pod1:[80]] @ 07/29/23 12:22:44.181
  Jul 29 12:22:44.190: INFO: successfully validated that service endpoint-test2 in namespace services-6857 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 07/29/23 12:22:44.19
  Jul 29 12:22:44.190: INFO: Creating new exec pod
  Jul 29 12:22:47.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6857 exec execpodr2tc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 29 12:22:47.331: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 29 12:22:47.331: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 12:22:47.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6857 exec execpodr2tc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.36 80'
  Jul 29 12:22:47.449: INFO: stderr: "+ nc -v -t -w 2 10.152.183.36 80\n+ echo hostName\nConnection to 10.152.183.36 80 port [tcp/http] succeeded!\n"
  Jul 29 12:22:47.449: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-6857 @ 07/29/23 12:22:47.449
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6857 to expose endpoints map[pod1:[80] pod2:[80]] @ 07/29/23 12:22:49.467
  Jul 29 12:22:49.482: INFO: successfully validated that service endpoint-test2 in namespace services-6857 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 07/29/23 12:22:49.482
  Jul 29 12:22:50.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6857 exec execpodr2tc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 29 12:22:50.610: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 29 12:22:50.610: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 12:22:50.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6857 exec execpodr2tc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.36 80'
  Jul 29 12:22:50.738: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.36 80\nConnection to 10.152.183.36 80 port [tcp/http] succeeded!\n"
  Jul 29 12:22:50.738: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-6857 @ 07/29/23 12:22:50.738
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6857 to expose endpoints map[pod2:[80]] @ 07/29/23 12:22:50.749
  Jul 29 12:22:51.776: INFO: successfully validated that service endpoint-test2 in namespace services-6857 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 07/29/23 12:22:51.776
  Jul 29 12:22:52.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6857 exec execpodr2tc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 29 12:22:52.903: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 29 12:22:52.903: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 12:22:52.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6857 exec execpodr2tc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.36 80'
  Jul 29 12:22:53.034: INFO: stderr: "+ nc -v -t -w 2 10.152.183.36 80\n+ echo hostName\nConnection to 10.152.183.36 80 port [tcp/http] succeeded!\n"
  Jul 29 12:22:53.034: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-6857 @ 07/29/23 12:22:53.034
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6857 to expose endpoints map[] @ 07/29/23 12:22:53.05
  Jul 29 12:22:53.062: INFO: successfully validated that service endpoint-test2 in namespace services-6857 exposes endpoints map[]
  Jul 29 12:22:53.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6857" for this suite. @ 07/29/23 12:22:53.085
• [11.987 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 07/29/23 12:22:53.095
  Jul 29 12:22:53.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 12:22:53.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:22:53.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:22:53.121
  STEP: Creating configMap with name configmap-test-volume-6871f3a4-0b6f-41ec-be68-37053f3c190e @ 07/29/23 12:22:53.124
  STEP: Creating a pod to test consume configMaps @ 07/29/23 12:22:53.128
  STEP: Saw pod success @ 07/29/23 12:22:57.149
  Jul 29 12:22:57.152: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-configmaps-deb48585-d3ff-431d-ade4-5db54577698f container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 12:22:57.158
  Jul 29 12:22:57.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7746" for this suite. @ 07/29/23 12:22:57.178
• [4.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 07/29/23 12:22:57.187
  Jul 29 12:22:57.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 12:22:57.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:22:57.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:22:57.207
  STEP: Starting the proxy @ 07/29/23 12:22:57.211
  Jul 29 12:22:57.211: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-3961 proxy --unix-socket=/tmp/kubectl-proxy-unix422322530/test'
  STEP: retrieving proxy /api/ output @ 07/29/23 12:22:57.256
  Jul 29 12:22:57.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3961" for this suite. @ 07/29/23 12:22:57.262
• [0.082 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 07/29/23 12:22:57.269
  Jul 29 12:22:57.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 12:22:57.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:22:57.293
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:22:57.295
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/29/23 12:22:57.298
  STEP: Saw pod success @ 07/29/23 12:23:01.322
  Jul 29 12:23:01.326: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-e27ed878-47d1-4779-8c28-6ab8b6f59766 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 12:23:01.332
  Jul 29 12:23:01.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4674" for this suite. @ 07/29/23 12:23:01.363
• [4.101 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 07/29/23 12:23:01.37
  Jul 29 12:23:01.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 12:23:01.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:23:01.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:23:01.394
  STEP: Counting existing ResourceQuota @ 07/29/23 12:23:01.397
  STEP: Creating a ResourceQuota @ 07/29/23 12:23:06.401
  STEP: Ensuring resource quota status is calculated @ 07/29/23 12:23:06.407
  STEP: Creating a ReplicaSet @ 07/29/23 12:23:08.411
  STEP: Ensuring resource quota status captures replicaset creation @ 07/29/23 12:23:08.422
  STEP: Deleting a ReplicaSet @ 07/29/23 12:23:10.427
  STEP: Ensuring resource quota status released usage @ 07/29/23 12:23:10.434
  Jul 29 12:23:12.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-46" for this suite. @ 07/29/23 12:23:12.442
• [11.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 07/29/23 12:23:12.449
  Jul 29 12:23:12.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 12:23:12.45
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:23:12.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:23:12.508
  STEP: Counting existing ResourceQuota @ 07/29/23 12:23:12.511
  STEP: Creating a ResourceQuota @ 07/29/23 12:23:17.515
  STEP: Ensuring resource quota status is calculated @ 07/29/23 12:23:17.52
  STEP: Creating a ReplicationController @ 07/29/23 12:23:19.525
  STEP: Ensuring resource quota status captures replication controller creation @ 07/29/23 12:23:19.539
  STEP: Deleting a ReplicationController @ 07/29/23 12:23:21.542
  STEP: Ensuring resource quota status released usage @ 07/29/23 12:23:21.547
  Jul 29 12:23:23.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7451" for this suite. @ 07/29/23 12:23:23.558
• [11.116 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 07/29/23 12:23:23.566
  Jul 29 12:23:23.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename gc @ 07/29/23 12:23:23.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:23:23.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:23:23.586
  STEP: create the deployment @ 07/29/23 12:23:23.589
  W0729 12:23:23.596867      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/29/23 12:23:23.596
  STEP: delete the deployment @ 07/29/23 12:23:24.109
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 07/29/23 12:23:24.115
  STEP: Gathering metrics @ 07/29/23 12:23:24.638
  W0729 12:23:24.642780      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 29 12:23:24.642: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 12:23:24.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1521" for this suite. @ 07/29/23 12:23:24.646
• [1.087 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 07/29/23 12:23:24.654
  Jul 29 12:23:24.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename deployment @ 07/29/23 12:23:24.655
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:23:24.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:23:24.676
  Jul 29 12:23:24.680: INFO: Creating simple deployment test-new-deployment
  Jul 29 12:23:24.690: INFO: deployment "test-new-deployment" doesn't have the required revision set
  STEP: getting scale subresource @ 07/29/23 12:23:26.703
  STEP: updating a scale subresource @ 07/29/23 12:23:26.706
  STEP: verifying the deployment Spec.Replicas was modified @ 07/29/23 12:23:26.713
  STEP: Patch a scale subresource @ 07/29/23 12:23:26.715
  Jul 29 12:23:26.729: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-1331  3f4ee8be-63f8-4b7c-ac4d-8a3051cdd637 7901 3 2023-07-29 12:23:24 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-07-29 12:23:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 12:23:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bd9758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 12:23:25 +0000 UTC,LastTransitionTime:2023-07-29 12:23:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-07-29 12:23:25 +0000 UTC,LastTransitionTime:2023-07-29 12:23:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 29 12:23:26.737: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-1331  bf840810-716c-4970-ac7e-d49ac6708c62 7900 2 2023-07-29 12:23:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 3f4ee8be-63f8-4b7c-ac4d-8a3051cdd637 0xc003bd9b60 0xc003bd9b61}] [] [{kube-controller-manager Update apps/v1 2023-07-29 12:23:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-29 12:23:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f4ee8be-63f8-4b7c-ac4d-8a3051cdd637\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bd9be8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 12:23:26.740: INFO: Pod "test-new-deployment-67bd4bf6dc-2dh6l" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-2dh6l test-new-deployment-67bd4bf6dc- deployment-1331  aa4ab221-c493-4419-8db0-9adaccf211d5 7894 0 2023-07-29 12:23:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc bf840810-716c-4970-ac7e-d49ac6708c62 0xc003e761d0 0xc003e761d1}] [] [{kube-controller-manager Update v1 2023-07-29 12:23:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf840810-716c-4970-ac7e-d49ac6708c62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:23:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.10.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d4spp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d4spp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:23:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:23:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:23:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:23:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:192.168.10.22,StartTime:2023-07-29 12:23:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 12:23:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://990c7bfae8e66d7d14a922a047e5258c4cf9d484112569de98884d0efb5dde3e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.10.22,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:23:26.740: INFO: Pod "test-new-deployment-67bd4bf6dc-slksj" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-slksj test-new-deployment-67bd4bf6dc- deployment-1331  2495f273-1233-46cc-8a27-666d7a40f179 7906 0 2023-07-29 12:23:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc bf840810-716c-4970-ac7e-d49ac6708c62 0xc003e763c7 0xc003e763c8}] [] [{kube-controller-manager Update v1 2023-07-29 12:23:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf840810-716c-4970-ac7e-d49ac6708c62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cbz9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cbz9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:23:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:23:26.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1331" for this suite. @ 07/29/23 12:23:26.746
• [2.103 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 07/29/23 12:23:26.759
  Jul 29 12:23:26.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 12:23:26.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:23:26.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:23:26.789
  STEP: creating service in namespace services-6263 @ 07/29/23 12:23:26.791
  STEP: creating service affinity-clusterip in namespace services-6263 @ 07/29/23 12:23:26.791
  STEP: creating replication controller affinity-clusterip in namespace services-6263 @ 07/29/23 12:23:26.805
  I0729 12:23:26.813654      18 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-6263, replica count: 3
  I0729 12:23:29.865492      18 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 12:23:29.872: INFO: Creating new exec pod
  Jul 29 12:23:32.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6263 exec execpod-affinity7l47m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Jul 29 12:23:33.018: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jul 29 12:23:33.018: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 12:23:33.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6263 exec execpod-affinity7l47m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.29 80'
  Jul 29 12:23:33.140: INFO: stderr: "+ nc -v -t -w 2 10.152.183.29 80\n+ echo hostName\nConnection to 10.152.183.29 80 port [tcp/http] succeeded!\n"
  Jul 29 12:23:33.141: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 12:23:33.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-6263 exec execpod-affinity7l47m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.29:80/ ; done'
  Jul 29 12:23:33.339: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.29:80/\n"
  Jul 29 12:23:33.339: INFO: stdout: "\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l\naffinity-clusterip-xmd7l"
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Received response from host: affinity-clusterip-xmd7l
  Jul 29 12:23:33.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 12:23:33.343: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-6263, will wait for the garbage collector to delete the pods @ 07/29/23 12:23:33.353
  Jul 29 12:23:33.414: INFO: Deleting ReplicationController affinity-clusterip took: 7.393268ms
  Jul 29 12:23:33.515: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.127307ms
  STEP: Destroying namespace "services-6263" for this suite. @ 07/29/23 12:23:35.766
• [9.013 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 07/29/23 12:23:35.773
  Jul 29 12:23:35.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename sched-preemption @ 07/29/23 12:23:35.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:23:35.793
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:23:35.796
  Jul 29 12:23:35.812: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 12:24:35.829: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/29/23 12:24:35.833
  Jul 29 12:24:35.853: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul 29 12:24:35.862: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul 29 12:24:35.878: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul 29 12:24:35.890: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul 29 12:24:35.910: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul 29 12:24:35.918: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/29/23 12:24:35.918
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 07/29/23 12:24:37.942
  Jul 29 12:24:44.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-2842" for this suite. @ 07/29/23 12:24:44.061
• [68.295 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 07/29/23 12:24:44.072
  Jul 29 12:24:44.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 07/29/23 12:24:44.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:24:44.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:24:44.098
  STEP: creating a target pod @ 07/29/23 12:24:44.103
  STEP: adding an ephemeral container @ 07/29/23 12:24:46.131
  STEP: checking pod container endpoints @ 07/29/23 12:24:48.157
  Jul 29 12:24:48.157: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3279 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:24:48.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:24:48.158: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:24:48.158: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-3279/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Jul 29 12:24:48.217: INFO: Exec stderr: ""
  Jul 29 12:24:48.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-3279" for this suite. @ 07/29/23 12:24:48.237
• [4.171 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 07/29/23 12:24:48.244
  Jul 29 12:24:48.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 12:24:48.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:24:48.262
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:24:48.265
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 12:24:48.268
  STEP: Saw pod success @ 07/29/23 12:24:52.29
  Jul 29 12:24:52.294: INFO: Trying to get logs from node ip-172-31-19-67 pod downwardapi-volume-2ca98f53-6f31-4b76-a3ca-761f0dccdbf1 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 12:24:52.314
  Jul 29 12:24:52.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-606" for this suite. @ 07/29/23 12:24:52.332
• [4.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 07/29/23 12:24:52.343
  Jul 29 12:24:52.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 12:24:52.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:24:52.362
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:24:52.365
  STEP: Creating a ResourceQuota @ 07/29/23 12:24:52.367
  STEP: Getting a ResourceQuota @ 07/29/23 12:24:52.371
  STEP: Listing all ResourceQuotas with LabelSelector @ 07/29/23 12:24:52.375
  STEP: Patching the ResourceQuota @ 07/29/23 12:24:52.378
  STEP: Deleting a Collection of ResourceQuotas @ 07/29/23 12:24:52.383
  STEP: Verifying the deleted ResourceQuota @ 07/29/23 12:24:52.39
  Jul 29 12:24:52.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2492" for this suite. @ 07/29/23 12:24:52.398
• [0.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 07/29/23 12:24:52.406
  Jul 29 12:24:52.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename secrets @ 07/29/23 12:24:52.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:24:52.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:24:52.429
  STEP: Creating secret with name secret-test-3593aba9-3a48-48e3-a310-cb0484f21933 @ 07/29/23 12:24:52.459
  STEP: Creating a pod to test consume secrets @ 07/29/23 12:24:52.465
  STEP: Saw pod success @ 07/29/23 12:24:56.485
  Jul 29 12:24:56.488: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-secrets-fefdb56c-bb17-4514-b8c1-1d79d1c614f9 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 12:24:56.496
  Jul 29 12:24:56.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-272" for this suite. @ 07/29/23 12:24:56.515
  STEP: Destroying namespace "secret-namespace-5282" for this suite. @ 07/29/23 12:24:56.523
• [4.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 07/29/23 12:24:56.53
  Jul 29 12:24:56.530: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename csistoragecapacity @ 07/29/23 12:24:56.53
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:24:56.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:24:56.555
  STEP: getting /apis @ 07/29/23 12:24:56.558
  STEP: getting /apis/storage.k8s.io @ 07/29/23 12:24:56.561
  STEP: getting /apis/storage.k8s.io/v1 @ 07/29/23 12:24:56.562
  STEP: creating @ 07/29/23 12:24:56.564
  STEP: watching @ 07/29/23 12:24:56.584
  Jul 29 12:24:56.585: INFO: starting watch
  STEP: getting @ 07/29/23 12:24:56.592
  STEP: listing in namespace @ 07/29/23 12:24:56.596
  STEP: listing across namespaces @ 07/29/23 12:24:56.599
  STEP: patching @ 07/29/23 12:24:56.602
  STEP: updating @ 07/29/23 12:24:56.608
  Jul 29 12:24:56.612: INFO: waiting for watch events with expected annotations in namespace
  Jul 29 12:24:56.612: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 07/29/23 12:24:56.612
  STEP: deleting a collection @ 07/29/23 12:24:56.624
  Jul 29 12:24:56.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-965" for this suite. @ 07/29/23 12:24:56.643
• [0.118 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 07/29/23 12:24:56.649
  Jul 29 12:24:56.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename limitrange @ 07/29/23 12:24:56.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:24:56.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:24:56.671
  STEP: Creating a LimitRange @ 07/29/23 12:24:56.674
  STEP: Setting up watch @ 07/29/23 12:24:56.674
  STEP: Submitting a LimitRange @ 07/29/23 12:24:56.777
  STEP: Verifying LimitRange creation was observed @ 07/29/23 12:24:56.784
  STEP: Fetching the LimitRange to ensure it has proper values @ 07/29/23 12:24:56.784
  Jul 29 12:24:56.787: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul 29 12:24:56.787: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 07/29/23 12:24:56.787
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 07/29/23 12:24:56.793
  Jul 29 12:24:56.797: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul 29 12:24:56.797: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 07/29/23 12:24:56.797
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 07/29/23 12:24:56.802
  Jul 29 12:24:56.808: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jul 29 12:24:56.808: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 07/29/23 12:24:56.808
  STEP: Failing to create a Pod with more than max resources @ 07/29/23 12:24:56.81
  STEP: Updating a LimitRange @ 07/29/23 12:24:56.812
  STEP: Verifying LimitRange updating is effective @ 07/29/23 12:24:56.818
  STEP: Creating a Pod with less than former min resources @ 07/29/23 12:24:58.824
  STEP: Failing to create a Pod with more than max resources @ 07/29/23 12:24:58.83
  STEP: Deleting a LimitRange @ 07/29/23 12:24:58.832
  STEP: Verifying the LimitRange was deleted @ 07/29/23 12:24:58.842
  Jul 29 12:25:03.848: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 07/29/23 12:25:03.848
  Jul 29 12:25:03.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-3112" for this suite. @ 07/29/23 12:25:03.86
• [7.221 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 07/29/23 12:25:03.871
  Jul 29 12:25:03.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename job @ 07/29/23 12:25:03.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:25:03.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:25:03.893
  STEP: Creating a suspended job @ 07/29/23 12:25:03.902
  STEP: Patching the Job @ 07/29/23 12:25:03.91
  STEP: Watching for Job to be patched @ 07/29/23 12:25:03.918
  Jul 29 12:25:03.920: INFO: Event ADDED observed for Job e2e-tcjqw in namespace job-8983 with labels: map[e2e-job-label:e2e-tcjqw] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jul 29 12:25:03.920: INFO: Event MODIFIED found for Job e2e-tcjqw in namespace job-8983 with labels: map[e2e-job-label:e2e-tcjqw e2e-tcjqw:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 07/29/23 12:25:03.92
  STEP: Watching for Job to be updated @ 07/29/23 12:25:03.931
  Jul 29 12:25:03.933: INFO: Event MODIFIED found for Job e2e-tcjqw in namespace job-8983 with labels: map[e2e-job-label:e2e-tcjqw e2e-tcjqw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 12:25:03.933: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 07/29/23 12:25:03.933
  Jul 29 12:25:03.936: INFO: Job: e2e-tcjqw as labels: map[e2e-job-label:e2e-tcjqw e2e-tcjqw:patched]
  STEP: Waiting for job to complete @ 07/29/23 12:25:03.936
  STEP: Delete a job collection with a labelselector @ 07/29/23 12:25:13.942
  STEP: Watching for Job to be deleted @ 07/29/23 12:25:13.952
  Jul 29 12:25:13.954: INFO: Event MODIFIED observed for Job e2e-tcjqw in namespace job-8983 with labels: map[e2e-job-label:e2e-tcjqw e2e-tcjqw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 12:25:13.954: INFO: Event MODIFIED observed for Job e2e-tcjqw in namespace job-8983 with labels: map[e2e-job-label:e2e-tcjqw e2e-tcjqw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 12:25:13.954: INFO: Event MODIFIED observed for Job e2e-tcjqw in namespace job-8983 with labels: map[e2e-job-label:e2e-tcjqw e2e-tcjqw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 12:25:13.954: INFO: Event MODIFIED observed for Job e2e-tcjqw in namespace job-8983 with labels: map[e2e-job-label:e2e-tcjqw e2e-tcjqw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 12:25:13.955: INFO: Event MODIFIED observed for Job e2e-tcjqw in namespace job-8983 with labels: map[e2e-job-label:e2e-tcjqw e2e-tcjqw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 12:25:13.955: INFO: Event DELETED found for Job e2e-tcjqw in namespace job-8983 with labels: map[e2e-job-label:e2e-tcjqw e2e-tcjqw:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 07/29/23 12:25:13.955
  Jul 29 12:25:13.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8983" for this suite. @ 07/29/23 12:25:13.962
• [10.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 07/29/23 12:25:13.978
  Jul 29 12:25:13.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename ingressclass @ 07/29/23 12:25:13.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:25:13.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:25:14.002
  STEP: getting /apis @ 07/29/23 12:25:14.005
  STEP: getting /apis/networking.k8s.io @ 07/29/23 12:25:14.008
  STEP: getting /apis/networking.k8s.iov1 @ 07/29/23 12:25:14.009
  STEP: creating @ 07/29/23 12:25:14.01
  STEP: getting @ 07/29/23 12:25:14.024
  STEP: listing @ 07/29/23 12:25:14.028
  STEP: watching @ 07/29/23 12:25:14.032
  Jul 29 12:25:14.032: INFO: starting watch
  STEP: patching @ 07/29/23 12:25:14.033
  STEP: updating @ 07/29/23 12:25:14.039
  Jul 29 12:25:14.043: INFO: waiting for watch events with expected annotations
  Jul 29 12:25:14.043: INFO: saw patched and updated annotations
  STEP: deleting @ 07/29/23 12:25:14.044
  STEP: deleting a collection @ 07/29/23 12:25:14.06
  Jul 29 12:25:14.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-7405" for this suite. @ 07/29/23 12:25:14.078
• [0.106 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 07/29/23 12:25:14.085
  Jul 29 12:25:14.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename deployment @ 07/29/23 12:25:14.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:25:14.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:25:14.104
  STEP: creating a Deployment @ 07/29/23 12:25:14.111
  Jul 29 12:25:14.111: INFO: Creating simple deployment test-deployment-7pwcj
  Jul 29 12:25:14.125: INFO: deployment "test-deployment-7pwcj" doesn't have the required revision set
  STEP: Getting /status @ 07/29/23 12:25:16.141
  Jul 29 12:25:16.148: INFO: Deployment test-deployment-7pwcj has Conditions: [{Available True 2023-07-29 12:25:15 +0000 UTC 2023-07-29 12:25:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-07-29 12:25:15 +0000 UTC 2023-07-29 12:25:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7pwcj-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 07/29/23 12:25:16.149
  Jul 29 12:25:16.159: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 12, 25, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 25, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 12, 25, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 12, 25, 14, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-7pwcj-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 07/29/23 12:25:16.159
  Jul 29 12:25:16.161: INFO: Observed &Deployment event: ADDED
  Jul 29 12:25:16.161: INFO: Observed Deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 12:25:14 +0000 UTC 2023-07-29 12:25:14 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7pwcj-5994cf9475"}
  Jul 29 12:25:16.161: INFO: Observed &Deployment event: MODIFIED
  Jul 29 12:25:16.161: INFO: Observed Deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 12:25:14 +0000 UTC 2023-07-29 12:25:14 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7pwcj-5994cf9475"}
  Jul 29 12:25:16.161: INFO: Observed Deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 12:25:14 +0000 UTC 2023-07-29 12:25:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 29 12:25:16.161: INFO: Observed &Deployment event: MODIFIED
  Jul 29 12:25:16.161: INFO: Observed Deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 12:25:14 +0000 UTC 2023-07-29 12:25:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 29 12:25:16.161: INFO: Observed Deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 12:25:14 +0000 UTC 2023-07-29 12:25:14 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7pwcj-5994cf9475" is progressing.}
  Jul 29 12:25:16.162: INFO: Observed &Deployment event: MODIFIED
  Jul 29 12:25:16.162: INFO: Observed Deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 12:25:15 +0000 UTC 2023-07-29 12:25:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 29 12:25:16.162: INFO: Observed Deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 12:25:15 +0000 UTC 2023-07-29 12:25:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7pwcj-5994cf9475" has successfully progressed.}
  Jul 29 12:25:16.162: INFO: Observed &Deployment event: MODIFIED
  Jul 29 12:25:16.162: INFO: Observed Deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 12:25:15 +0000 UTC 2023-07-29 12:25:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 29 12:25:16.162: INFO: Observed Deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 12:25:15 +0000 UTC 2023-07-29 12:25:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7pwcj-5994cf9475" has successfully progressed.}
  Jul 29 12:25:16.162: INFO: Found Deployment test-deployment-7pwcj in namespace deployment-68 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 12:25:16.162: INFO: Deployment test-deployment-7pwcj has an updated status
  STEP: patching the Statefulset Status @ 07/29/23 12:25:16.162
  Jul 29 12:25:16.162: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 29 12:25:16.169: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 07/29/23 12:25:16.169
  Jul 29 12:25:16.170: INFO: Observed &Deployment event: ADDED
  Jul 29 12:25:16.170: INFO: Observed deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 12:25:14 +0000 UTC 2023-07-29 12:25:14 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7pwcj-5994cf9475"}
  Jul 29 12:25:16.170: INFO: Observed &Deployment event: MODIFIED
  Jul 29 12:25:16.170: INFO: Observed deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 12:25:14 +0000 UTC 2023-07-29 12:25:14 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7pwcj-5994cf9475"}
  Jul 29 12:25:16.170: INFO: Observed deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 12:25:14 +0000 UTC 2023-07-29 12:25:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 29 12:25:16.171: INFO: Observed &Deployment event: MODIFIED
  Jul 29 12:25:16.171: INFO: Observed deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 12:25:14 +0000 UTC 2023-07-29 12:25:14 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 29 12:25:16.171: INFO: Observed deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 12:25:14 +0000 UTC 2023-07-29 12:25:14 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7pwcj-5994cf9475" is progressing.}
  Jul 29 12:25:16.171: INFO: Observed &Deployment event: MODIFIED
  Jul 29 12:25:16.171: INFO: Observed deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 12:25:15 +0000 UTC 2023-07-29 12:25:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 29 12:25:16.171: INFO: Observed deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 12:25:15 +0000 UTC 2023-07-29 12:25:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7pwcj-5994cf9475" has successfully progressed.}
  Jul 29 12:25:16.171: INFO: Observed &Deployment event: MODIFIED
  Jul 29 12:25:16.171: INFO: Observed deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 12:25:15 +0000 UTC 2023-07-29 12:25:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 29 12:25:16.171: INFO: Observed deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 12:25:15 +0000 UTC 2023-07-29 12:25:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7pwcj-5994cf9475" has successfully progressed.}
  Jul 29 12:25:16.171: INFO: Observed deployment test-deployment-7pwcj in namespace deployment-68 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 12:25:16.172: INFO: Observed &Deployment event: MODIFIED
  Jul 29 12:25:16.172: INFO: Found deployment test-deployment-7pwcj in namespace deployment-68 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jul 29 12:25:16.172: INFO: Deployment test-deployment-7pwcj has a patched status
  Jul 29 12:25:16.177: INFO: Deployment "test-deployment-7pwcj":
  &Deployment{ObjectMeta:{test-deployment-7pwcj  deployment-68  28867f99-157c-4b58-91cf-6598479e30ac 8871 1 2023-07-29 12:25:14 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-07-29 12:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-07-29 12:25:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-07-29 12:25:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005255fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-7pwcj-5994cf9475",LastUpdateTime:2023-07-29 12:25:16 +0000 UTC,LastTransitionTime:2023-07-29 12:25:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 29 12:25:16.180: INFO: New ReplicaSet "test-deployment-7pwcj-5994cf9475" of Deployment "test-deployment-7pwcj":
  &ReplicaSet{ObjectMeta:{test-deployment-7pwcj-5994cf9475  deployment-68  d3674756-daf6-48c0-ba43-2be12a331e02 8867 1 2023-07-29 12:25:14 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-7pwcj 28867f99-157c-4b58-91cf-6598479e30ac 0xc003e74017 0xc003e74018}] [] [{kube-controller-manager Update apps/v1 2023-07-29 12:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28867f99-157c-4b58-91cf-6598479e30ac\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 12:25:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e740c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 12:25:16.183: INFO: Pod "test-deployment-7pwcj-5994cf9475-ndzq5" is available:
  &Pod{ObjectMeta:{test-deployment-7pwcj-5994cf9475-ndzq5 test-deployment-7pwcj-5994cf9475- deployment-68  afd95efa-a7d8-4e54-b98d-e98839847d3b 8866 0 2023-07-29 12:25:14 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-7pwcj-5994cf9475 d3674756-daf6-48c0-ba43-2be12a331e02 0xc003e74467 0xc003e74468}] [] [{kube-controller-manager Update v1 2023-07-29 12:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3674756-daf6-48c0-ba43-2be12a331e02\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:25:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.129.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dvg82,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dvg82,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:25:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:25:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:192.168.129.73,StartTime:2023-07-29 12:25:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 12:25:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f25a0f546de3f119910791d751ef33a41f76b0023d5f8549e3ee2f58bd451413,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.129.73,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:25:16.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-68" for this suite. @ 07/29/23 12:25:16.186
• [2.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 07/29/23 12:25:16.193
  Jul 29 12:25:16.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 12:25:16.194
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:25:16.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:25:16.238
  Jul 29 12:25:16.262: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 12:25:16.27
  Jul 29 12:25:16.275: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:16.275: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:16.278: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 12:25:16.278: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  Jul 29 12:25:17.282: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:17.283: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:17.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 12:25:17.285: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  Jul 29 12:25:18.287: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:18.287: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:18.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 12:25:18.292: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 07/29/23 12:25:18.308
  STEP: Check that daemon pods images are updated. @ 07/29/23 12:25:18.32
  Jul 29 12:25:18.327: INFO: Wrong image for pod: daemon-set-dj5kj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 12:25:18.327: INFO: Wrong image for pod: daemon-set-gf5fj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 12:25:18.327: INFO: Wrong image for pod: daemon-set-zmkh5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 12:25:18.331: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:18.331: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:19.335: INFO: Wrong image for pod: daemon-set-dj5kj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 12:25:19.336: INFO: Pod daemon-set-js9lm is not available
  Jul 29 12:25:19.336: INFO: Wrong image for pod: daemon-set-zmkh5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 12:25:19.339: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:19.339: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:20.335: INFO: Wrong image for pod: daemon-set-dj5kj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 12:25:20.335: INFO: Pod daemon-set-js9lm is not available
  Jul 29 12:25:20.335: INFO: Wrong image for pod: daemon-set-zmkh5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 12:25:20.338: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:20.338: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:21.335: INFO: Pod daemon-set-2s9f2 is not available
  Jul 29 12:25:21.336: INFO: Wrong image for pod: daemon-set-dj5kj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 12:25:21.339: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:21.339: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:22.335: INFO: Pod daemon-set-nv72l is not available
  Jul 29 12:25:22.339: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:22.339: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 07/29/23 12:25:22.339
  Jul 29 12:25:22.342: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:22.343: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:22.346: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 12:25:22.346: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  Jul 29 12:25:23.350: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:23.350: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:23.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 12:25:23.353: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  Jul 29 12:25:24.351: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:24.351: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:25:24.355: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 12:25:24.355: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 12:25:24.37
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3068, will wait for the garbage collector to delete the pods @ 07/29/23 12:25:24.37
  Jul 29 12:25:24.430: INFO: Deleting DaemonSet.extensions daemon-set took: 6.581695ms
  Jul 29 12:25:24.530: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.590615ms
  Jul 29 12:25:25.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 12:25:25.935: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 12:25:25.938: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9108"},"items":null}

  Jul 29 12:25:25.942: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9108"},"items":null}

  Jul 29 12:25:25.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3068" for this suite. @ 07/29/23 12:25:25.957
• [9.769 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 07/29/23 12:25:25.964
  Jul 29 12:25:25.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 12:25:25.964
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:25:25.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:25:25.985
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-1403 @ 07/29/23 12:25:25.988
  STEP: changing the ExternalName service to type=NodePort @ 07/29/23 12:25:25.992
  STEP: creating replication controller externalname-service in namespace services-1403 @ 07/29/23 12:25:26.013
  I0729 12:25:26.021157      18 runners.go:194] Created replication controller with name: externalname-service, namespace: services-1403, replica count: 2
  I0729 12:25:29.071926      18 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 12:25:29.071: INFO: Creating new exec pod
  Jul 29 12:25:32.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-1403 exec execpodrw5fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul 29 12:25:32.214: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 29 12:25:32.214: INFO: stdout: "externalname-service-kc5jz"
  Jul 29 12:25:32.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-1403 exec execpodrw5fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.126 80'
  Jul 29 12:25:32.344: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.126 80\nConnection to 10.152.183.126 80 port [tcp/http] succeeded!\n"
  Jul 29 12:25:32.344: INFO: stdout: ""
  Jul 29 12:25:33.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-1403 exec execpodrw5fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.126 80'
  Jul 29 12:25:33.481: INFO: stderr: "+ nc -v -t -w 2 10.152.183.126 80\n+ echo hostName\nConnection to 10.152.183.126 80 port [tcp/http] succeeded!\n"
  Jul 29 12:25:33.481: INFO: stdout: "externalname-service-kc5jz"
  Jul 29 12:25:33.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-1403 exec execpodrw5fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.5.66 31112'
  Jul 29 12:25:33.603: INFO: stderr: "+ nc -v -t -w 2 172.31.5.66 31112\n+ echo hostName\nConnection to 172.31.5.66 31112 port [tcp/*] succeeded!\n"
  Jul 29 12:25:33.603: INFO: stdout: "externalname-service-z6fd6"
  Jul 29 12:25:33.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-1403 exec execpodrw5fh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.19.67 31112'
  Jul 29 12:25:33.738: INFO: stderr: "+ nc -v -t -w 2 172.31.19.67 31112\n+ echo hostName\nConnection to 172.31.19.67 31112 port [tcp/*] succeeded!\n"
  Jul 29 12:25:33.738: INFO: stdout: "externalname-service-z6fd6"
  Jul 29 12:25:33.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 12:25:33.744: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-1403" for this suite. @ 07/29/23 12:25:33.765
• [7.808 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 07/29/23 12:25:33.772
  Jul 29 12:25:33.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 12:25:33.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:25:33.847
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:25:33.852
  STEP: Creating pod liveness-35208bb9-4bc3-43e7-95a8-a2c0288c4b08 in namespace container-probe-9900 @ 07/29/23 12:25:33.857
  Jul 29 12:25:35.881: INFO: Started pod liveness-35208bb9-4bc3-43e7-95a8-a2c0288c4b08 in namespace container-probe-9900
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 12:25:35.881
  Jul 29 12:25:35.884: INFO: Initial restart count of pod liveness-35208bb9-4bc3-43e7-95a8-a2c0288c4b08 is 0
  Jul 29 12:25:55.938: INFO: Restart count of pod container-probe-9900/liveness-35208bb9-4bc3-43e7-95a8-a2c0288c4b08 is now 1 (20.053737716s elapsed)
  Jul 29 12:26:15.980: INFO: Restart count of pod container-probe-9900/liveness-35208bb9-4bc3-43e7-95a8-a2c0288c4b08 is now 2 (40.095994854s elapsed)
  Jul 29 12:26:36.024: INFO: Restart count of pod container-probe-9900/liveness-35208bb9-4bc3-43e7-95a8-a2c0288c4b08 is now 3 (1m0.139700602s elapsed)
  Jul 29 12:26:56.067: INFO: Restart count of pod container-probe-9900/liveness-35208bb9-4bc3-43e7-95a8-a2c0288c4b08 is now 4 (1m20.182955383s elapsed)
  Jul 29 12:28:08.227: INFO: Restart count of pod container-probe-9900/liveness-35208bb9-4bc3-43e7-95a8-a2c0288c4b08 is now 5 (2m32.342577009s elapsed)
  Jul 29 12:28:08.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 12:28:08.231
  STEP: Destroying namespace "container-probe-9900" for this suite. @ 07/29/23 12:28:08.242
• [154.476 seconds]
------------------------------
SS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 07/29/23 12:28:08.249
  Jul 29 12:28:08.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename conformance-tests @ 07/29/23 12:28:08.25
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:28:08.267
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:28:08.27
  STEP: Getting node addresses @ 07/29/23 12:28:08.273
  Jul 29 12:28:08.273: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jul 29 12:28:08.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-5504" for this suite. @ 07/29/23 12:28:08.281
• [0.038 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 07/29/23 12:28:08.289
  Jul 29 12:28:08.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 12:28:08.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:28:08.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:28:08.311
  STEP: set up a multi version CRD @ 07/29/23 12:28:08.314
  Jul 29 12:28:08.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: mark a version not serverd @ 07/29/23 12:28:11.668
  STEP: check the unserved version gets removed @ 07/29/23 12:28:11.686
  STEP: check the other version is not changed @ 07/29/23 12:28:12.576
  Jul 29 12:28:15.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5804" for this suite. @ 07/29/23 12:28:15.186
• [6.904 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 07/29/23 12:28:15.194
  Jul 29 12:28:15.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 12:28:15.195
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:28:15.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:28:15.215
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 12:28:15.218
  STEP: Saw pod success @ 07/29/23 12:28:19.24
  Jul 29 12:28:19.244: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-4ed78c62-d436-40df-ab75-3e75d66324b8 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 12:28:19.261
  Jul 29 12:28:19.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3176" for this suite. @ 07/29/23 12:28:19.277
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 07/29/23 12:28:19.284
  Jul 29 12:28:19.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename secrets @ 07/29/23 12:28:19.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:28:19.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:28:19.307
  STEP: creating secret secrets-6455/secret-test-820c7ac8-34d4-459a-9b3a-8a973908ac05 @ 07/29/23 12:28:19.31
  STEP: Creating a pod to test consume secrets @ 07/29/23 12:28:19.315
  STEP: Saw pod success @ 07/29/23 12:28:23.335
  Jul 29 12:28:23.339: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-configmaps-8ffdd1ba-c98c-4e4f-a2f1-fbfd27f7b876 container env-test: <nil>
  STEP: delete the pod @ 07/29/23 12:28:23.345
  Jul 29 12:28:23.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6455" for this suite. @ 07/29/23 12:28:23.362
• [4.085 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 07/29/23 12:28:23.37
  Jul 29 12:28:23.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 12:28:23.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:28:23.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:28:23.393
  Jul 29 12:29:23.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-6536" for this suite. @ 07/29/23 12:29:23.411
• [60.047 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 07/29/23 12:29:23.417
  Jul 29 12:29:23.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 12:29:23.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:29:23.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:29:23.438
  STEP: Setting up server cert @ 07/29/23 12:29:23.464
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 12:29:23.824
  STEP: Deploying the webhook pod @ 07/29/23 12:29:23.835
  STEP: Wait for the deployment to be ready @ 07/29/23 12:29:23.847
  Jul 29 12:29:23.870: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 12:29:25.879
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 12:29:25.887
  Jul 29 12:29:26.887: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 29 12:29:26.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 07/29/23 12:29:27.398
  STEP: Creating a custom resource that should be denied by the webhook @ 07/29/23 12:29:27.415
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 07/29/23 12:29:29.441
  STEP: Updating the custom resource with disallowed data should be denied @ 07/29/23 12:29:29.448
  STEP: Deleting the custom resource should be denied @ 07/29/23 12:29:29.456
  STEP: Remove the offending key and value from the custom resource data @ 07/29/23 12:29:29.463
  STEP: Deleting the updated custom resource should be successful @ 07/29/23 12:29:29.475
  Jul 29 12:29:29.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2521" for this suite. @ 07/29/23 12:29:30.048
  STEP: Destroying namespace "webhook-markers-1769" for this suite. @ 07/29/23 12:29:30.058
• [6.647 seconds]
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 07/29/23 12:29:30.064
  Jul 29 12:29:30.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 12:29:30.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:29:30.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:29:30.083
  STEP: creating a Service @ 07/29/23 12:29:30.09
  STEP: watching for the Service to be added @ 07/29/23 12:29:30.1
  Jul 29 12:29:30.102: INFO: Found Service test-service-t8gv7 in namespace services-7961 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jul 29 12:29:30.102: INFO: Service test-service-t8gv7 created
  STEP: Getting /status @ 07/29/23 12:29:30.102
  Jul 29 12:29:30.106: INFO: Service test-service-t8gv7 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 07/29/23 12:29:30.106
  STEP: watching for the Service to be patched @ 07/29/23 12:29:30.112
  Jul 29 12:29:30.114: INFO: observed Service test-service-t8gv7 in namespace services-7961 with annotations: map[] & LoadBalancer: {[]}
  Jul 29 12:29:30.114: INFO: Found Service test-service-t8gv7 in namespace services-7961 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jul 29 12:29:30.114: INFO: Service test-service-t8gv7 has service status patched
  STEP: updating the ServiceStatus @ 07/29/23 12:29:30.114
  Jul 29 12:29:30.122: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 07/29/23 12:29:30.122
  Jul 29 12:29:30.124: INFO: Observed Service test-service-t8gv7 in namespace services-7961 with annotations: map[] & Conditions: {[]}
  Jul 29 12:29:30.124: INFO: Observed event: &Service{ObjectMeta:{test-service-t8gv7  services-7961  aab9a7d0-a183-4143-9e4e-0c53668c64e6 10017 0 2023-07-29 12:29:30 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-07-29 12:29:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-07-29 12:29:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.160,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.160],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jul 29 12:29:30.124: INFO: Found Service test-service-t8gv7 in namespace services-7961 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 29 12:29:30.124: INFO: Service test-service-t8gv7 has service status updated
  STEP: patching the service @ 07/29/23 12:29:30.124
  STEP: watching for the Service to be patched @ 07/29/23 12:29:30.136
  Jul 29 12:29:30.138: INFO: observed Service test-service-t8gv7 in namespace services-7961 with labels: map[test-service-static:true]
  Jul 29 12:29:30.138: INFO: observed Service test-service-t8gv7 in namespace services-7961 with labels: map[test-service-static:true]
  Jul 29 12:29:30.138: INFO: observed Service test-service-t8gv7 in namespace services-7961 with labels: map[test-service-static:true]
  Jul 29 12:29:30.138: INFO: Found Service test-service-t8gv7 in namespace services-7961 with labels: map[test-service:patched test-service-static:true]
  Jul 29 12:29:30.138: INFO: Service test-service-t8gv7 patched
  STEP: deleting the service @ 07/29/23 12:29:30.138
  STEP: watching for the Service to be deleted @ 07/29/23 12:29:30.149
  Jul 29 12:29:30.150: INFO: Observed event: ADDED
  Jul 29 12:29:30.151: INFO: Observed event: MODIFIED
  Jul 29 12:29:30.151: INFO: Observed event: MODIFIED
  Jul 29 12:29:30.151: INFO: Observed event: MODIFIED
  Jul 29 12:29:30.151: INFO: Found Service test-service-t8gv7 in namespace services-7961 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jul 29 12:29:30.151: INFO: Service test-service-t8gv7 deleted
  Jul 29 12:29:30.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7961" for this suite. @ 07/29/23 12:29:30.154
• [0.095 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 07/29/23 12:29:30.16
  Jul 29 12:29:30.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 12:29:30.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:29:30.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:29:30.181
  STEP: Creating configMap with name configmap-test-upd-da686df1-1ce2-4b4d-a3be-0e648651664c @ 07/29/23 12:29:30.191
  STEP: Creating the pod @ 07/29/23 12:29:30.195
  STEP: Updating configmap configmap-test-upd-da686df1-1ce2-4b4d-a3be-0e648651664c @ 07/29/23 12:29:32.221
  STEP: waiting to observe update in volume @ 07/29/23 12:29:32.227
  Jul 29 12:30:36.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8247" for this suite. @ 07/29/23 12:30:36.504
• [66.350 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 07/29/23 12:30:36.51
  Jul 29 12:30:36.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 12:30:36.511
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:30:36.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:30:36.532
  STEP: Creating configMap with name cm-test-opt-del-2b6f25ad-34c8-4fc4-88d6-42e261364564 @ 07/29/23 12:30:36.539
  STEP: Creating configMap with name cm-test-opt-upd-87c7ed5c-65df-44cf-af93-48baa6c2b24a @ 07/29/23 12:30:36.543
  STEP: Creating the pod @ 07/29/23 12:30:36.548
  STEP: Deleting configmap cm-test-opt-del-2b6f25ad-34c8-4fc4-88d6-42e261364564 @ 07/29/23 12:30:38.587
  STEP: Updating configmap cm-test-opt-upd-87c7ed5c-65df-44cf-af93-48baa6c2b24a @ 07/29/23 12:30:38.593
  STEP: Creating configMap with name cm-test-opt-create-996b70c1-acbc-4b04-8689-cab9b0d475f6 @ 07/29/23 12:30:38.598
  STEP: waiting to observe update in volume @ 07/29/23 12:30:38.605
  Jul 29 12:31:40.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4186" for this suite. @ 07/29/23 12:31:40.919
• [64.415 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 07/29/23 12:31:40.925
  Jul 29 12:31:40.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 12:31:40.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:31:40.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:31:40.945
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/29/23 12:31:40.949
  Jul 29 12:31:40.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-5399 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Jul 29 12:31:41.019: INFO: stderr: ""
  Jul 29 12:31:41.019: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/29/23 12:31:41.019
  Jul 29 12:31:41.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-5399 delete pods e2e-test-httpd-pod'
  Jul 29 12:31:43.542: INFO: stderr: ""
  Jul 29 12:31:43.542: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 29 12:31:43.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5399" for this suite. @ 07/29/23 12:31:43.545
• [2.626 seconds]
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 07/29/23 12:31:43.551
  Jul 29 12:31:43.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename prestop @ 07/29/23 12:31:43.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:31:43.574
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:31:43.577
  STEP: Creating server pod server in namespace prestop-5428 @ 07/29/23 12:31:43.58
  STEP: Waiting for pods to come up. @ 07/29/23 12:31:43.588
  STEP: Creating tester pod tester in namespace prestop-5428 @ 07/29/23 12:31:45.598
  STEP: Deleting pre-stop pod @ 07/29/23 12:31:47.61
  Jul 29 12:31:52.624: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jul 29 12:31:52.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 07/29/23 12:31:52.628
  STEP: Destroying namespace "prestop-5428" for this suite. @ 07/29/23 12:31:52.642
• [9.097 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 07/29/23 12:31:52.65
  Jul 29 12:31:52.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 12:31:52.651
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:31:52.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:31:52.673
  STEP: Creating a test namespace @ 07/29/23 12:31:52.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:31:52.697
  STEP: Creating a service in the namespace @ 07/29/23 12:31:52.701
  STEP: Deleting the namespace @ 07/29/23 12:31:52.711
  STEP: Waiting for the namespace to be removed. @ 07/29/23 12:31:52.722
  STEP: Recreating the namespace @ 07/29/23 12:31:58.727
  STEP: Verifying there is no service in the namespace @ 07/29/23 12:31:58.741
  Jul 29 12:31:58.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-437" for this suite. @ 07/29/23 12:31:58.752
  STEP: Destroying namespace "nsdeletetest-1497" for this suite. @ 07/29/23 12:31:58.764
  Jul 29 12:31:58.767: INFO: Namespace nsdeletetest-1497 was already deleted
  STEP: Destroying namespace "nsdeletetest-882" for this suite. @ 07/29/23 12:31:58.767
• [6.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 07/29/23 12:31:58.776
  Jul 29 12:31:58.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 12:31:58.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:31:58.792
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:31:58.795
  Jul 29 12:31:58.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/29/23 12:32:00.175
  Jul 29 12:32:00.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-4169 --namespace=crd-publish-openapi-4169 create -f -'
  Jul 29 12:32:00.934: INFO: stderr: ""
  Jul 29 12:32:00.934: INFO: stdout: "e2e-test-crd-publish-openapi-2136-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul 29 12:32:00.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-4169 --namespace=crd-publish-openapi-4169 delete e2e-test-crd-publish-openapi-2136-crds test-cr'
  Jul 29 12:32:01.028: INFO: stderr: ""
  Jul 29 12:32:01.028: INFO: stdout: "e2e-test-crd-publish-openapi-2136-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jul 29 12:32:01.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-4169 --namespace=crd-publish-openapi-4169 apply -f -'
  Jul 29 12:32:01.361: INFO: stderr: ""
  Jul 29 12:32:01.361: INFO: stdout: "e2e-test-crd-publish-openapi-2136-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul 29 12:32:01.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-4169 --namespace=crd-publish-openapi-4169 delete e2e-test-crd-publish-openapi-2136-crds test-cr'
  Jul 29 12:32:01.430: INFO: stderr: ""
  Jul 29 12:32:01.430: INFO: stdout: "e2e-test-crd-publish-openapi-2136-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/29/23 12:32:01.43
  Jul 29 12:32:01.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-4169 explain e2e-test-crd-publish-openapi-2136-crds'
  Jul 29 12:32:02.023: INFO: stderr: ""
  Jul 29 12:32:02.023: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-2136-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Jul 29 12:32:03.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4169" for this suite. @ 07/29/23 12:32:03.451
• [4.681 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 07/29/23 12:32:03.457
  Jul 29 12:32:03.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 12:32:03.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:32:03.479
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:32:03.482
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 12:32:03.485
  STEP: Saw pod success @ 07/29/23 12:32:07.503
  Jul 29 12:32:07.506: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-7e7c5e2c-5770-4b5f-bd4a-8ccae9b80add container client-container: <nil>
  STEP: delete the pod @ 07/29/23 12:32:07.513
  Jul 29 12:32:07.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3613" for this suite. @ 07/29/23 12:32:07.532
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 07/29/23 12:32:07.541
  Jul 29 12:32:07.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename dns @ 07/29/23 12:32:07.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:32:07.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:32:07.562
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4090.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4090.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 07/29/23 12:32:07.565
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4090.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4090.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 07/29/23 12:32:07.565
  STEP: creating a pod to probe /etc/hosts @ 07/29/23 12:32:07.565
  STEP: submitting the pod to kubernetes @ 07/29/23 12:32:07.565
  STEP: retrieving the pod @ 07/29/23 12:32:09.587
  STEP: looking for the results for each expected name from probers @ 07/29/23 12:32:09.59
  Jul 29 12:32:09.607: INFO: DNS probes using dns-4090/dns-test-6121f7ca-6ba9-45b9-b903-006322a95702 succeeded

  Jul 29 12:32:09.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 12:32:09.611
  STEP: Destroying namespace "dns-4090" for this suite. @ 07/29/23 12:32:09.627
• [2.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 07/29/23 12:32:09.637
  Jul 29 12:32:09.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 12:32:09.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:32:09.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:32:09.661
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/29/23 12:32:09.664
  Jul 29 12:32:09.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-5341 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul 29 12:32:09.730: INFO: stderr: ""
  Jul 29 12:32:09.730: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 07/29/23 12:32:09.73
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/29/23 12:32:14.782
  Jul 29 12:32:14.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-5341 get pod e2e-test-httpd-pod -o json'
  Jul 29 12:32:14.842: INFO: stderr: ""
  Jul 29 12:32:14.842: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-07-29T12:32:09Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5341\",\n        \"resourceVersion\": \"10666\",\n        \"uid\": \"096f6403-9393-4e15-8530-c676caaad573\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5cz54\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-33-37\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5cz54\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T12:32:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T12:32:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T12:32:11Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T12:32:09Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://82090563877a859c38b27f7c5dc024efb272fbe62338b2f92a70af70d3a76c4e\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-07-29T12:32:10Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.33.37\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.129.87\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.129.87\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-07-29T12:32:09Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 07/29/23 12:32:14.843
  Jul 29 12:32:14.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-5341 replace -f -'
  Jul 29 12:32:15.367: INFO: stderr: ""
  Jul 29 12:32:15.367: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 07/29/23 12:32:15.367
  Jul 29 12:32:15.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-5341 delete pods e2e-test-httpd-pod'
  Jul 29 12:32:17.440: INFO: stderr: ""
  Jul 29 12:32:17.440: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 29 12:32:17.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5341" for this suite. @ 07/29/23 12:32:17.444
• [7.815 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 07/29/23 12:32:17.452
  Jul 29 12:32:17.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename discovery @ 07/29/23 12:32:17.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:32:17.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:32:17.471
  STEP: Setting up server cert @ 07/29/23 12:32:17.475
  Jul 29 12:32:18.136: INFO: Checking APIGroup: apiregistration.k8s.io
  Jul 29 12:32:18.137: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jul 29 12:32:18.137: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jul 29 12:32:18.137: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jul 29 12:32:18.137: INFO: Checking APIGroup: apps
  Jul 29 12:32:18.138: INFO: PreferredVersion.GroupVersion: apps/v1
  Jul 29 12:32:18.138: INFO: Versions found [{apps/v1 v1}]
  Jul 29 12:32:18.138: INFO: apps/v1 matches apps/v1
  Jul 29 12:32:18.138: INFO: Checking APIGroup: events.k8s.io
  Jul 29 12:32:18.139: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jul 29 12:32:18.140: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jul 29 12:32:18.140: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jul 29 12:32:18.140: INFO: Checking APIGroup: authentication.k8s.io
  Jul 29 12:32:18.140: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jul 29 12:32:18.141: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jul 29 12:32:18.141: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jul 29 12:32:18.141: INFO: Checking APIGroup: authorization.k8s.io
  Jul 29 12:32:18.141: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jul 29 12:32:18.141: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jul 29 12:32:18.141: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jul 29 12:32:18.141: INFO: Checking APIGroup: autoscaling
  Jul 29 12:32:18.142: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jul 29 12:32:18.142: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jul 29 12:32:18.142: INFO: autoscaling/v2 matches autoscaling/v2
  Jul 29 12:32:18.142: INFO: Checking APIGroup: batch
  Jul 29 12:32:18.143: INFO: PreferredVersion.GroupVersion: batch/v1
  Jul 29 12:32:18.143: INFO: Versions found [{batch/v1 v1}]
  Jul 29 12:32:18.143: INFO: batch/v1 matches batch/v1
  Jul 29 12:32:18.143: INFO: Checking APIGroup: certificates.k8s.io
  Jul 29 12:32:18.144: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jul 29 12:32:18.144: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jul 29 12:32:18.145: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jul 29 12:32:18.145: INFO: Checking APIGroup: networking.k8s.io
  Jul 29 12:32:18.146: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jul 29 12:32:18.146: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jul 29 12:32:18.146: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jul 29 12:32:18.146: INFO: Checking APIGroup: policy
  Jul 29 12:32:18.147: INFO: PreferredVersion.GroupVersion: policy/v1
  Jul 29 12:32:18.147: INFO: Versions found [{policy/v1 v1}]
  Jul 29 12:32:18.147: INFO: policy/v1 matches policy/v1
  Jul 29 12:32:18.147: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jul 29 12:32:18.148: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jul 29 12:32:18.148: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jul 29 12:32:18.148: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jul 29 12:32:18.148: INFO: Checking APIGroup: storage.k8s.io
  Jul 29 12:32:18.149: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jul 29 12:32:18.149: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jul 29 12:32:18.149: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jul 29 12:32:18.149: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jul 29 12:32:18.150: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jul 29 12:32:18.150: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jul 29 12:32:18.150: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jul 29 12:32:18.150: INFO: Checking APIGroup: apiextensions.k8s.io
  Jul 29 12:32:18.151: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jul 29 12:32:18.151: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jul 29 12:32:18.151: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jul 29 12:32:18.151: INFO: Checking APIGroup: scheduling.k8s.io
  Jul 29 12:32:18.152: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jul 29 12:32:18.152: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jul 29 12:32:18.152: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jul 29 12:32:18.152: INFO: Checking APIGroup: coordination.k8s.io
  Jul 29 12:32:18.153: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jul 29 12:32:18.153: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jul 29 12:32:18.153: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jul 29 12:32:18.153: INFO: Checking APIGroup: node.k8s.io
  Jul 29 12:32:18.154: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jul 29 12:32:18.154: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jul 29 12:32:18.154: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jul 29 12:32:18.154: INFO: Checking APIGroup: discovery.k8s.io
  Jul 29 12:32:18.155: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jul 29 12:32:18.155: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jul 29 12:32:18.155: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jul 29 12:32:18.155: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jul 29 12:32:18.156: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jul 29 12:32:18.156: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jul 29 12:32:18.156: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jul 29 12:32:18.156: INFO: Checking APIGroup: metrics.k8s.io
  Jul 29 12:32:18.157: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Jul 29 12:32:18.157: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Jul 29 12:32:18.157: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Jul 29 12:32:18.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-710" for this suite. @ 07/29/23 12:32:18.161
• [0.717 seconds]
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 07/29/23 12:32:18.169
  Jul 29 12:32:18.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename security-context-test @ 07/29/23 12:32:18.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:32:18.186
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:32:18.188
  Jul 29 12:32:22.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9502" for this suite. @ 07/29/23 12:32:22.212
• [4.049 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 07/29/23 12:32:22.219
  Jul 29 12:32:22.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 12:32:22.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:32:22.238
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:32:22.24
  Jul 29 12:32:22.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-2430 create -f -'
  Jul 29 12:32:22.919: INFO: stderr: ""
  Jul 29 12:32:22.919: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jul 29 12:32:22.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-2430 create -f -'
  Jul 29 12:32:23.176: INFO: stderr: ""
  Jul 29 12:32:23.176: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/29/23 12:32:23.176
  Jul 29 12:32:24.181: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 12:32:24.181: INFO: Found 0 / 1
  Jul 29 12:32:25.180: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 12:32:25.180: INFO: Found 1 / 1
  Jul 29 12:32:25.180: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul 29 12:32:25.184: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 12:32:25.184: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 29 12:32:25.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-2430 describe pod agnhost-primary-v5dkd'
  Jul 29 12:32:25.254: INFO: stderr: ""
  Jul 29 12:32:25.254: INFO: stdout: "Name:             agnhost-primary-v5dkd\nNamespace:        kubectl-2430\nPriority:         0\nService Account:  default\nNode:             ip-172-31-33-37/172.31.33.37\nStart Time:       Sat, 29 Jul 2023 12:32:22 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               192.168.129.89\nIPs:\n  IP:           192.168.129.89\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://9a46be1fb5fd0d43701503e373d1354ed59213839a9963fc3cff500f4634e036\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 29 Jul 2023 12:32:23 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cpncw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-cpncw:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-2430/agnhost-primary-v5dkd to ip-172-31-33-37\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
  Jul 29 12:32:25.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-2430 describe rc agnhost-primary'
  Jul 29 12:32:25.328: INFO: stderr: ""
  Jul 29 12:32:25.328: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2430\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-v5dkd\n"
  Jul 29 12:32:25.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-2430 describe service agnhost-primary'
  Jul 29 12:32:25.397: INFO: stderr: ""
  Jul 29 12:32:25.397: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2430\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.64\nIPs:               10.152.183.64\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.129.89:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jul 29 12:32:25.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-2430 describe node ip-172-31-18-12'
  Jul 29 12:32:25.491: INFO: stderr: ""
  Jul 29 12:32:25.491: INFO: stdout: "Name:               ip-172-31-18-12\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    juju-application=kubernetes-control-plane\n                    juju-charm=kubernetes-control-plane\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-18-12\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 29 Jul 2023 11:56:17 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-18-12\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 29 Jul 2023 12:32:19 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 29 Jul 2023 12:28:13 +0000   Sat, 29 Jul 2023 11:56:17 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 29 Jul 2023 12:28:13 +0000   Sat, 29 Jul 2023 11:56:17 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 29 Jul 2023 12:28:13 +0000   Sat, 29 Jul 2023 11:56:17 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 29 Jul 2023 12:28:13 +0000   Sat, 29 Jul 2023 11:56:37 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.18.12\n  Hostname:    ip-172-31-18-12\nCapacity:\n  cpu:                2\n  ephemeral-storage:  16069568Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16070192Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  14809713845\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15967792Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ec22b8ed0092137fee1e3b22b53a18ee\n  System UUID:                ec22b8ed-0092-137f-ee1e-3b22b53a18ee\n  Boot ID:                    e6c06b5e-9c91-4760-85fe-ab5fb8b81a4c\n  Kernel Version:             5.19.0-1029-aws\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.27.4\n  Kube-Proxy Version:         v1.27.4\nNon-terminated Pods:          (1 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-nnr2z    0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests  Limits\n  --------           --------  ------\n  cpu                0 (0%)    0 (0%)\n  memory             0 (0%)    0 (0%)\n  ephemeral-storage  0 (0%)    0 (0%)\n  hugepages-1Gi      0 (0%)    0 (0%)\n  hugepages-2Mi      0 (0%)    0 (0%)\nEvents:\n  Type    Reason          Age   From             Message\n  ----    ------          ----  ----             -------\n  Normal  RegisteredNode  35m   node-controller  Node ip-172-31-18-12 event: Registered Node ip-172-31-18-12 in Controller\n"
  Jul 29 12:32:25.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-2430 describe namespace kubectl-2430'
  Jul 29 12:32:25.567: INFO: stderr: ""
  Jul 29 12:32:25.567: INFO: stdout: "Name:         kubectl-2430\nLabels:       e2e-framework=kubectl\n              e2e-run=b54c8ec5-3416-4458-8e59-a1ab1dbd9640\n              kubernetes.io/metadata.name=kubectl-2430\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jul 29 12:32:25.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2430" for this suite. @ 07/29/23 12:32:25.57
• [3.358 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 07/29/23 12:32:25.577
  Jul 29 12:32:25.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 12:32:25.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:32:25.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:32:25.601
  Jul 29 12:32:25.628: INFO: Create a RollingUpdate DaemonSet
  Jul 29 12:32:25.632: INFO: Check that daemon pods launch on every node of the cluster
  Jul 29 12:32:25.635: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:32:25.635: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:32:25.640: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 12:32:25.640: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  Jul 29 12:32:26.645: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:32:26.645: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:32:26.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 12:32:26.649: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Jul 29 12:32:26.649: INFO: Update the DaemonSet to trigger a rollout
  Jul 29 12:32:26.661: INFO: Updating DaemonSet daemon-set
  Jul 29 12:32:28.679: INFO: Roll back the DaemonSet before rollout is complete
  Jul 29 12:32:28.691: INFO: Updating DaemonSet daemon-set
  Jul 29 12:32:28.691: INFO: Make sure DaemonSet rollback is complete
  Jul 29 12:32:28.695: INFO: Wrong image for pod: daemon-set-2nxs5. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jul 29 12:32:28.695: INFO: Pod daemon-set-2nxs5 is not available
  Jul 29 12:32:28.700: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:32:28.700: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:32:29.709: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:32:29.709: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:32:30.704: INFO: Pod daemon-set-tbt88 is not available
  Jul 29 12:32:30.709: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:32:30.709: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 12:32:30.717
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5438, will wait for the garbage collector to delete the pods @ 07/29/23 12:32:30.717
  Jul 29 12:32:30.776: INFO: Deleting DaemonSet.extensions daemon-set took: 6.602563ms
  Jul 29 12:32:30.876: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.239215ms
  Jul 29 12:32:32.679: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 12:32:32.680: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 12:32:32.682: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11004"},"items":null}

  Jul 29 12:32:32.685: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11004"},"items":null}

  Jul 29 12:32:32.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5438" for this suite. @ 07/29/23 12:32:32.7
• [7.133 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 07/29/23 12:32:32.711
  Jul 29 12:32:32.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 12:32:32.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:32:32.729
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:32:32.732
  STEP: creating service in namespace services-185 @ 07/29/23 12:32:32.736
  STEP: creating service affinity-nodeport-transition in namespace services-185 @ 07/29/23 12:32:32.736
  STEP: creating replication controller affinity-nodeport-transition in namespace services-185 @ 07/29/23 12:32:32.758
  I0729 12:32:32.769666      18 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-185, replica count: 3
  I0729 12:32:35.821346      18 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 12:32:35.848: INFO: Creating new exec pod
  Jul 29 12:32:38.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-185 exec execpod-affinitykc5zj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jul 29 12:32:39.014: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jul 29 12:32:39.015: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 12:32:39.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-185 exec execpod-affinitykc5zj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.104 80'
  Jul 29 12:32:39.137: INFO: stderr: "+ nc -v -t -w 2 10.152.183.104 80\n+ echo hostName\nConnection to 10.152.183.104 80 port [tcp/http] succeeded!\n"
  Jul 29 12:32:39.137: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 12:32:39.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-185 exec execpod-affinitykc5zj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.5.66 30706'
  Jul 29 12:32:39.262: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.5.66 30706\nConnection to 172.31.5.66 30706 port [tcp/*] succeeded!\n"
  Jul 29 12:32:39.262: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 12:32:39.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-185 exec execpod-affinitykc5zj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.19.67 30706'
  Jul 29 12:32:39.382: INFO: stderr: "+ nc -v -t -w 2 172.31.19.67 30706\n+ echo hostName\nConnection to 172.31.19.67 30706 port [tcp/*] succeeded!\n"
  Jul 29 12:32:39.382: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 12:32:39.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-185 exec execpod-affinitykc5zj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.19.67:30706/ ; done'
  Jul 29 12:32:39.625: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n"
  Jul 29 12:32:39.625: INFO: stdout: "\naffinity-nodeport-transition-j5m59\naffinity-nodeport-transition-dncbs\naffinity-nodeport-transition-dncbs\naffinity-nodeport-transition-dncbs\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-j5m59\naffinity-nodeport-transition-j5m59\naffinity-nodeport-transition-j5m59\naffinity-nodeport-transition-dncbs\naffinity-nodeport-transition-dncbs\naffinity-nodeport-transition-dncbs\naffinity-nodeport-transition-dncbs\naffinity-nodeport-transition-dncbs\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-j5m59\naffinity-nodeport-transition-dncbs"
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-j5m59
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-dncbs
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-dncbs
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-dncbs
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-j5m59
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-j5m59
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-j5m59
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-dncbs
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-dncbs
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-dncbs
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-dncbs
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-dncbs
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-j5m59
  Jul 29 12:32:39.625: INFO: Received response from host: affinity-nodeport-transition-dncbs
  Jul 29 12:32:39.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-185 exec execpod-affinitykc5zj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.19.67:30706/ ; done'
  Jul 29 12:32:39.843: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:30706/\n"
  Jul 29 12:32:39.843: INFO: stdout: "\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq\naffinity-nodeport-transition-nd5pq"
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Received response from host: affinity-nodeport-transition-nd5pq
  Jul 29 12:32:39.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 12:32:39.848: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-185, will wait for the garbage collector to delete the pods @ 07/29/23 12:32:39.859
  Jul 29 12:32:39.919: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.855043ms
  Jul 29 12:32:40.019: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.08837ms
  STEP: Destroying namespace "services-185" for this suite. @ 07/29/23 12:32:41.743
• [9.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 07/29/23 12:32:41.749
  Jul 29 12:32:41.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 12:32:41.75
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:32:41.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:32:41.772
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 07/29/23 12:32:41.775
  Jul 29 12:32:41.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:32:43.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:32:48.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5532" for this suite. @ 07/29/23 12:32:48.531
• [6.790 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 07/29/23 12:32:48.54
  Jul 29 12:32:48.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 12:32:48.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:32:48.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:32:48.563
  Jul 29 12:32:48.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  W0729 12:32:51.117753      18 warnings.go:70] unknown field "alpha"
  W0729 12:32:51.117773      18 warnings.go:70] unknown field "beta"
  W0729 12:32:51.117779      18 warnings.go:70] unknown field "delta"
  W0729 12:32:51.117785      18 warnings.go:70] unknown field "epsilon"
  W0729 12:32:51.117791      18 warnings.go:70] unknown field "gamma"
  Jul 29 12:32:51.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7216" for this suite. @ 07/29/23 12:32:51.679
• [3.145 seconds]
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 07/29/23 12:32:51.686
  Jul 29 12:32:51.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 12:32:51.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:32:51.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:32:51.708
  Jul 29 12:32:55.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1246" for this suite. @ 07/29/23 12:32:55.732
• [4.053 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 07/29/23 12:32:55.739
  Jul 29 12:32:55.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename crd-watch @ 07/29/23 12:32:55.74
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:32:55.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:32:55.76
  Jul 29 12:32:55.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Creating first CR  @ 07/29/23 12:32:58.313
  Jul 29 12:32:58.318: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T12:32:58Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T12:32:58Z]] name:name1 resourceVersion:11327 uid:f761efa7-e34d-4bc4-b0b9-970fe6e2b8c5] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 07/29/23 12:33:08.319
  Jul 29 12:33:08.324: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T12:33:08Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T12:33:08Z]] name:name2 resourceVersion:11359 uid:bb662ea1-c7af-40ad-92f1-04632eb1008b] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 07/29/23 12:33:18.326
  Jul 29 12:33:18.332: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T12:32:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T12:33:18Z]] name:name1 resourceVersion:11380 uid:f761efa7-e34d-4bc4-b0b9-970fe6e2b8c5] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 07/29/23 12:33:28.333
  Jul 29 12:33:28.339: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T12:33:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T12:33:28Z]] name:name2 resourceVersion:11401 uid:bb662ea1-c7af-40ad-92f1-04632eb1008b] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 07/29/23 12:33:38.34
  Jul 29 12:33:38.349: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T12:32:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T12:33:18Z]] name:name1 resourceVersion:11421 uid:f761efa7-e34d-4bc4-b0b9-970fe6e2b8c5] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 07/29/23 12:33:48.351
  Jul 29 12:33:48.358: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T12:33:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T12:33:28Z]] name:name2 resourceVersion:11441 uid:bb662ea1-c7af-40ad-92f1-04632eb1008b] num:map[num1:9223372036854775807 num2:1000000]]}
  Jul 29 12:33:58.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-8190" for this suite. @ 07/29/23 12:33:58.879
• [63.149 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 07/29/23 12:33:58.89
  Jul 29 12:33:58.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 12:33:58.891
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:33:58.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:33:58.911
  STEP: Creating service test in namespace statefulset-2486 @ 07/29/23 12:33:58.919
  STEP: Creating a new StatefulSet @ 07/29/23 12:33:58.923
  Jul 29 12:33:58.937: INFO: Found 0 stateful pods, waiting for 3
  Jul 29 12:34:08.942: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 12:34:08.942: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 12:34:08.943: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/29/23 12:34:08.953
  Jul 29 12:34:08.972: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/29/23 12:34:08.972
  STEP: Not applying an update when the partition is greater than the number of replicas @ 07/29/23 12:34:18.987
  STEP: Performing a canary update @ 07/29/23 12:34:18.987
  Jul 29 12:34:19.008: INFO: Updating stateful set ss2
  Jul 29 12:34:19.020: INFO: Waiting for Pod statefulset-2486/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 07/29/23 12:34:29.03
  Jul 29 12:34:29.071: INFO: Found 1 stateful pods, waiting for 3
  Jul 29 12:34:39.078: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 12:34:39.078: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 12:34:39.078: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 07/29/23 12:34:39.087
  Jul 29 12:34:39.115: INFO: Updating stateful set ss2
  Jul 29 12:34:39.137: INFO: Waiting for Pod statefulset-2486/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jul 29 12:34:49.174: INFO: Updating stateful set ss2
  Jul 29 12:34:49.184: INFO: Waiting for StatefulSet statefulset-2486/ss2 to complete update
  Jul 29 12:34:49.184: INFO: Waiting for Pod statefulset-2486/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jul 29 12:34:59.193: INFO: Deleting all statefulset in ns statefulset-2486
  Jul 29 12:34:59.197: INFO: Scaling statefulset ss2 to 0
  Jul 29 12:35:09.214: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 12:35:09.217: INFO: Deleting statefulset ss2
  Jul 29 12:35:09.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2486" for this suite. @ 07/29/23 12:35:09.236
• [70.354 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 07/29/23 12:35:09.244
  Jul 29 12:35:09.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 12:35:09.245
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:35:09.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:35:09.27
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 07/29/23 12:35:09.273
  Jul 29 12:35:09.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 07/29/23 12:35:14.557
  Jul 29 12:35:14.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:35:15.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:35:21.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4792" for this suite. @ 07/29/23 12:35:21.314
• [12.077 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 07/29/23 12:35:21.322
  Jul 29 12:35:21.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 12:35:21.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:35:21.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:35:21.344
  STEP: Creating a pod to test downward api env vars @ 07/29/23 12:35:21.347
  STEP: Saw pod success @ 07/29/23 12:35:25.369
  Jul 29 12:35:25.373: INFO: Trying to get logs from node ip-172-31-33-37 pod downward-api-3b1639de-18b2-4b36-af0f-57d180ffd884 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 12:35:25.392
  Jul 29 12:35:25.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7599" for this suite. @ 07/29/23 12:35:25.409
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 07/29/23 12:35:25.417
  Jul 29 12:35:25.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 12:35:25.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:35:25.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:35:25.44
  STEP: Creating a pod to test service account token:  @ 07/29/23 12:35:25.442
  STEP: Saw pod success @ 07/29/23 12:35:29.463
  Jul 29 12:35:29.466: INFO: Trying to get logs from node ip-172-31-33-37 pod test-pod-95108290-9949-497b-99ee-2a7b3fdfe021 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 12:35:29.473
  Jul 29 12:35:29.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4400" for this suite. @ 07/29/23 12:35:29.494
• [4.084 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 07/29/23 12:35:29.502
  Jul 29 12:35:29.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 12:35:29.503
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:35:29.52
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:35:29.523
  STEP: Creating pod busybox-6bcf70bc-758e-4c52-9e87-d5056bf9f87f in namespace container-probe-6017 @ 07/29/23 12:35:29.526
  Jul 29 12:35:31.543: INFO: Started pod busybox-6bcf70bc-758e-4c52-9e87-d5056bf9f87f in namespace container-probe-6017
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 12:35:31.543
  Jul 29 12:35:31.547: INFO: Initial restart count of pod busybox-6bcf70bc-758e-4c52-9e87-d5056bf9f87f is 0
  Jul 29 12:36:21.669: INFO: Restart count of pod container-probe-6017/busybox-6bcf70bc-758e-4c52-9e87-d5056bf9f87f is now 1 (50.12156544s elapsed)
  Jul 29 12:36:21.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 12:36:21.675
  STEP: Destroying namespace "container-probe-6017" for this suite. @ 07/29/23 12:36:21.69
• [52.195 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 07/29/23 12:36:21.699
  Jul 29 12:36:21.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 12:36:21.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:36:21.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:36:21.735
  STEP: Creating configMap with name projected-configmap-test-volume-map-e305ddf7-00db-4f17-bacd-eb8386feb2c5 @ 07/29/23 12:36:21.747
  STEP: Creating a pod to test consume configMaps @ 07/29/23 12:36:21.754
  STEP: Saw pod success @ 07/29/23 12:36:25.781
  Jul 29 12:36:25.785: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-projected-configmaps-21373c9b-be64-4ec1-a235-92f37639d529 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 12:36:25.791
  Jul 29 12:36:25.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-803" for this suite. @ 07/29/23 12:36:25.813
• [4.121 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 07/29/23 12:36:25.821
  Jul 29 12:36:25.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 12:36:25.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:36:25.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:36:25.845
  STEP: creating service in namespace services-3359 @ 07/29/23 12:36:25.848
  STEP: creating service affinity-nodeport in namespace services-3359 @ 07/29/23 12:36:25.848
  STEP: creating replication controller affinity-nodeport in namespace services-3359 @ 07/29/23 12:36:25.862
  I0729 12:36:25.869508      18 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-3359, replica count: 3
  I0729 12:36:28.920897      18 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 12:36:28.929: INFO: Creating new exec pod
  Jul 29 12:36:31.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-3359 exec execpod-affinitymt6m8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Jul 29 12:36:32.074: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jul 29 12:36:32.074: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 12:36:32.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-3359 exec execpod-affinitymt6m8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.30 80'
  Jul 29 12:36:32.196: INFO: stderr: "+ nc -v -t -w 2 10.152.183.30 80\n+ echo hostName\nConnection to 10.152.183.30 80 port [tcp/http] succeeded!\n"
  Jul 29 12:36:32.196: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 12:36:32.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-3359 exec execpod-affinitymt6m8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.33.37 31651'
  Jul 29 12:36:32.324: INFO: stderr: "+ nc -v -t -w 2 172.31.33.37 31651\n+ echo hostName\nConnection to 172.31.33.37 31651 port [tcp/*] succeeded!\n"
  Jul 29 12:36:32.325: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 12:36:32.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-3359 exec execpod-affinitymt6m8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.19.67 31651'
  Jul 29 12:36:32.450: INFO: stderr: "+ nc -v -t -w 2 172.31.19.67 31651\n+ echo hostName\nConnection to 172.31.19.67 31651 port [tcp/*] succeeded!\n"
  Jul 29 12:36:32.450: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 12:36:32.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-3359 exec execpod-affinitymt6m8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.19.67:31651/ ; done'
  Jul 29 12:36:32.681: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.19.67:31651/\n"
  Jul 29 12:36:32.681: INFO: stdout: "\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698\naffinity-nodeport-2v698"
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Received response from host: affinity-nodeport-2v698
  Jul 29 12:36:32.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 12:36:32.686: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-3359, will wait for the garbage collector to delete the pods @ 07/29/23 12:36:32.699
  Jul 29 12:36:32.760: INFO: Deleting ReplicationController affinity-nodeport took: 6.584338ms
  Jul 29 12:36:32.860: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.107495ms
  STEP: Destroying namespace "services-3359" for this suite. @ 07/29/23 12:36:35.181
• [9.368 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 07/29/23 12:36:35.189
  Jul 29 12:36:35.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 12:36:35.19
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:36:35.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:36:35.211
  STEP: Creating pod test-webserver-e3742112-de54-4349-b00e-f74dd7a6ae11 in namespace container-probe-6898 @ 07/29/23 12:36:35.214
  Jul 29 12:36:37.229: INFO: Started pod test-webserver-e3742112-de54-4349-b00e-f74dd7a6ae11 in namespace container-probe-6898
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 12:36:37.229
  Jul 29 12:36:37.233: INFO: Initial restart count of pod test-webserver-e3742112-de54-4349-b00e-f74dd7a6ae11 is 0
  Jul 29 12:40:37.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 12:40:37.768
  STEP: Destroying namespace "container-probe-6898" for this suite. @ 07/29/23 12:40:37.781
• [242.599 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 07/29/23 12:40:37.788
  Jul 29 12:40:37.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 12:40:37.789
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:40:37.808
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:40:37.811
  STEP: Creating a pod to test substitution in volume subpath @ 07/29/23 12:40:37.814
  STEP: Saw pod success @ 07/29/23 12:40:41.837
  Jul 29 12:40:41.840: INFO: Trying to get logs from node ip-172-31-33-37 pod var-expansion-2a84d588-47c4-4183-9b9a-6c7fc33407e1 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 12:40:41.858
  Jul 29 12:40:41.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5485" for this suite. @ 07/29/23 12:40:41.879
• [4.109 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 07/29/23 12:40:41.898
  Jul 29 12:40:41.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 12:40:41.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:40:41.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:40:41.92
  STEP: Creating service test in namespace statefulset-640 @ 07/29/23 12:40:41.923
  STEP: Creating statefulset ss in namespace statefulset-640 @ 07/29/23 12:40:41.928
  Jul 29 12:40:41.940: INFO: Found 0 stateful pods, waiting for 1
  Jul 29 12:40:51.945: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 07/29/23 12:40:51.952
  STEP: updating a scale subresource @ 07/29/23 12:40:51.954
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/29/23 12:40:51.961
  STEP: Patch a scale subresource @ 07/29/23 12:40:51.965
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/29/23 12:40:51.971
  Jul 29 12:40:51.978: INFO: Deleting all statefulset in ns statefulset-640
  Jul 29 12:40:51.983: INFO: Scaling statefulset ss to 0
  Jul 29 12:41:02.004: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 12:41:02.008: INFO: Deleting statefulset ss
  Jul 29 12:41:02.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-640" for this suite. @ 07/29/23 12:41:02.025
• [20.134 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 07/29/23 12:41:02.033
  Jul 29 12:41:02.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename job @ 07/29/23 12:41:02.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:41:02.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:41:02.056
  STEP: Creating Indexed job @ 07/29/23 12:41:02.058
  STEP: Ensuring job reaches completions @ 07/29/23 12:41:02.068
  STEP: Ensuring pods with index for job exist @ 07/29/23 12:41:10.072
  Jul 29 12:41:10.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8539" for this suite. @ 07/29/23 12:41:10.08
• [8.052 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 07/29/23 12:41:10.087
  Jul 29 12:41:10.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/29/23 12:41:10.088
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:41:10.104
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:41:10.107
  STEP: create the container to handle the HTTPGet hook request. @ 07/29/23 12:41:10.115
  STEP: create the pod with lifecycle hook @ 07/29/23 12:41:12.135
  STEP: delete the pod with lifecycle hook @ 07/29/23 12:41:14.156
  STEP: check prestop hook @ 07/29/23 12:41:16.176
  Jul 29 12:41:16.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-2200" for this suite. @ 07/29/23 12:41:16.205
• [6.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 07/29/23 12:41:16.216
  Jul 29 12:41:16.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 12:41:16.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:41:16.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:41:16.239
  STEP: create the container @ 07/29/23 12:41:16.245
  W0729 12:41:16.343843      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/29/23 12:41:16.344
  STEP: get the container status @ 07/29/23 12:41:19.363
  STEP: the container should be terminated @ 07/29/23 12:41:19.368
  STEP: the termination message should be set @ 07/29/23 12:41:19.368
  Jul 29 12:41:19.368: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/29/23 12:41:19.368
  Jul 29 12:41:19.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9385" for this suite. @ 07/29/23 12:41:19.391
• [3.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 07/29/23 12:41:19.401
  Jul 29 12:41:19.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename sysctl @ 07/29/23 12:41:19.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:41:19.424
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:41:19.43
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 07/29/23 12:41:19.434
  STEP: Watching for error events or started pod @ 07/29/23 12:41:19.444
  STEP: Waiting for pod completion @ 07/29/23 12:41:21.449
  STEP: Checking that the pod succeeded @ 07/29/23 12:41:23.461
  STEP: Getting logs from the pod @ 07/29/23 12:41:23.461
  STEP: Checking that the sysctl is actually updated @ 07/29/23 12:41:23.468
  Jul 29 12:41:23.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-8390" for this suite. @ 07/29/23 12:41:23.474
• [4.081 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 07/29/23 12:41:23.483
  Jul 29 12:41:23.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 12:41:23.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:41:23.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:41:23.507
  STEP: Setting up server cert @ 07/29/23 12:41:23.547
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 12:41:24.096
  STEP: Deploying the webhook pod @ 07/29/23 12:41:24.103
  STEP: Wait for the deployment to be ready @ 07/29/23 12:41:24.116
  Jul 29 12:41:24.123: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 12:41:26.135
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 12:41:26.145
  Jul 29 12:41:27.145: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 29 12:41:27.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6791-crds.webhook.example.com via the AdmissionRegistration API @ 07/29/23 12:41:27.663
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/29/23 12:41:27.684
  Jul 29 12:41:29.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4595" for this suite. @ 07/29/23 12:41:30.331
  STEP: Destroying namespace "webhook-markers-1774" for this suite. @ 07/29/23 12:41:30.348
• [6.877 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 07/29/23 12:41:30.361
  Jul 29 12:41:30.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename init-container @ 07/29/23 12:41:30.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:41:30.388
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:41:30.393
  STEP: creating the pod @ 07/29/23 12:41:30.399
  Jul 29 12:41:30.399: INFO: PodSpec: initContainers in spec.initContainers
  Jul 29 12:41:33.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8648" for this suite. @ 07/29/23 12:41:33.822
• [3.468 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 07/29/23 12:41:33.83
  Jul 29 12:41:33.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 12:41:33.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:41:33.847
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:41:33.85
  STEP: Creating ServiceAccount "e2e-sa-8hbls"  @ 07/29/23 12:41:33.852
  Jul 29 12:41:33.856: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-8hbls"  @ 07/29/23 12:41:33.856
  Jul 29 12:41:33.864: INFO: AutomountServiceAccountToken: true
  Jul 29 12:41:33.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8361" for this suite. @ 07/29/23 12:41:33.869
• [0.045 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 07/29/23 12:41:33.875
  Jul 29 12:41:33.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 12:41:33.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:41:33.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:41:33.896
  STEP: create the container @ 07/29/23 12:41:33.899
  W0729 12:41:33.909311      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/29/23 12:41:33.909
  STEP: get the container status @ 07/29/23 12:41:36.928
  STEP: the container should be terminated @ 07/29/23 12:41:36.931
  STEP: the termination message should be set @ 07/29/23 12:41:36.931
  Jul 29 12:41:36.931: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 07/29/23 12:41:36.931
  Jul 29 12:41:36.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3684" for this suite. @ 07/29/23 12:41:36.949
• [3.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 07/29/23 12:41:36.959
  Jul 29 12:41:36.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename disruption @ 07/29/23 12:41:36.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:41:36.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:41:36.982
  STEP: Creating a pdb that targets all three pods in a test replica set @ 07/29/23 12:41:36.985
  STEP: Waiting for the pdb to be processed @ 07/29/23 12:41:36.99
  STEP: First trying to evict a pod which shouldn't be evictable @ 07/29/23 12:41:39.003
  STEP: Waiting for all pods to be running @ 07/29/23 12:41:39.003
  Jul 29 12:41:39.005: INFO: pods: 0 < 3
  STEP: locating a running pod @ 07/29/23 12:41:41.01
  STEP: Updating the pdb to allow a pod to be evicted @ 07/29/23 12:41:41.02
  STEP: Waiting for the pdb to be processed @ 07/29/23 12:41:41.029
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/29/23 12:41:43.036
  STEP: Waiting for all pods to be running @ 07/29/23 12:41:43.036
  STEP: Waiting for the pdb to observed all healthy pods @ 07/29/23 12:41:43.04
  STEP: Patching the pdb to disallow a pod to be evicted @ 07/29/23 12:41:43.066
  STEP: Waiting for the pdb to be processed @ 07/29/23 12:41:43.082
  STEP: Waiting for all pods to be running @ 07/29/23 12:41:45.091
  STEP: locating a running pod @ 07/29/23 12:41:45.094
  STEP: Deleting the pdb to allow a pod to be evicted @ 07/29/23 12:41:45.103
  STEP: Waiting for the pdb to be deleted @ 07/29/23 12:41:45.111
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/29/23 12:41:45.114
  STEP: Waiting for all pods to be running @ 07/29/23 12:41:45.114
  Jul 29 12:41:45.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6609" for this suite. @ 07/29/23 12:41:45.137
• [8.189 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 07/29/23 12:41:45.148
  Jul 29 12:41:45.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 12:41:45.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:41:45.222
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:41:45.226
  STEP: Setting up server cert @ 07/29/23 12:41:45.259
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 12:41:45.662
  STEP: Deploying the webhook pod @ 07/29/23 12:41:45.667
  STEP: Wait for the deployment to be ready @ 07/29/23 12:41:45.679
  Jul 29 12:41:45.686: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 12:41:47.696
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 12:41:47.705
  Jul 29 12:41:48.706: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 07/29/23 12:41:48.71
  STEP: create a configmap that should be updated by the webhook @ 07/29/23 12:41:48.726
  Jul 29 12:41:48.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6857" for this suite. @ 07/29/23 12:41:48.794
  STEP: Destroying namespace "webhook-markers-8737" for this suite. @ 07/29/23 12:41:48.803
• [3.664 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 07/29/23 12:41:48.814
  Jul 29 12:41:48.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 12:41:48.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:41:48.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:41:48.838
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/29/23 12:41:48.842
  STEP: Saw pod success @ 07/29/23 12:41:52.859
  Jul 29 12:41:52.863: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-ae92dada-cd3c-4579-8f8c-b99f4f734933 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 12:41:52.87
  Jul 29 12:41:52.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5316" for this suite. @ 07/29/23 12:41:52.89
• [4.082 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 07/29/23 12:41:52.897
  Jul 29 12:41:52.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 12:41:52.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:41:52.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:41:52.919
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 12:41:52.923
  STEP: Saw pod success @ 07/29/23 12:41:56.943
  Jul 29 12:41:56.947: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-531cc047-133d-4fcf-b641-910566e692fb container client-container: <nil>
  STEP: delete the pod @ 07/29/23 12:41:56.957
  Jul 29 12:41:56.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9248" for this suite. @ 07/29/23 12:41:56.978
• [4.087 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 07/29/23 12:41:56.985
  Jul 29 12:41:56.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 12:41:56.986
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:41:57.002
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:41:57.005
  STEP: Creating simple DaemonSet "daemon-set" @ 07/29/23 12:41:57.028
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 12:41:57.032
  Jul 29 12:41:57.037: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:41:57.037: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:41:57.041: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 12:41:57.041: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  Jul 29 12:41:58.045: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:41:58.045: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:41:58.049: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 12:41:58.049: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  Jul 29 12:41:59.046: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:41:59.046: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:41:59.050: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 12:41:59.050: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 07/29/23 12:41:59.053
  Jul 29 12:41:59.058: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 07/29/23 12:41:59.058
  Jul 29 12:41:59.067: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 07/29/23 12:41:59.067
  Jul 29 12:41:59.069: INFO: Observed &DaemonSet event: ADDED
  Jul 29 12:41:59.069: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 12:41:59.069: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 12:41:59.069: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 12:41:59.070: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 12:41:59.070: INFO: Found daemon set daemon-set in namespace daemonsets-8069 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 29 12:41:59.070: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 07/29/23 12:41:59.07
  STEP: watching for the daemon set status to be patched @ 07/29/23 12:41:59.079
  Jul 29 12:41:59.081: INFO: Observed &DaemonSet event: ADDED
  Jul 29 12:41:59.081: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 12:41:59.081: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 12:41:59.082: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 12:41:59.082: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 12:41:59.082: INFO: Observed daemon set daemon-set in namespace daemonsets-8069 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 29 12:41:59.082: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 12:41:59.082: INFO: Found daemon set daemon-set in namespace daemonsets-8069 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jul 29 12:41:59.082: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 12:41:59.085
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8069, will wait for the garbage collector to delete the pods @ 07/29/23 12:41:59.085
  Jul 29 12:41:59.145: INFO: Deleting DaemonSet.extensions daemon-set took: 6.199212ms
  Jul 29 12:41:59.246: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.49005ms
  Jul 29 12:42:00.749: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 12:42:00.749: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 12:42:00.752: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14067"},"items":null}

  Jul 29 12:42:00.756: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14067"},"items":null}

  Jul 29 12:42:00.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8069" for this suite. @ 07/29/23 12:42:00.772
• [3.794 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 07/29/23 12:42:00.781
  Jul 29 12:42:00.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename secrets @ 07/29/23 12:42:00.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:00.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:00.804
  STEP: Creating secret with name secret-test-map-64a5594f-cc79-41c1-a96b-cce61ac3204c @ 07/29/23 12:42:00.807
  STEP: Creating a pod to test consume secrets @ 07/29/23 12:42:00.812
  STEP: Saw pod success @ 07/29/23 12:42:04.836
  Jul 29 12:42:04.839: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-secrets-d14ea0de-38c5-4ddb-a0e7-4eca3616ccb8 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 12:42:04.847
  Jul 29 12:42:04.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6300" for this suite. @ 07/29/23 12:42:04.867
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 07/29/23 12:42:04.874
  Jul 29 12:42:04.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename containers @ 07/29/23 12:42:04.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:04.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:04.894
  Jul 29 12:42:06.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9653" for this suite. @ 07/29/23 12:42:06.922
• [2.055 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 07/29/23 12:42:06.929
  Jul 29 12:42:06.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 12:42:06.93
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:06.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:06.95
  Jul 29 12:42:06.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7613" for this suite. @ 07/29/23 12:42:06.96
• [0.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 07/29/23 12:42:06.967
  Jul 29 12:42:06.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 12:42:06.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:06.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:06.988
  STEP: starting the proxy server @ 07/29/23 12:42:06.991
  Jul 29 12:42:06.991: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-3932 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 07/29/23 12:42:07.036
  Jul 29 12:42:07.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3932" for this suite. @ 07/29/23 12:42:07.046
• [0.087 seconds]
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 07/29/23 12:42:07.054
  Jul 29 12:42:07.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 12:42:07.055
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:07.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:07.077
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 07/29/23 12:42:07.08
  Jul 29 12:42:07.088: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul 29 12:42:12.094: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 12:42:12.094
  STEP: getting scale subresource @ 07/29/23 12:42:12.095
  STEP: updating a scale subresource @ 07/29/23 12:42:12.097
  STEP: verifying the replicaset Spec.Replicas was modified @ 07/29/23 12:42:12.103
  STEP: Patch a scale subresource @ 07/29/23 12:42:12.107
  Jul 29 12:42:12.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2288" for this suite. @ 07/29/23 12:42:12.121
• [5.074 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 07/29/23 12:42:12.129
  Jul 29 12:42:12.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 12:42:12.13
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:12.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:12.169
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/29/23 12:42:12.173
  STEP: Saw pod success @ 07/29/23 12:42:16.194
  Jul 29 12:42:16.197: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-e0d43c68-865a-4131-b180-94313c30452a container test-container: <nil>
  STEP: delete the pod @ 07/29/23 12:42:16.204
  Jul 29 12:42:16.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2315" for this suite. @ 07/29/23 12:42:16.221
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 07/29/23 12:42:16.235
  Jul 29 12:42:16.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 12:42:16.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:16.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:16.254
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/29/23 12:42:16.257
  STEP: Saw pod success @ 07/29/23 12:42:20.28
  Jul 29 12:42:20.283: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-715af711-4127-489e-9a68-5e7d0476d509 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 12:42:20.29
  Jul 29 12:42:20.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3425" for this suite. @ 07/29/23 12:42:20.31
• [4.082 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 07/29/23 12:42:20.318
  Jul 29 12:42:20.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 12:42:20.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:20.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:20.339
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 12:42:20.342
  STEP: Saw pod success @ 07/29/23 12:42:24.361
  Jul 29 12:42:24.365: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-d8ee6dec-0e8d-4b4b-a4d3-13201e7c1444 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 12:42:24.372
  Jul 29 12:42:24.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8843" for this suite. @ 07/29/23 12:42:24.391
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 07/29/23 12:42:24.399
  Jul 29 12:42:24.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 12:42:24.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:24.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:24.419
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4518 @ 07/29/23 12:42:24.422
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/29/23 12:42:24.431
  STEP: creating service externalsvc in namespace services-4518 @ 07/29/23 12:42:24.431
  STEP: creating replication controller externalsvc in namespace services-4518 @ 07/29/23 12:42:24.441
  I0729 12:42:24.450012      18 runners.go:194] Created replication controller with name: externalsvc, namespace: services-4518, replica count: 2
  I0729 12:42:27.501788      18 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 07/29/23 12:42:27.506
  Jul 29 12:42:27.519: INFO: Creating new exec pod
  Jul 29 12:42:29.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-4518 exec execpodxqb2g -- /bin/sh -x -c nslookup clusterip-service.services-4518.svc.cluster.local'
  Jul 29 12:42:29.698: INFO: stderr: "+ nslookup clusterip-service.services-4518.svc.cluster.local\n"
  Jul 29 12:42:29.698: INFO: stdout: "Server:\t\t10.152.183.124\nAddress:\t10.152.183.124#53\n\nclusterip-service.services-4518.svc.cluster.local\tcanonical name = externalsvc.services-4518.svc.cluster.local.\nName:\texternalsvc.services-4518.svc.cluster.local\nAddress: 10.152.183.39\n\n"
  Jul 29 12:42:29.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-4518, will wait for the garbage collector to delete the pods @ 07/29/23 12:42:29.703
  Jul 29 12:42:29.765: INFO: Deleting ReplicationController externalsvc took: 6.252765ms
  Jul 29 12:42:29.865: INFO: Terminating ReplicationController externalsvc pods took: 100.829223ms
  Jul 29 12:42:31.781: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-4518" for this suite. @ 07/29/23 12:42:31.794
• [7.402 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 07/29/23 12:42:31.805
  Jul 29 12:42:31.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 12:42:31.806
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:31.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:31.832
  STEP: Creating configMap with name projected-configmap-test-volume-map-3548be29-7e6a-4c23-be4c-8135cec13c9b @ 07/29/23 12:42:31.839
  STEP: Creating a pod to test consume configMaps @ 07/29/23 12:42:31.843
  STEP: Saw pod success @ 07/29/23 12:42:35.862
  Jul 29 12:42:35.866: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-projected-configmaps-af81789c-00b4-479a-acdf-c9d6faac0c08 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 12:42:35.873
  Jul 29 12:42:35.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1923" for this suite. @ 07/29/23 12:42:35.89
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 07/29/23 12:42:35.906
  Jul 29 12:42:35.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 12:42:35.907
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:35.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:35.929
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/29/23 12:42:35.932
  STEP: Saw pod success @ 07/29/23 12:42:39.952
  Jul 29 12:42:39.957: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-78345230-ee28-43b5-a6e8-c673f43c71bd container test-container: <nil>
  STEP: delete the pod @ 07/29/23 12:42:39.963
  Jul 29 12:42:39.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5804" for this suite. @ 07/29/23 12:42:39.982
• [4.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 07/29/23 12:42:39.991
  Jul 29 12:42:39.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename deployment @ 07/29/23 12:42:39.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:40.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:40.011
  Jul 29 12:42:40.014: INFO: Creating deployment "webserver-deployment"
  Jul 29 12:42:40.018: INFO: Waiting for observed generation 1
  Jul 29 12:42:42.026: INFO: Waiting for all required pods to come up
  Jul 29 12:42:42.030: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 07/29/23 12:42:42.03
  Jul 29 12:42:42.030: INFO: Waiting for deployment "webserver-deployment" to complete
  Jul 29 12:42:42.036: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jul 29 12:42:42.047: INFO: Updating deployment webserver-deployment
  Jul 29 12:42:42.047: INFO: Waiting for observed generation 2
  Jul 29 12:42:44.053: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jul 29 12:42:44.057: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jul 29 12:42:44.060: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul 29 12:42:44.070: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jul 29 12:42:44.070: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jul 29 12:42:44.073: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul 29 12:42:44.085: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jul 29 12:42:44.085: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jul 29 12:42:44.125: INFO: Updating deployment webserver-deployment
  Jul 29 12:42:44.125: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jul 29 12:42:44.143: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Jul 29 12:42:44.150: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Jul 29 12:42:46.164: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-845  00fe5009-294b-4a5f-b806-46132354a0bc 14939 3 2023-07-29 12:42:40 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00533cd28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-29 12:42:44 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-07-29 12:42:44 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jul 29 12:42:46.170: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-845  8cad0270-f594-44b8-8c35-316ce14e428f 14931 3 2023-07-29 12:42:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 00fe5009-294b-4a5f-b806-46132354a0bc 0xc00542c907 0xc00542c908}] [] [{kube-controller-manager Update apps/v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00fe5009-294b-4a5f-b806-46132354a0bc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00542c9a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 12:42:46.170: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jul 29 12:42:46.170: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-845  5db1a265-6ff3-40ab-9ee9-53bb981c6b45 14932 3 2023-07-29 12:42:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 00fe5009-294b-4a5f-b806-46132354a0bc 0xc00542c817 0xc00542c818}] [] [{kube-controller-manager Update apps/v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00fe5009-294b-4a5f-b806-46132354a0bc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00542c8a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 12:42:46.186: INFO: Pod "webserver-deployment-67bd4bf6dc-4f977" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4f977 webserver-deployment-67bd4bf6dc- deployment-845  8cd23a0f-0120-437d-a2db-6edd6e41b95e 14952 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542c0f7 0xc00542c0f8}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdxhn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdxhn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.66,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.186: INFO: Pod "webserver-deployment-67bd4bf6dc-4fxg2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4fxg2 webserver-deployment-67bd4bf6dc- deployment-845  f48aeca9-1949-4163-b0f4-f4d6ecdf5d45 14954 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542c2c7 0xc00542c2c8}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jhht7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jhht7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.186: INFO: Pod "webserver-deployment-67bd4bf6dc-4knnx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4knnx webserver-deployment-67bd4bf6dc- deployment-845  5efd8603-9ea4-4436-93c9-8adeeca7062a 14928 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542c497 0xc00542c498}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hj7t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hj7t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.187: INFO: Pod "webserver-deployment-67bd4bf6dc-5nmw9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5nmw9 webserver-deployment-67bd4bf6dc- deployment-845  821556db-36dc-4471-9ea5-76003d14e3fd 14971 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542c677 0xc00542c678}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8pfjz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8pfjz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.187: INFO: Pod "webserver-deployment-67bd4bf6dc-78wzf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-78wzf webserver-deployment-67bd4bf6dc- deployment-845  7c0c531a-f8e0-45f6-8b78-d029fa21eb8b 14973 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542cf07 0xc00542cf08}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2hnq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2hnq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.188: INFO: Pod "webserver-deployment-67bd4bf6dc-7zn8q" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7zn8q webserver-deployment-67bd4bf6dc- deployment-845  6a3e7fe9-55f8-44a9-a357-c6a6da21d863 14963 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542d0d7 0xc00542d0d8}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8qv54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8qv54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.66,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.188: INFO: Pod "webserver-deployment-67bd4bf6dc-8dlfw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8dlfw webserver-deployment-67bd4bf6dc- deployment-845  98b9df4a-3c93-4bdc-a737-ce0f25011e52 14899 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542d2a7 0xc00542d2a8}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94p75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94p75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.189: INFO: Pod "webserver-deployment-67bd4bf6dc-bn7tw" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-bn7tw webserver-deployment-67bd4bf6dc- deployment-845  759da580-325f-44cb-ac8e-b973be5817b3 14742 0 2023-07-29 12:42:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542d477 0xc00542d478}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.8.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p76zc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p76zc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.66,PodIP:192.168.8.157,StartTime:2023-07-29 12:42:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 12:42:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://45e98275a8e10376f2897bffde3df4662b1ee46835cdd26db084b9c7ff3daffd,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.8.157,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.189: INFO: Pod "webserver-deployment-67bd4bf6dc-cls85" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-cls85 webserver-deployment-67bd4bf6dc- deployment-845  2ef75cb1-e68d-42d2-b95c-c0d80cea824e 14929 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542d667 0xc00542d668}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k65wr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k65wr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.190: INFO: Pod "webserver-deployment-67bd4bf6dc-ggq7q" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ggq7q webserver-deployment-67bd4bf6dc- deployment-845  3df522cf-41b1-4f79-85c0-a8847b3743c7 14957 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542d837 0xc00542d838}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h6wgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h6wgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.66,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.190: INFO: Pod "webserver-deployment-67bd4bf6dc-h59bf" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-h59bf webserver-deployment-67bd4bf6dc- deployment-845  69650cc4-8379-4edb-8343-a330b55f5822 14737 0 2023-07-29 12:42:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542da07 0xc00542da08}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.10.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vmqv2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vmqv2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:192.168.10.48,StartTime:2023-07-29 12:42:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 12:42:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3b4030b2e5d7735ece2a1ac545bde7c02e192a68a57f3d56bfa7c87c20c7d0b0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.10.48,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.190: INFO: Pod "webserver-deployment-67bd4bf6dc-kcztg" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-kcztg webserver-deployment-67bd4bf6dc- deployment-845  c18f1849-abdc-4fce-b122-39b4d2ebfd47 14752 0 2023-07-29 12:42:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542dbf7 0xc00542dbf8}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.129.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7qjpl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7qjpl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:192.168.129.74,StartTime:2023-07-29 12:42:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 12:42:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://23b87da114caa4056c37b821b03c083b7aa55f75b3ba4d4d33a0f58e79f92412,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.129.74,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.191: INFO: Pod "webserver-deployment-67bd4bf6dc-m7s2r" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-m7s2r webserver-deployment-67bd4bf6dc- deployment-845  b8191142-8d39-4592-b673-a6225051b37b 14734 0 2023-07-29 12:42:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542dde7 0xc00542dde8}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.10.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nhgk2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nhgk2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:192.168.10.49,StartTime:2023-07-29 12:42:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 12:42:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://89ef633c236fcfe4d4d4470caa2bbdcd8518762dc9d04a2a2230325aa635a0e0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.10.49,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.191: INFO: Pod "webserver-deployment-67bd4bf6dc-pmzgd" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-pmzgd webserver-deployment-67bd4bf6dc- deployment-845  95ad949e-5ab4-4dad-8da1-b6e9ba750101 14743 0 2023-07-29 12:42:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00542dfd7 0xc00542dfd8}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.10.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z4fhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z4fhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:192.168.10.47,StartTime:2023-07-29 12:42:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 12:42:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://19c60745fbf80f0c9374f0313326783918892b8e40cb4026fd2a71c921143274,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.10.47,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.191: INFO: Pod "webserver-deployment-67bd4bf6dc-s5mf6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-s5mf6 webserver-deployment-67bd4bf6dc- deployment-845  c8679a3f-a48b-4bc9-939b-84c789f64d49 14968 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00533c1e7 0xc00533c1e8}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p77f6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p77f6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.191: INFO: Pod "webserver-deployment-67bd4bf6dc-ttsqd" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ttsqd webserver-deployment-67bd4bf6dc- deployment-845  3daf1a5c-3d58-461b-89ff-b4e7e0243c67 14738 0 2023-07-29 12:42:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00533c3c7 0xc00533c3c8}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.8.156\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z6ltg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6ltg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.66,PodIP:192.168.8.156,StartTime:2023-07-29 12:42:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 12:42:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://70d1952d5157babcca061ab7d34a31b49749dcfd90eba42c35a616d3481822ea,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.8.156,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.191: INFO: Pod "webserver-deployment-67bd4bf6dc-twbsb" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-twbsb webserver-deployment-67bd4bf6dc- deployment-845  ab411d75-656d-4783-847d-47aeeecfe582 14749 0 2023-07-29 12:42:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00533c5b7 0xc00533c5b8}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.8.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tl44z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tl44z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.66,PodIP:192.168.8.158,StartTime:2023-07-29 12:42:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 12:42:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d518f6370594a8f6a2324b3289b6b5163e182c4784dadbe52e7068768557b8ba,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.8.158,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.192: INFO: Pod "webserver-deployment-67bd4bf6dc-wbfwd" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wbfwd webserver-deployment-67bd4bf6dc- deployment-845  0cc34178-32fb-4dc0-9e11-8cd421ba10b2 14756 0 2023-07-29 12:42:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00533c7a7 0xc00533c7a8}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.129.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lz5w4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lz5w4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:192.168.129.71,StartTime:2023-07-29 12:42:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 12:42:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://03fee3bb66134c63370630762050d7ec935187776222b3133af7049e8cf02e5c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.129.71,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.192: INFO: Pod "webserver-deployment-67bd4bf6dc-xxctf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xxctf webserver-deployment-67bd4bf6dc- deployment-845  e013a870-edad-446e-9c8e-fb7aa1bf4827 14966 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00533c9a7 0xc00533c9a8}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pglw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pglw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.192: INFO: Pod "webserver-deployment-67bd4bf6dc-z7v2p" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-z7v2p webserver-deployment-67bd4bf6dc- deployment-845  70b107de-d4c7-410b-9447-0060c7313bab 14959 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 5db1a265-6ff3-40ab-9ee9-53bb981c6b45 0xc00533cb77 0xc00533cb78}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db1a265-6ff3-40ab-9ee9-53bb981c6b45\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-97sdf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-97sdf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.192: INFO: Pod "webserver-deployment-7b75d79cf5-5whfb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5whfb webserver-deployment-7b75d79cf5- deployment-845  dee3351c-9921-4655-83fb-d402703b25db 14860 0 2023-07-29 12:42:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8cad0270-f594-44b8-8c35-316ce14e428f 0xc00533ce77 0xc00533ce78}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cad0270-f594-44b8-8c35-316ce14e428f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.10.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lwzqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lwzqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:192.168.10.50,StartTime:2023-07-29 12:42:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.10.50,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.192: INFO: Pod "webserver-deployment-7b75d79cf5-7v94h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-7v94h webserver-deployment-7b75d79cf5- deployment-845  44271e8c-d040-42b4-a363-b22c02ac1d55 14864 0 2023-07-29 12:42:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8cad0270-f594-44b8-8c35-316ce14e428f 0xc00533d227 0xc00533d228}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cad0270-f594-44b8-8c35-316ce14e428f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.129.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mcwpf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mcwpf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:192.168.129.68,StartTime:2023-07-29 12:42:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.129.68,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.193: INFO: Pod "webserver-deployment-7b75d79cf5-87kmd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-87kmd webserver-deployment-7b75d79cf5- deployment-845  fe5f9380-9fd3-4138-a37f-02c9d5389e98 14855 0 2023-07-29 12:42:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8cad0270-f594-44b8-8c35-316ce14e428f 0xc00533d447 0xc00533d448}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cad0270-f594-44b8-8c35-316ce14e428f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.10.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dbrb5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dbrb5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:192.168.10.51,StartTime:2023-07-29 12:42:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.10.51,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.193: INFO: Pod "webserver-deployment-7b75d79cf5-c46th" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-c46th webserver-deployment-7b75d79cf5- deployment-845  cf070902-2b0d-4536-8ac5-6ef84f6740b4 15054 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8cad0270-f594-44b8-8c35-316ce14e428f 0xc00533d667 0xc00533d668}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cad0270-f594-44b8-8c35-316ce14e428f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.10.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dxz6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dxz6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:192.168.10.52,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.10.52,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.193: INFO: Pod "webserver-deployment-7b75d79cf5-hzn2l" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hzn2l webserver-deployment-7b75d79cf5- deployment-845  44db26e7-41d1-4ba0-9d0f-14d763226fea 14868 0 2023-07-29 12:42:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8cad0270-f594-44b8-8c35-316ce14e428f 0xc00533d887 0xc00533d888}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cad0270-f594-44b8-8c35-316ce14e428f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.129.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cx7pj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cx7pj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:192.168.129.73,StartTime:2023-07-29 12:42:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.129.73,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.193: INFO: Pod "webserver-deployment-7b75d79cf5-jgnmb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jgnmb webserver-deployment-7b75d79cf5- deployment-845  eb376264-e817-4d76-b715-75098bcf5475 14946 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8cad0270-f594-44b8-8c35-316ce14e428f 0xc00533daa7 0xc00533daa8}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cad0270-f594-44b8-8c35-316ce14e428f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r82jn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r82jn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.193: INFO: Pod "webserver-deployment-7b75d79cf5-lmr29" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lmr29 webserver-deployment-7b75d79cf5- deployment-845  b95eb36c-ce19-4b8b-85d9-8536a23d1fae 14951 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8cad0270-f594-44b8-8c35-316ce14e428f 0xc00533dc97 0xc00533dc98}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cad0270-f594-44b8-8c35-316ce14e428f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-trdpm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-trdpm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.194: INFO: Pod "webserver-deployment-7b75d79cf5-mq8kp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-mq8kp webserver-deployment-7b75d79cf5- deployment-845  6c7ae8b3-f07c-4f66-a320-bcae6bc44904 14969 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8cad0270-f594-44b8-8c35-316ce14e428f 0xc00533de87 0xc00533de88}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cad0270-f594-44b8-8c35-316ce14e428f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xwf78,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xwf78,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.194: INFO: Pod "webserver-deployment-7b75d79cf5-q59x2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-q59x2 webserver-deployment-7b75d79cf5- deployment-845  be0b0ad9-59a1-42c5-9041-f345ff5e61d1 14917 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8cad0270-f594-44b8-8c35-316ce14e428f 0xc0043e6077 0xc0043e6078}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cad0270-f594-44b8-8c35-316ce14e428f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nb88l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nb88l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.66,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.194: INFO: Pod "webserver-deployment-7b75d79cf5-qhbmn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-qhbmn webserver-deployment-7b75d79cf5- deployment-845  5108b95b-c6cf-41f2-b303-e8d2debe07ad 14967 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8cad0270-f594-44b8-8c35-316ce14e428f 0xc0043e6260 0xc0043e6261}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cad0270-f594-44b8-8c35-316ce14e428f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pqscs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pqscs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.66,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.195: INFO: Pod "webserver-deployment-7b75d79cf5-vbqzw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-vbqzw webserver-deployment-7b75d79cf5- deployment-845  b76517b1-aac9-4663-ba7f-70a3afba2764 14972 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8cad0270-f594-44b8-8c35-316ce14e428f 0xc0043e6450 0xc0043e6451}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cad0270-f594-44b8-8c35-316ce14e428f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9jxt6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9jxt6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.66,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.195: INFO: Pod "webserver-deployment-7b75d79cf5-w8zzk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-w8zzk webserver-deployment-7b75d79cf5- deployment-845  ffee8c9d-524a-4834-a376-3c6a9b561691 14862 0 2023-07-29 12:42:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8cad0270-f594-44b8-8c35-316ce14e428f 0xc0043e6630 0xc0043e6631}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cad0270-f594-44b8-8c35-316ce14e428f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.8.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8577k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8577k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-5-66,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.5.66,PodIP:192.168.8.159,StartTime:2023-07-29 12:42:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.8.159,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.195: INFO: Pod "webserver-deployment-7b75d79cf5-x8x7c" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-x8x7c webserver-deployment-7b75d79cf5- deployment-845  d45b44de-ab09-41ce-b1fc-860d1709d85d 14961 0 2023-07-29 12:42:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 8cad0270-f594-44b8-8c35-316ce14e428f 0xc0043e6840 0xc0043e6841}] [] [{kube-controller-manager Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8cad0270-f594-44b8-8c35-316ce14e428f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:42:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5wdtq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5wdtq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:42:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:,StartTime:2023-07-29 12:42:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:42:46.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-845" for this suite. @ 07/29/23 12:42:46.2
• [6.218 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 07/29/23 12:42:46.21
  Jul 29 12:42:46.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 12:42:46.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:46.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:46.238
  STEP: Create a ReplicaSet @ 07/29/23 12:42:46.242
  STEP: Verify that the required pods have come up @ 07/29/23 12:42:46.247
  Jul 29 12:42:46.249: INFO: Pod name sample-pod: Found 0 pods out of 3
  Jul 29 12:42:51.254: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 07/29/23 12:42:51.254
  Jul 29 12:42:51.258: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 07/29/23 12:42:51.258
  STEP: DeleteCollection of the ReplicaSets @ 07/29/23 12:42:51.261
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 07/29/23 12:42:51.307
  Jul 29 12:42:51.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2934" for this suite. @ 07/29/23 12:42:51.327
• [5.127 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 07/29/23 12:42:51.337
  Jul 29 12:42:51.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/29/23 12:42:51.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:42:51.362
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:42:51.366
  STEP: Creating 50 configmaps @ 07/29/23 12:42:51.369
  STEP: Creating RC which spawns configmap-volume pods @ 07/29/23 12:42:51.641
  Jul 29 12:42:51.704: INFO: Pod name wrapped-volume-race-8f08c753-2212-4c50-832c-2a66114608e2: Found 3 pods out of 5
  Jul 29 12:42:56.710: INFO: Pod name wrapped-volume-race-8f08c753-2212-4c50-832c-2a66114608e2: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/29/23 12:42:56.71
  STEP: Creating RC which spawns configmap-volume pods @ 07/29/23 12:42:58.737
  Jul 29 12:42:58.749: INFO: Pod name wrapped-volume-race-d6340820-2dc4-4955-a9d5-bc2911def078: Found 0 pods out of 5
  Jul 29 12:43:03.758: INFO: Pod name wrapped-volume-race-d6340820-2dc4-4955-a9d5-bc2911def078: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/29/23 12:43:03.758
  STEP: Creating RC which spawns configmap-volume pods @ 07/29/23 12:43:03.781
  Jul 29 12:43:03.797: INFO: Pod name wrapped-volume-race-997dbd0f-4785-43ed-a43f-b984e1a3f8fe: Found 0 pods out of 5
  Jul 29 12:43:08.805: INFO: Pod name wrapped-volume-race-997dbd0f-4785-43ed-a43f-b984e1a3f8fe: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/29/23 12:43:08.805
  Jul 29 12:43:08.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-997dbd0f-4785-43ed-a43f-b984e1a3f8fe in namespace emptydir-wrapper-6480, will wait for the garbage collector to delete the pods @ 07/29/23 12:43:08.828
  Jul 29 12:43:08.889: INFO: Deleting ReplicationController wrapped-volume-race-997dbd0f-4785-43ed-a43f-b984e1a3f8fe took: 7.529059ms
  Jul 29 12:43:08.990: INFO: Terminating ReplicationController wrapped-volume-race-997dbd0f-4785-43ed-a43f-b984e1a3f8fe pods took: 100.610477ms
  STEP: deleting ReplicationController wrapped-volume-race-d6340820-2dc4-4955-a9d5-bc2911def078 in namespace emptydir-wrapper-6480, will wait for the garbage collector to delete the pods @ 07/29/23 12:43:11.291
  Jul 29 12:43:11.353: INFO: Deleting ReplicationController wrapped-volume-race-d6340820-2dc4-4955-a9d5-bc2911def078 took: 7.686489ms
  Jul 29 12:43:11.454: INFO: Terminating ReplicationController wrapped-volume-race-d6340820-2dc4-4955-a9d5-bc2911def078 pods took: 101.066362ms
  STEP: deleting ReplicationController wrapped-volume-race-8f08c753-2212-4c50-832c-2a66114608e2 in namespace emptydir-wrapper-6480, will wait for the garbage collector to delete the pods @ 07/29/23 12:43:13.355
  Jul 29 12:43:13.420: INFO: Deleting ReplicationController wrapped-volume-race-8f08c753-2212-4c50-832c-2a66114608e2 took: 8.129282ms
  Jul 29 12:43:13.521: INFO: Terminating ReplicationController wrapped-volume-race-8f08c753-2212-4c50-832c-2a66114608e2 pods took: 100.910034ms
  STEP: Cleaning up the configMaps @ 07/29/23 12:43:15.422
  STEP: Destroying namespace "emptydir-wrapper-6480" for this suite. @ 07/29/23 12:43:15.705
• [24.375 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 07/29/23 12:43:15.715
  Jul 29 12:43:15.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 12:43:15.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:43:15.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:43:15.742
  STEP: Setting up server cert @ 07/29/23 12:43:15.767
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 12:43:16.035
  STEP: Deploying the webhook pod @ 07/29/23 12:43:16.045
  STEP: Wait for the deployment to be ready @ 07/29/23 12:43:16.058
  Jul 29 12:43:16.065: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 12:43:18.075
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 12:43:18.086
  Jul 29 12:43:19.087: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/29/23 12:43:19.091
  STEP: create a pod that should be denied by the webhook @ 07/29/23 12:43:19.106
  STEP: create a pod that causes the webhook to hang @ 07/29/23 12:43:19.12
  STEP: create a configmap that should be denied by the webhook @ 07/29/23 12:43:29.129
  STEP: create a configmap that should be admitted by the webhook @ 07/29/23 12:43:29.177
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/29/23 12:43:29.186
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/29/23 12:43:29.195
  STEP: create a namespace that bypass the webhook @ 07/29/23 12:43:29.2
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 07/29/23 12:43:29.22
  Jul 29 12:43:29.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2985" for this suite. @ 07/29/23 12:43:29.288
  STEP: Destroying namespace "webhook-markers-5750" for this suite. @ 07/29/23 12:43:29.295
  STEP: Destroying namespace "exempted-namespace-199" for this suite. @ 07/29/23 12:43:29.306
• [13.597 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 07/29/23 12:43:29.313
  Jul 29 12:43:29.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 12:43:29.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:43:29.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:43:29.334
  STEP: Creating a simple DaemonSet "daemon-set" @ 07/29/23 12:43:29.355
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 12:43:29.362
  Jul 29 12:43:29.365: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:43:29.366: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:43:29.368: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 12:43:29.368: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  Jul 29 12:43:30.373: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:43:30.373: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:43:30.376: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 12:43:30.376: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  Jul 29 12:43:31.373: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:43:31.373: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:43:31.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 12:43:31.377: INFO: Node ip-172-31-5-66 is running 0 daemon pod, expected 1
  Jul 29 12:43:32.372: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:43:32.372: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:43:32.376: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 12:43:32.376: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 07/29/23 12:43:32.378
  Jul 29 12:43:32.422: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:43:32.422: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 12:43:32.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 12:43:32.441: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 07/29/23 12:43:32.441
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 12:43:33.453
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1215, will wait for the garbage collector to delete the pods @ 07/29/23 12:43:33.453
  Jul 29 12:43:33.513: INFO: Deleting DaemonSet.extensions daemon-set took: 6.886925ms
  Jul 29 12:43:33.614: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.615906ms
  Jul 29 12:43:35.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 12:43:35.118: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 12:43:35.120: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16390"},"items":null}

  Jul 29 12:43:35.124: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16390"},"items":null}

  Jul 29 12:43:35.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1215" for this suite. @ 07/29/23 12:43:35.143
• [5.836 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 07/29/23 12:43:35.151
  Jul 29 12:43:35.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename containers @ 07/29/23 12:43:35.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:43:35.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:43:35.171
  STEP: Creating a pod to test override arguments @ 07/29/23 12:43:35.174
  STEP: Saw pod success @ 07/29/23 12:43:39.191
  Jul 29 12:43:39.194: INFO: Trying to get logs from node ip-172-31-33-37 pod client-containers-fbb0d358-4294-4786-9685-c7b18d3155e5 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 12:43:39.202
  Jul 29 12:43:39.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2376" for this suite. @ 07/29/23 12:43:39.221
• [4.077 seconds]
------------------------------
SSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 07/29/23 12:43:39.228
  Jul 29 12:43:39.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename hostport @ 07/29/23 12:43:39.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:43:39.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:43:39.252
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 07/29/23 12:43:39.258
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.19.67 on the node which pod1 resides and expect scheduled @ 07/29/23 12:43:41.274
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.19.67 but use UDP protocol on the node which pod2 resides @ 07/29/23 12:43:45.293
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 07/29/23 12:43:49.324
  Jul 29 12:43:49.324: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.19.67 http://127.0.0.1:54323/hostname] Namespace:hostport-2061 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:43:49.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:43:49.324: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:43:49.324: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-2061/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.19.67+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.19.67, port: 54323 @ 07/29/23 12:43:49.394
  Jul 29 12:43:49.394: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.19.67:54323/hostname] Namespace:hostport-2061 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:43:49.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:43:49.395: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:43:49.395: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-2061/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.19.67%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.19.67, port: 54323 UDP @ 07/29/23 12:43:49.475
  Jul 29 12:43:49.475: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.31.19.67 54323] Namespace:hostport-2061 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:43:49.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:43:49.476: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:43:49.476: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-2061/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.31.19.67+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Jul 29 12:43:54.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-2061" for this suite. @ 07/29/23 12:43:54.533
• [15.310 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 07/29/23 12:43:54.539
  Jul 29 12:43:54.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename security-context-test @ 07/29/23 12:43:54.54
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:43:54.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:43:54.561
  Jul 29 12:43:58.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9462" for this suite. @ 07/29/23 12:43:58.596
• [4.065 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 07/29/23 12:43:58.606
  Jul 29 12:43:58.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename job @ 07/29/23 12:43:58.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:43:58.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:43:58.633
  STEP: Creating a job @ 07/29/23 12:43:58.635
  STEP: Ensuring job reaches completions @ 07/29/23 12:43:58.643
  Jul 29 12:44:08.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6625" for this suite. @ 07/29/23 12:44:08.651
• [10.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 07/29/23 12:44:08.659
  Jul 29 12:44:08.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename gc @ 07/29/23 12:44:08.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:44:08.677
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:44:08.681
  STEP: create the rc @ 07/29/23 12:44:08.687
  W0729 12:44:08.691551      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 07/29/23 12:44:14.696
  STEP: wait for the rc to be deleted @ 07/29/23 12:44:14.714
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 07/29/23 12:44:19.722
  STEP: Gathering metrics @ 07/29/23 12:44:49.731
  W0729 12:44:49.736352      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 29 12:44:49.736: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 12:44:49.736: INFO: Deleting pod "simpletest.rc-24wxs" in namespace "gc-5065"
  Jul 29 12:44:49.748: INFO: Deleting pod "simpletest.rc-27hjn" in namespace "gc-5065"
  Jul 29 12:44:49.772: INFO: Deleting pod "simpletest.rc-4q7wh" in namespace "gc-5065"
  Jul 29 12:44:49.783: INFO: Deleting pod "simpletest.rc-4rstc" in namespace "gc-5065"
  Jul 29 12:44:49.799: INFO: Deleting pod "simpletest.rc-4vc6r" in namespace "gc-5065"
  Jul 29 12:44:49.813: INFO: Deleting pod "simpletest.rc-4xk6x" in namespace "gc-5065"
  Jul 29 12:44:49.827: INFO: Deleting pod "simpletest.rc-52qjf" in namespace "gc-5065"
  Jul 29 12:44:49.839: INFO: Deleting pod "simpletest.rc-55p2s" in namespace "gc-5065"
  Jul 29 12:44:49.851: INFO: Deleting pod "simpletest.rc-5gk96" in namespace "gc-5065"
  Jul 29 12:44:49.865: INFO: Deleting pod "simpletest.rc-5ksgw" in namespace "gc-5065"
  Jul 29 12:44:49.878: INFO: Deleting pod "simpletest.rc-5vj25" in namespace "gc-5065"
  Jul 29 12:44:49.892: INFO: Deleting pod "simpletest.rc-645ll" in namespace "gc-5065"
  Jul 29 12:44:49.906: INFO: Deleting pod "simpletest.rc-65dfp" in namespace "gc-5065"
  Jul 29 12:44:49.917: INFO: Deleting pod "simpletest.rc-66tkp" in namespace "gc-5065"
  Jul 29 12:44:49.930: INFO: Deleting pod "simpletest.rc-6fj6r" in namespace "gc-5065"
  Jul 29 12:44:49.940: INFO: Deleting pod "simpletest.rc-6nn7f" in namespace "gc-5065"
  Jul 29 12:44:49.953: INFO: Deleting pod "simpletest.rc-6sq7p" in namespace "gc-5065"
  Jul 29 12:44:49.965: INFO: Deleting pod "simpletest.rc-6xzmz" in namespace "gc-5065"
  Jul 29 12:44:49.979: INFO: Deleting pod "simpletest.rc-7pswb" in namespace "gc-5065"
  Jul 29 12:44:49.994: INFO: Deleting pod "simpletest.rc-899fr" in namespace "gc-5065"
  Jul 29 12:44:50.009: INFO: Deleting pod "simpletest.rc-8c28g" in namespace "gc-5065"
  Jul 29 12:44:50.021: INFO: Deleting pod "simpletest.rc-8hh85" in namespace "gc-5065"
  Jul 29 12:44:50.034: INFO: Deleting pod "simpletest.rc-8mnq9" in namespace "gc-5065"
  Jul 29 12:44:50.054: INFO: Deleting pod "simpletest.rc-8n7jq" in namespace "gc-5065"
  Jul 29 12:44:50.071: INFO: Deleting pod "simpletest.rc-92xnm" in namespace "gc-5065"
  Jul 29 12:44:50.086: INFO: Deleting pod "simpletest.rc-9v2h7" in namespace "gc-5065"
  Jul 29 12:44:50.103: INFO: Deleting pod "simpletest.rc-9x2kb" in namespace "gc-5065"
  Jul 29 12:44:50.121: INFO: Deleting pod "simpletest.rc-b7x4v" in namespace "gc-5065"
  Jul 29 12:44:50.135: INFO: Deleting pod "simpletest.rc-bcbv2" in namespace "gc-5065"
  Jul 29 12:44:50.154: INFO: Deleting pod "simpletest.rc-bfvgb" in namespace "gc-5065"
  Jul 29 12:44:50.167: INFO: Deleting pod "simpletest.rc-blvjc" in namespace "gc-5065"
  Jul 29 12:44:50.180: INFO: Deleting pod "simpletest.rc-bscsj" in namespace "gc-5065"
  Jul 29 12:44:50.195: INFO: Deleting pod "simpletest.rc-bxvtj" in namespace "gc-5065"
  Jul 29 12:44:50.207: INFO: Deleting pod "simpletest.rc-bzxrq" in namespace "gc-5065"
  Jul 29 12:44:50.220: INFO: Deleting pod "simpletest.rc-cmkxh" in namespace "gc-5065"
  Jul 29 12:44:50.231: INFO: Deleting pod "simpletest.rc-djksk" in namespace "gc-5065"
  Jul 29 12:44:50.242: INFO: Deleting pod "simpletest.rc-dm4jw" in namespace "gc-5065"
  Jul 29 12:44:50.259: INFO: Deleting pod "simpletest.rc-drrps" in namespace "gc-5065"
  Jul 29 12:44:50.269: INFO: Deleting pod "simpletest.rc-dtjrt" in namespace "gc-5065"
  Jul 29 12:44:50.284: INFO: Deleting pod "simpletest.rc-f8cfz" in namespace "gc-5065"
  Jul 29 12:44:50.294: INFO: Deleting pod "simpletest.rc-f8lfg" in namespace "gc-5065"
  Jul 29 12:44:50.308: INFO: Deleting pod "simpletest.rc-fbchc" in namespace "gc-5065"
  Jul 29 12:44:50.332: INFO: Deleting pod "simpletest.rc-fbsbd" in namespace "gc-5065"
  Jul 29 12:44:50.345: INFO: Deleting pod "simpletest.rc-fd7h9" in namespace "gc-5065"
  Jul 29 12:44:50.359: INFO: Deleting pod "simpletest.rc-ff6kq" in namespace "gc-5065"
  Jul 29 12:44:50.369: INFO: Deleting pod "simpletest.rc-fkvpg" in namespace "gc-5065"
  Jul 29 12:44:50.385: INFO: Deleting pod "simpletest.rc-fpsvg" in namespace "gc-5065"
  Jul 29 12:44:50.399: INFO: Deleting pod "simpletest.rc-ftc76" in namespace "gc-5065"
  Jul 29 12:44:50.414: INFO: Deleting pod "simpletest.rc-fvpbh" in namespace "gc-5065"
  Jul 29 12:44:50.427: INFO: Deleting pod "simpletest.rc-gnw4l" in namespace "gc-5065"
  Jul 29 12:44:50.436: INFO: Deleting pod "simpletest.rc-h889m" in namespace "gc-5065"
  Jul 29 12:44:50.452: INFO: Deleting pod "simpletest.rc-hd2xb" in namespace "gc-5065"
  Jul 29 12:44:50.466: INFO: Deleting pod "simpletest.rc-j2kdg" in namespace "gc-5065"
  Jul 29 12:44:50.480: INFO: Deleting pod "simpletest.rc-jtrtw" in namespace "gc-5065"
  Jul 29 12:44:50.491: INFO: Deleting pod "simpletest.rc-jwv4b" in namespace "gc-5065"
  Jul 29 12:44:50.504: INFO: Deleting pod "simpletest.rc-k5h4c" in namespace "gc-5065"
  Jul 29 12:44:50.516: INFO: Deleting pod "simpletest.rc-k7lb6" in namespace "gc-5065"
  Jul 29 12:44:50.530: INFO: Deleting pod "simpletest.rc-k7nxw" in namespace "gc-5065"
  Jul 29 12:44:50.542: INFO: Deleting pod "simpletest.rc-kf4mr" in namespace "gc-5065"
  Jul 29 12:44:50.557: INFO: Deleting pod "simpletest.rc-kl8vm" in namespace "gc-5065"
  Jul 29 12:44:50.572: INFO: Deleting pod "simpletest.rc-klcfd" in namespace "gc-5065"
  Jul 29 12:44:50.588: INFO: Deleting pod "simpletest.rc-kmztv" in namespace "gc-5065"
  Jul 29 12:44:50.602: INFO: Deleting pod "simpletest.rc-lg7tn" in namespace "gc-5065"
  Jul 29 12:44:50.615: INFO: Deleting pod "simpletest.rc-lkfhv" in namespace "gc-5065"
  Jul 29 12:44:50.627: INFO: Deleting pod "simpletest.rc-m245k" in namespace "gc-5065"
  Jul 29 12:44:50.639: INFO: Deleting pod "simpletest.rc-m6pzv" in namespace "gc-5065"
  Jul 29 12:44:50.654: INFO: Deleting pod "simpletest.rc-n4m46" in namespace "gc-5065"
  Jul 29 12:44:50.684: INFO: Deleting pod "simpletest.rc-nrf45" in namespace "gc-5065"
  Jul 29 12:44:50.738: INFO: Deleting pod "simpletest.rc-qh9p6" in namespace "gc-5065"
  Jul 29 12:44:50.785: INFO: Deleting pod "simpletest.rc-qqt6v" in namespace "gc-5065"
  Jul 29 12:44:50.836: INFO: Deleting pod "simpletest.rc-r52kg" in namespace "gc-5065"
  Jul 29 12:44:50.885: INFO: Deleting pod "simpletest.rc-r5ppz" in namespace "gc-5065"
  Jul 29 12:44:50.935: INFO: Deleting pod "simpletest.rc-rrvwg" in namespace "gc-5065"
  Jul 29 12:44:50.985: INFO: Deleting pod "simpletest.rc-rzsx4" in namespace "gc-5065"
  Jul 29 12:44:51.037: INFO: Deleting pod "simpletest.rc-s4rf8" in namespace "gc-5065"
  Jul 29 12:44:51.086: INFO: Deleting pod "simpletest.rc-s8s8v" in namespace "gc-5065"
  Jul 29 12:44:51.137: INFO: Deleting pod "simpletest.rc-sd2vl" in namespace "gc-5065"
  Jul 29 12:44:51.188: INFO: Deleting pod "simpletest.rc-sf6cq" in namespace "gc-5065"
  Jul 29 12:44:51.235: INFO: Deleting pod "simpletest.rc-sfd5s" in namespace "gc-5065"
  Jul 29 12:44:51.286: INFO: Deleting pod "simpletest.rc-sfw5q" in namespace "gc-5065"
  Jul 29 12:44:51.336: INFO: Deleting pod "simpletest.rc-swkp2" in namespace "gc-5065"
  Jul 29 12:44:51.388: INFO: Deleting pod "simpletest.rc-sxbnl" in namespace "gc-5065"
  Jul 29 12:44:51.436: INFO: Deleting pod "simpletest.rc-t5jcc" in namespace "gc-5065"
  Jul 29 12:44:51.489: INFO: Deleting pod "simpletest.rc-tm2c7" in namespace "gc-5065"
  Jul 29 12:44:51.543: INFO: Deleting pod "simpletest.rc-vckml" in namespace "gc-5065"
  Jul 29 12:44:51.586: INFO: Deleting pod "simpletest.rc-vj2s7" in namespace "gc-5065"
  Jul 29 12:44:51.636: INFO: Deleting pod "simpletest.rc-vkjvk" in namespace "gc-5065"
  Jul 29 12:44:51.687: INFO: Deleting pod "simpletest.rc-vrw24" in namespace "gc-5065"
  Jul 29 12:44:51.734: INFO: Deleting pod "simpletest.rc-w9xx6" in namespace "gc-5065"
  Jul 29 12:44:51.789: INFO: Deleting pod "simpletest.rc-whgbf" in namespace "gc-5065"
  Jul 29 12:44:51.836: INFO: Deleting pod "simpletest.rc-wq552" in namespace "gc-5065"
  Jul 29 12:44:51.888: INFO: Deleting pod "simpletest.rc-wx5tt" in namespace "gc-5065"
  Jul 29 12:44:51.937: INFO: Deleting pod "simpletest.rc-x8mvc" in namespace "gc-5065"
  Jul 29 12:44:51.986: INFO: Deleting pod "simpletest.rc-xb7xt" in namespace "gc-5065"
  Jul 29 12:44:52.034: INFO: Deleting pod "simpletest.rc-xdrk8" in namespace "gc-5065"
  Jul 29 12:44:52.088: INFO: Deleting pod "simpletest.rc-xk6q8" in namespace "gc-5065"
  Jul 29 12:44:52.138: INFO: Deleting pod "simpletest.rc-xqhbb" in namespace "gc-5065"
  Jul 29 12:44:52.191: INFO: Deleting pod "simpletest.rc-xzldg" in namespace "gc-5065"
  Jul 29 12:44:52.239: INFO: Deleting pod "simpletest.rc-z5kb8" in namespace "gc-5065"
  Jul 29 12:44:52.287: INFO: Deleting pod "simpletest.rc-z957p" in namespace "gc-5065"
  Jul 29 12:44:52.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5065" for this suite. @ 07/29/23 12:44:52.378
• [43.772 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 07/29/23 12:44:52.434
  Jul 29 12:44:52.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 12:44:52.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:44:52.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:44:52.465
  STEP: Creating a pod to test substitution in container's command @ 07/29/23 12:44:52.468
  STEP: Saw pod success @ 07/29/23 12:45:02.503
  Jul 29 12:45:02.506: INFO: Trying to get logs from node ip-172-31-33-37 pod var-expansion-75d3056b-cf2b-423e-b42c-deceda3db7d0 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 12:45:02.515
  Jul 29 12:45:02.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7668" for this suite. @ 07/29/23 12:45:02.538
• [10.112 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 07/29/23 12:45:02.546
  Jul 29 12:45:02.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 12:45:02.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:45:02.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:45:02.572
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 12:45:02.577
  STEP: Saw pod success @ 07/29/23 12:45:06.602
  Jul 29 12:45:06.606: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-ec382bb9-9b07-4012-90e9-03588455c0b5 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 12:45:06.614
  Jul 29 12:45:06.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3875" for this suite. @ 07/29/23 12:45:06.633
• [4.093 seconds]
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 07/29/23 12:45:06.64
  Jul 29 12:45:06.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename security-context-test @ 07/29/23 12:45:06.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:45:06.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:45:06.661
  Jul 29 12:45:10.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5957" for this suite. @ 07/29/23 12:45:10.69
• [4.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 07/29/23 12:45:10.697
  Jul 29 12:45:10.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename secrets @ 07/29/23 12:45:10.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:45:10.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:45:10.72
  STEP: creating a secret @ 07/29/23 12:45:10.723
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 07/29/23 12:45:10.728
  STEP: patching the secret @ 07/29/23 12:45:10.732
  STEP: deleting the secret using a LabelSelector @ 07/29/23 12:45:10.739
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 07/29/23 12:45:10.749
  Jul 29 12:45:10.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5805" for this suite. @ 07/29/23 12:45:10.757
• [0.066 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 07/29/23 12:45:10.764
  Jul 29 12:45:10.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename sched-preemption @ 07/29/23 12:45:10.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:45:10.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:45:10.787
  Jul 29 12:45:10.804: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 12:46:10.820: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/29/23 12:46:10.825
  Jul 29 12:46:10.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/29/23 12:46:10.825
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:46:10.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:46:10.848
  Jul 29 12:46:10.863: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jul 29 12:46:10.865: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jul 29 12:46:10.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 12:46:10.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-904" for this suite. @ 07/29/23 12:46:10.941
  STEP: Destroying namespace "sched-preemption-1459" for this suite. @ 07/29/23 12:46:10.947
• [60.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 07/29/23 12:46:10.96
  Jul 29 12:46:10.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename secrets @ 07/29/23 12:46:10.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:46:10.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:46:10.981
  STEP: Creating secret with name secret-test-b2356955-3256-4061-864b-12fba6fc9d65 @ 07/29/23 12:46:10.984
  STEP: Creating a pod to test consume secrets @ 07/29/23 12:46:10.989
  STEP: Saw pod success @ 07/29/23 12:46:15.035
  Jul 29 12:46:15.040: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-secrets-158e99a6-8415-4bbc-9569-21926b0f80a4 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 12:46:15.05
  Jul 29 12:46:15.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2589" for this suite. @ 07/29/23 12:46:15.068
• [4.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 07/29/23 12:46:15.077
  Jul 29 12:46:15.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename init-container @ 07/29/23 12:46:15.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:46:15.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:46:15.178
  STEP: creating the pod @ 07/29/23 12:46:15.181
  Jul 29 12:46:15.181: INFO: PodSpec: initContainers in spec.initContainers
  Jul 29 12:46:19.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9862" for this suite. @ 07/29/23 12:46:19.718
• [4.647 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 07/29/23 12:46:19.726
  Jul 29 12:46:19.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename containers @ 07/29/23 12:46:19.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:46:19.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:46:19.743
  STEP: Creating a pod to test override command @ 07/29/23 12:46:19.746
  STEP: Saw pod success @ 07/29/23 12:46:23.766
  Jul 29 12:46:23.772: INFO: Trying to get logs from node ip-172-31-33-37 pod client-containers-6f324ed9-fe06-4012-a88c-07374a9013d9 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 12:46:23.781
  Jul 29 12:46:23.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6304" for this suite. @ 07/29/23 12:46:23.806
• [4.089 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 07/29/23 12:46:23.816
  Jul 29 12:46:23.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 12:46:23.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:46:23.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:46:23.838
  STEP: reading a file in the container @ 07/29/23 12:46:25.873
  Jul 29 12:46:25.873: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3780 pod-service-account-def26d15-6718-44e7-b937-0964112316ea -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 07/29/23 12:46:25.994
  Jul 29 12:46:25.994: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3780 pod-service-account-def26d15-6718-44e7-b937-0964112316ea -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 07/29/23 12:46:26.114
  Jul 29 12:46:26.114: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3780 pod-service-account-def26d15-6718-44e7-b937-0964112316ea -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Jul 29 12:46:26.237: INFO: Got root ca configmap in namespace "svcaccounts-3780"
  Jul 29 12:46:26.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3780" for this suite. @ 07/29/23 12:46:26.244
• [2.434 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 07/29/23 12:46:26.251
  Jul 29 12:46:26.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 12:46:26.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:46:26.268
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:46:26.271
  Jul 29 12:46:48.337: INFO: Container started at 2023-07-29 12:46:26 +0000 UTC, pod became ready at 2023-07-29 12:46:46 +0000 UTC
  Jul 29 12:46:48.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1410" for this suite. @ 07/29/23 12:46:48.341
• [22.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 07/29/23 12:46:48.35
  Jul 29 12:46:48.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 12:46:48.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:46:48.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:46:48.372
  STEP: Creating service test in namespace statefulset-7837 @ 07/29/23 12:46:48.374
  STEP: Creating stateful set ss in namespace statefulset-7837 @ 07/29/23 12:46:48.38
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7837 @ 07/29/23 12:46:48.385
  Jul 29 12:46:48.388: INFO: Found 0 stateful pods, waiting for 1
  Jul 29 12:46:58.393: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 07/29/23 12:46:58.393
  Jul 29 12:46:58.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-7837 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 12:46:58.522: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 12:46:58.522: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 12:46:58.522: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 12:46:58.525: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Jul 29 12:47:08.531: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 12:47:08.531: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 12:47:08.553: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
  Jul 29 12:47:08.553: INFO: ss-0  ip-172-31-33-37  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:46:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:46:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:46:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:46:48 +0000 UTC  }]
  Jul 29 12:47:08.553: INFO: 
  Jul 29 12:47:08.553: INFO: StatefulSet ss has not reached scale 3, at 1
  Jul 29 12:47:09.558: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995299753s
  Jul 29 12:47:10.565: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990795709s
  Jul 29 12:47:11.571: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982567714s
  Jul 29 12:47:12.576: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976740621s
  Jul 29 12:47:13.583: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971939877s
  Jul 29 12:47:14.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965409835s
  Jul 29 12:47:15.591: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961186903s
  Jul 29 12:47:16.596: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957218208s
  Jul 29 12:47:17.600: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.319058ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7837 @ 07/29/23 12:47:18.6
  Jul 29 12:47:18.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-7837 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 12:47:18.726: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 12:47:18.726: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 12:47:18.726: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 12:47:18.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-7837 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 12:47:18.870: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul 29 12:47:18.870: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 12:47:18.870: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 12:47:18.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-7837 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 12:47:18.997: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul 29 12:47:18.997: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 12:47:18.997: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 12:47:19.001: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 12:47:19.001: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 12:47:19.001: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 07/29/23 12:47:19.001
  Jul 29 12:47:19.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-7837 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 12:47:19.134: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 12:47:19.134: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 12:47:19.134: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 12:47:19.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-7837 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 12:47:19.264: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 12:47:19.264: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 12:47:19.264: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 12:47:19.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-7837 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 12:47:19.387: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 12:47:19.387: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 12:47:19.387: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 12:47:19.387: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 12:47:19.391: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  Jul 29 12:47:29.399: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 12:47:29.399: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 12:47:29.399: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 12:47:29.411: INFO: POD   NODE             PHASE    GRACE  CONDITIONS
  Jul 29 12:47:29.411: INFO: ss-0  ip-172-31-33-37  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:46:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:46:48 +0000 UTC  }]
  Jul 29 12:47:29.411: INFO: ss-1  ip-172-31-5-66   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:08 +0000 UTC  }]
  Jul 29 12:47:29.411: INFO: ss-2  ip-172-31-19-67  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:08 +0000 UTC  }]
  Jul 29 12:47:29.411: INFO: 
  Jul 29 12:47:29.411: INFO: StatefulSet ss has not reached scale 0, at 3
  Jul 29 12:47:30.415: INFO: POD   NODE             PHASE      GRACE  CONDITIONS
  Jul 29 12:47:30.415: INFO: ss-0  ip-172-31-33-37  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:46:48 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:19 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:19 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:46:48 +0000 UTC  }]
  Jul 29 12:47:30.415: INFO: ss-1  ip-172-31-5-66   Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:08 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:19 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:19 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 12:47:08 +0000 UTC  }]
  Jul 29 12:47:30.415: INFO: 
  Jul 29 12:47:30.415: INFO: StatefulSet ss has not reached scale 0, at 2
  Jul 29 12:47:31.419: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.99225936s
  Jul 29 12:47:32.423: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.988668945s
  Jul 29 12:47:33.428: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.984686979s
  Jul 29 12:47:34.431: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.980242905s
  Jul 29 12:47:35.435: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.97667312s
  Jul 29 12:47:36.439: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.973154085s
  Jul 29 12:47:37.443: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.968529909s
  Jul 29 12:47:38.447: INFO: Verifying statefulset ss doesn't scale past 0 for another 964.249583ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7837 @ 07/29/23 12:47:39.447
  Jul 29 12:47:39.452: INFO: Scaling statefulset ss to 0
  Jul 29 12:47:39.464: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 12:47:39.467: INFO: Deleting all statefulset in ns statefulset-7837
  Jul 29 12:47:39.469: INFO: Scaling statefulset ss to 0
  Jul 29 12:47:39.479: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 12:47:39.482: INFO: Deleting statefulset ss
  Jul 29 12:47:39.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7837" for this suite. @ 07/29/23 12:47:39.498
• [51.156 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 07/29/23 12:47:39.507
  Jul 29 12:47:39.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 12:47:39.508
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:47:39.526
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:47:39.528
  STEP: Creating resourceQuota "e2e-rq-status-478dd" @ 07/29/23 12:47:39.534
  Jul 29 12:47:39.541: INFO: Resource quota "e2e-rq-status-478dd" reports spec: hard cpu limit of 500m
  Jul 29 12:47:39.541: INFO: Resource quota "e2e-rq-status-478dd" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-478dd" /status @ 07/29/23 12:47:39.541
  STEP: Confirm /status for "e2e-rq-status-478dd" resourceQuota via watch @ 07/29/23 12:47:39.566
  Jul 29 12:47:39.568: INFO: observed resourceQuota "e2e-rq-status-478dd" in namespace "resourcequota-868" with hard status: v1.ResourceList(nil)
  Jul 29 12:47:39.568: INFO: Found resourceQuota "e2e-rq-status-478dd" in namespace "resourcequota-868" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul 29 12:47:39.568: INFO: ResourceQuota "e2e-rq-status-478dd" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 07/29/23 12:47:39.572
  Jul 29 12:47:39.578: INFO: Resource quota "e2e-rq-status-478dd" reports spec: hard cpu limit of 1
  Jul 29 12:47:39.578: INFO: Resource quota "e2e-rq-status-478dd" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-478dd" /status @ 07/29/23 12:47:39.578
  STEP: Confirm /status for "e2e-rq-status-478dd" resourceQuota via watch @ 07/29/23 12:47:39.584
  Jul 29 12:47:39.585: INFO: observed resourceQuota "e2e-rq-status-478dd" in namespace "resourcequota-868" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul 29 12:47:39.585: INFO: Found resourceQuota "e2e-rq-status-478dd" in namespace "resourcequota-868" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jul 29 12:47:39.585: INFO: ResourceQuota "e2e-rq-status-478dd" /status was patched
  STEP: Get "e2e-rq-status-478dd" /status @ 07/29/23 12:47:39.585
  Jul 29 12:47:39.589: INFO: Resourcequota "e2e-rq-status-478dd" reports status: hard cpu of 1
  Jul 29 12:47:39.589: INFO: Resourcequota "e2e-rq-status-478dd" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-478dd" /status before checking Spec is unchanged @ 07/29/23 12:47:39.592
  Jul 29 12:47:39.602: INFO: Resourcequota "e2e-rq-status-478dd" reports status: hard cpu of 2
  Jul 29 12:47:39.602: INFO: Resourcequota "e2e-rq-status-478dd" reports status: hard memory of 2Gi
  Jul 29 12:47:39.603: INFO: observed resourceQuota "e2e-rq-status-478dd" in namespace "resourcequota-868" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jul 29 12:47:39.603: INFO: Found resourceQuota "e2e-rq-status-478dd" in namespace "resourcequota-868" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Jul 29 12:51:39.609: INFO: ResourceQuota "e2e-rq-status-478dd" Spec was unchanged and /status reset
  Jul 29 12:51:39.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-868" for this suite. @ 07/29/23 12:51:39.614
• [240.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 07/29/23 12:51:39.624
  Jul 29 12:51:39.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 12:51:39.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:51:39.64
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:51:39.643
  STEP: Creating a ForbidConcurrent cronjob @ 07/29/23 12:51:39.645
  STEP: Ensuring a job is scheduled @ 07/29/23 12:51:39.65
  STEP: Ensuring exactly one is scheduled @ 07/29/23 12:52:01.654
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/29/23 12:52:01.657
  STEP: Ensuring no more jobs are scheduled @ 07/29/23 12:52:01.661
  STEP: Removing cronjob @ 07/29/23 12:57:01.668
  Jul 29 12:57:01.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7204" for this suite. @ 07/29/23 12:57:01.68
• [322.063 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 07/29/23 12:57:01.687
  Jul 29 12:57:01.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 12:57:01.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:57:01.715
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:57:01.719
  Jul 29 12:57:01.739: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul 29 12:57:06.745: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 12:57:06.745
  STEP: Scaling up "test-rs" replicaset  @ 07/29/23 12:57:06.745
  Jul 29 12:57:06.755: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 07/29/23 12:57:06.755
  W0729 12:57:06.764002      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 29 12:57:06.765: INFO: observed ReplicaSet test-rs in namespace replicaset-7240 with ReadyReplicas 1, AvailableReplicas 1
  Jul 29 12:57:06.781: INFO: observed ReplicaSet test-rs in namespace replicaset-7240 with ReadyReplicas 1, AvailableReplicas 1
  Jul 29 12:57:06.811: INFO: observed ReplicaSet test-rs in namespace replicaset-7240 with ReadyReplicas 1, AvailableReplicas 1
  Jul 29 12:57:06.819: INFO: observed ReplicaSet test-rs in namespace replicaset-7240 with ReadyReplicas 1, AvailableReplicas 1
  Jul 29 12:57:07.850: INFO: observed ReplicaSet test-rs in namespace replicaset-7240 with ReadyReplicas 2, AvailableReplicas 2
  Jul 29 12:57:08.093: INFO: observed Replicaset test-rs in namespace replicaset-7240 with ReadyReplicas 3 found true
  Jul 29 12:57:08.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7240" for this suite. @ 07/29/23 12:57:08.098
• [6.418 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 07/29/23 12:57:08.108
  Jul 29 12:57:08.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 12:57:08.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:57:08.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:57:08.127
  STEP: Setting up server cert @ 07/29/23 12:57:08.152
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 12:57:08.559
  STEP: Deploying the webhook pod @ 07/29/23 12:57:08.568
  STEP: Wait for the deployment to be ready @ 07/29/23 12:57:08.695
  Jul 29 12:57:08.702: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 12:57:10.712
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 12:57:10.724
  Jul 29 12:57:11.726: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 07/29/23 12:57:11.73
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/29/23 12:57:11.73
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 07/29/23 12:57:11.749
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 07/29/23 12:57:12.758
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/29/23 12:57:12.758
  STEP: Having no error when timeout is longer than webhook latency @ 07/29/23 12:57:13.791
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/29/23 12:57:13.791
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 07/29/23 12:57:18.826
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/29/23 12:57:18.826
  Jul 29 12:57:23.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7399" for this suite. @ 07/29/23 12:57:23.921
  STEP: Destroying namespace "webhook-markers-9356" for this suite. @ 07/29/23 12:57:23.927
• [15.826 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 07/29/23 12:57:23.936
  Jul 29 12:57:23.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename subpath @ 07/29/23 12:57:23.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:57:23.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:57:23.958
  STEP: Setting up data @ 07/29/23 12:57:23.961
  STEP: Creating pod pod-subpath-test-configmap-6p8z @ 07/29/23 12:57:23.972
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 12:57:23.972
  STEP: Saw pod success @ 07/29/23 12:57:48.037
  Jul 29 12:57:48.041: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-subpath-test-configmap-6p8z container test-container-subpath-configmap-6p8z: <nil>
  STEP: delete the pod @ 07/29/23 12:57:48.059
  STEP: Deleting pod pod-subpath-test-configmap-6p8z @ 07/29/23 12:57:48.081
  Jul 29 12:57:48.081: INFO: Deleting pod "pod-subpath-test-configmap-6p8z" in namespace "subpath-1640"
  Jul 29 12:57:48.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1640" for this suite. @ 07/29/23 12:57:48.087
• [24.158 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 07/29/23 12:57:48.094
  Jul 29 12:57:48.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 12:57:48.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:57:48.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:57:48.121
  Jul 29 12:57:50.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1708" for this suite. @ 07/29/23 12:57:50.154
• [2.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 07/29/23 12:57:50.161
  Jul 29 12:57:50.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 12:57:50.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:57:50.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:57:50.184
  STEP: Creating configMap with name projected-configmap-test-volume-578b0766-d006-435a-b277-9b8677cd9f5a @ 07/29/23 12:57:50.187
  STEP: Creating a pod to test consume configMaps @ 07/29/23 12:57:50.193
  STEP: Saw pod success @ 07/29/23 12:57:54.221
  Jul 29 12:57:54.225: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-projected-configmaps-be919073-ec19-40f0-9bb7-e79e0d3d400b container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 12:57:54.236
  Jul 29 12:57:54.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6907" for this suite. @ 07/29/23 12:57:54.26
• [4.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 07/29/23 12:57:54.271
  Jul 29 12:57:54.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 12:57:54.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:57:54.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:57:54.291
  STEP: creating the pod @ 07/29/23 12:57:54.295
  Jul 29 12:57:54.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6317 create -f -'
  Jul 29 12:57:54.599: INFO: stderr: ""
  Jul 29 12:57:54.599: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 07/29/23 12:57:56.612
  Jul 29 12:57:56.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6317 label pods pause testing-label=testing-label-value'
  Jul 29 12:57:56.695: INFO: stderr: ""
  Jul 29 12:57:56.695: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 07/29/23 12:57:56.696
  Jul 29 12:57:56.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6317 get pod pause -L testing-label'
  Jul 29 12:57:56.759: INFO: stderr: ""
  Jul 29 12:57:56.759: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 07/29/23 12:57:56.759
  Jul 29 12:57:56.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6317 label pods pause testing-label-'
  Jul 29 12:57:56.833: INFO: stderr: ""
  Jul 29 12:57:56.833: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 07/29/23 12:57:56.833
  Jul 29 12:57:56.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6317 get pod pause -L testing-label'
  Jul 29 12:57:56.895: INFO: stderr: ""
  Jul 29 12:57:56.895: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 07/29/23 12:57:56.895
  Jul 29 12:57:56.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6317 delete --grace-period=0 --force -f -'
  Jul 29 12:57:56.973: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 12:57:56.973: INFO: stdout: "pod \"pause\" force deleted\n"
  Jul 29 12:57:56.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6317 get rc,svc -l name=pause --no-headers'
  Jul 29 12:57:57.038: INFO: stderr: "No resources found in kubectl-6317 namespace.\n"
  Jul 29 12:57:57.039: INFO: stdout: ""
  Jul 29 12:57:57.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6317 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 29 12:57:57.097: INFO: stderr: ""
  Jul 29 12:57:57.097: INFO: stdout: ""
  Jul 29 12:57:57.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6317" for this suite. @ 07/29/23 12:57:57.102
• [2.840 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 07/29/23 12:57:57.111
  Jul 29 12:57:57.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 12:57:57.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:57:57.13
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:57:57.137
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 12:57:57.141
  STEP: Saw pod success @ 07/29/23 12:58:01.172
  Jul 29 12:58:01.175: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-c14a0fdd-7bc6-4ef0-96c4-f38ad964fca2 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 12:58:01.182
  Jul 29 12:58:01.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8473" for this suite. @ 07/29/23 12:58:01.202
• [4.098 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 07/29/23 12:58:01.21
  Jul 29 12:58:01.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 12:58:01.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:58:01.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:58:01.231
  STEP: Setting up server cert @ 07/29/23 12:58:01.257
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 12:58:01.537
  STEP: Deploying the webhook pod @ 07/29/23 12:58:01.546
  STEP: Wait for the deployment to be ready @ 07/29/23 12:58:01.561
  Jul 29 12:58:01.568: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 12:58:03.579
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 12:58:03.587
  Jul 29 12:58:04.588: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 07/29/23 12:58:04.664
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 12:58:04.714
  STEP: Deleting the collection of validation webhooks @ 07/29/23 12:58:04.749
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 12:58:04.804
  Jul 29 12:58:04.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2842" for this suite. @ 07/29/23 12:58:04.863
  STEP: Destroying namespace "webhook-markers-4506" for this suite. @ 07/29/23 12:58:04.873
• [3.676 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 07/29/23 12:58:04.887
  Jul 29 12:58:04.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 12:58:04.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:58:04.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:58:04.908
  STEP: Setting up server cert @ 07/29/23 12:58:04.937
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 12:58:05.409
  STEP: Deploying the webhook pod @ 07/29/23 12:58:05.415
  STEP: Wait for the deployment to be ready @ 07/29/23 12:58:05.433
  Jul 29 12:58:05.447: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 12:58:07.46
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 12:58:07.472
  Jul 29 12:58:08.472: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 07/29/23 12:58:08.477
  STEP: create a pod that should be updated by the webhook @ 07/29/23 12:58:08.493
  Jul 29 12:58:08.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6623" for this suite. @ 07/29/23 12:58:08.575
  STEP: Destroying namespace "webhook-markers-8715" for this suite. @ 07/29/23 12:58:08.584
• [3.706 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 07/29/23 12:58:08.594
  Jul 29 12:58:08.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename deployment @ 07/29/23 12:58:08.595
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:58:08.612
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:58:08.615
  Jul 29 12:58:08.620: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jul 29 12:58:08.630: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul 29 12:58:13.634: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 12:58:13.634
  Jul 29 12:58:13.634: INFO: Creating deployment "test-rolling-update-deployment"
  Jul 29 12:58:13.641: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jul 29 12:58:13.650: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  Jul 29 12:58:15.657: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jul 29 12:58:15.660: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jul 29 12:58:15.669: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7313  261b02af-e69b-4d8e-9847-788cb0fd29de 22097 1 2023-07-29 12:58:13 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-07-29 12:58:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 12:58:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bd9f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 12:58:13 +0000 UTC,LastTransitionTime:2023-07-29 12:58:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-07-29 12:58:15 +0000 UTC,LastTransitionTime:2023-07-29 12:58:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 29 12:58:15.673: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-7313  125fb4e5-3002-4006-ad8f-000ea5a6628d 22087 1 2023-07-29 12:58:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 261b02af-e69b-4d8e-9847-788cb0fd29de 0xc00437c407 0xc00437c408}] [] [{kube-controller-manager Update apps/v1 2023-07-29 12:58:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"261b02af-e69b-4d8e-9847-788cb0fd29de\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 12:58:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00437c4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 12:58:15.673: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jul 29 12:58:15.673: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7313  560ab0bb-1a13-40c7-b4ae-926415e106d7 22096 2 2023-07-29 12:58:08 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 261b02af-e69b-4d8e-9847-788cb0fd29de 0xc00437c2d7 0xc00437c2d8}] [] [{e2e.test Update apps/v1 2023-07-29 12:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 12:58:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"261b02af-e69b-4d8e-9847-788cb0fd29de\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-29 12:58:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00437c398 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 12:58:15.677: INFO: Pod "test-rolling-update-deployment-656d657cd8-7qfdm" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-7qfdm test-rolling-update-deployment-656d657cd8- deployment-7313  b945b6fa-8fcc-4f65-b299-bcd89ba71490 22086 0 2023-07-29 12:58:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 125fb4e5-3002-4006-ad8f-000ea5a6628d 0xc00437c937 0xc00437c938}] [] [{kube-controller-manager Update v1 2023-07-29 12:58:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"125fb4e5-3002-4006-ad8f-000ea5a6628d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 12:58:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.10.42\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lq7sb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lq7sb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:58:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:58:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:58:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 12:58:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:192.168.10.42,StartTime:2023-07-29 12:58:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 12:58:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://3d168465672ffe4c88c0f20de5a9b0ae3fe88b128ceacc695587312baa662e60,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.10.42,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 12:58:15.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7313" for this suite. @ 07/29/23 12:58:15.681
• [7.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 07/29/23 12:58:15.688
  Jul 29 12:58:15.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 12:58:15.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:58:15.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:58:15.707
  Jul 29 12:58:15.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  W0729 12:58:18.267510      18 warnings.go:70] unknown field "alpha"
  W0729 12:58:18.267531      18 warnings.go:70] unknown field "beta"
  W0729 12:58:18.267538      18 warnings.go:70] unknown field "delta"
  W0729 12:58:18.267544      18 warnings.go:70] unknown field "epsilon"
  W0729 12:58:18.267550      18 warnings.go:70] unknown field "gamma"
  Jul 29 12:58:18.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8508" for this suite. @ 07/29/23 12:58:18.811
• [3.128 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 07/29/23 12:58:18.817
  Jul 29 12:58:18.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename proxy @ 07/29/23 12:58:18.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:58:18.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:58:18.847
  Jul 29 12:58:18.850: INFO: Creating pod...
  Jul 29 12:58:20.867: INFO: Creating service...
  Jul 29 12:58:20.876: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/pods/agnhost/proxy/some/path/with/DELETE
  Jul 29 12:58:20.884: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 29 12:58:20.884: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/pods/agnhost/proxy/some/path/with/GET
  Jul 29 12:58:20.890: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul 29 12:58:20.890: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/pods/agnhost/proxy/some/path/with/HEAD
  Jul 29 12:58:20.896: INFO: http.Client request:HEAD | StatusCode:200
  Jul 29 12:58:20.896: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/pods/agnhost/proxy/some/path/with/OPTIONS
  Jul 29 12:58:20.899: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 29 12:58:20.899: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/pods/agnhost/proxy/some/path/with/PATCH
  Jul 29 12:58:20.904: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 29 12:58:20.904: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/pods/agnhost/proxy/some/path/with/POST
  Jul 29 12:58:20.909: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 29 12:58:20.909: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/pods/agnhost/proxy/some/path/with/PUT
  Jul 29 12:58:20.912: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 29 12:58:20.912: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/services/test-service/proxy/some/path/with/DELETE
  Jul 29 12:58:20.918: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 29 12:58:20.918: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/services/test-service/proxy/some/path/with/GET
  Jul 29 12:58:20.924: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul 29 12:58:20.924: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/services/test-service/proxy/some/path/with/HEAD
  Jul 29 12:58:20.931: INFO: http.Client request:HEAD | StatusCode:200
  Jul 29 12:58:20.931: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/services/test-service/proxy/some/path/with/OPTIONS
  Jul 29 12:58:20.937: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 29 12:58:20.937: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/services/test-service/proxy/some/path/with/PATCH
  Jul 29 12:58:20.944: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 29 12:58:20.944: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/services/test-service/proxy/some/path/with/POST
  Jul 29 12:58:20.949: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 29 12:58:20.949: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-8860/services/test-service/proxy/some/path/with/PUT
  Jul 29 12:58:20.955: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 29 12:58:20.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-8860" for this suite. @ 07/29/23 12:58:20.96
• [2.150 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 07/29/23 12:58:20.968
  Jul 29 12:58:20.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 12:58:20.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:58:20.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:58:20.99
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 07/29/23 12:58:20.994
  STEP: When a replicaset with a matching selector is created @ 07/29/23 12:58:23.013
  STEP: Then the orphan pod is adopted @ 07/29/23 12:58:23.019
  STEP: When the matched label of one of its pods change @ 07/29/23 12:58:24.025
  Jul 29 12:58:24.029: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/29/23 12:58:24.041
  Jul 29 12:58:25.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-963" for this suite. @ 07/29/23 12:58:25.054
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 07/29/23 12:58:25.063
  Jul 29 12:58:25.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename proxy @ 07/29/23 12:58:25.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:58:25.078
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:58:25.08
  STEP: starting an echo server on multiple ports @ 07/29/23 12:58:25.092
  STEP: creating replication controller proxy-service-mwgmg in namespace proxy-2426 @ 07/29/23 12:58:25.092
  I0729 12:58:25.101149      18 runners.go:194] Created replication controller with name: proxy-service-mwgmg, namespace: proxy-2426, replica count: 1
  I0729 12:58:26.152159      18 runners.go:194] proxy-service-mwgmg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0729 12:58:27.152938      18 runners.go:194] proxy-service-mwgmg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 12:58:27.157: INFO: setup took 2.07433576s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 07/29/23 12:58:27.157
  Jul 29 12:58:27.171: INFO: (0) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 13.597062ms)
  Jul 29 12:58:27.179: INFO: (0) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 21.703488ms)
  Jul 29 12:58:27.179: INFO: (0) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 22.056247ms)
  Jul 29 12:58:27.179: INFO: (0) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 21.786198ms)
  Jul 29 12:58:27.179: INFO: (0) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 21.762151ms)
  Jul 29 12:58:27.183: INFO: (0) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 24.728191ms)
  Jul 29 12:58:27.183: INFO: (0) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 25.38287ms)
  Jul 29 12:58:27.183: INFO: (0) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 25.270642ms)
  Jul 29 12:58:27.183: INFO: (0) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 25.628503ms)
  Jul 29 12:58:27.184: INFO: (0) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 26.379954ms)
  Jul 29 12:58:27.184: INFO: (0) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 26.585048ms)
  Jul 29 12:58:27.184: INFO: (0) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 26.527331ms)
  Jul 29 12:58:27.184: INFO: (0) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 26.964374ms)
  Jul 29 12:58:27.185: INFO: (0) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 26.980115ms)
  Jul 29 12:58:27.185: INFO: (0) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 26.919695ms)
  Jul 29 12:58:27.185: INFO: (0) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 27.566166ms)
  Jul 29 12:58:27.191: INFO: (1) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 5.081102ms)
  Jul 29 12:58:27.191: INFO: (1) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 5.545337ms)
  Jul 29 12:58:27.192: INFO: (1) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 6.763757ms)
  Jul 29 12:58:27.192: INFO: (1) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 7.196777ms)
  Jul 29 12:58:27.193: INFO: (1) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 7.299165ms)
  Jul 29 12:58:27.194: INFO: (1) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 8.373222ms)
  Jul 29 12:58:27.194: INFO: (1) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 8.460391ms)
  Jul 29 12:58:27.194: INFO: (1) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 8.590656ms)
  Jul 29 12:58:27.195: INFO: (1) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 8.867934ms)
  Jul 29 12:58:27.195: INFO: (1) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 9.397458ms)
  Jul 29 12:58:27.196: INFO: (1) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 10.444977ms)
  Jul 29 12:58:27.196: INFO: (1) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 10.470194ms)
  Jul 29 12:58:27.196: INFO: (1) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 10.15223ms)
  Jul 29 12:58:27.197: INFO: (1) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 11.158045ms)
  Jul 29 12:58:27.197: INFO: (1) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 11.53888ms)
  Jul 29 12:58:27.198: INFO: (1) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 12.07279ms)
  Jul 29 12:58:27.202: INFO: (2) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 4.290815ms)
  Jul 29 12:58:27.202: INFO: (2) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 4.634663ms)
  Jul 29 12:58:27.203: INFO: (2) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 4.900063ms)
  Jul 29 12:58:27.204: INFO: (2) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 6.324289ms)
  Jul 29 12:58:27.205: INFO: (2) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 6.38437ms)
  Jul 29 12:58:27.205: INFO: (2) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 6.458599ms)
  Jul 29 12:58:27.206: INFO: (2) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 7.300217ms)
  Jul 29 12:58:27.206: INFO: (2) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 8.039345ms)
  Jul 29 12:58:27.206: INFO: (2) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 8.152954ms)
  Jul 29 12:58:27.207: INFO: (2) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 8.358001ms)
  Jul 29 12:58:27.207: INFO: (2) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 8.911428ms)
  Jul 29 12:58:27.208: INFO: (2) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 9.380747ms)
  Jul 29 12:58:27.208: INFO: (2) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 9.720796ms)
  Jul 29 12:58:27.208: INFO: (2) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 9.950635ms)
  Jul 29 12:58:27.208: INFO: (2) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 10.04911ms)
  Jul 29 12:58:27.208: INFO: (2) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 9.89995ms)
  Jul 29 12:58:27.213: INFO: (3) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 4.349818ms)
  Jul 29 12:58:27.213: INFO: (3) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 5.006769ms)
  Jul 29 12:58:27.214: INFO: (3) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 5.213439ms)
  Jul 29 12:58:27.214: INFO: (3) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 5.345014ms)
  Jul 29 12:58:27.216: INFO: (3) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 7.223991ms)
  Jul 29 12:58:27.216: INFO: (3) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 7.195927ms)
  Jul 29 12:58:27.216: INFO: (3) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 7.711246ms)
  Jul 29 12:58:27.216: INFO: (3) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 7.623177ms)
  Jul 29 12:58:27.217: INFO: (3) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 8.304419ms)
  Jul 29 12:58:27.217: INFO: (3) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 8.00226ms)
  Jul 29 12:58:27.218: INFO: (3) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 8.814666ms)
  Jul 29 12:58:27.218: INFO: (3) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 9.57113ms)
  Jul 29 12:58:27.219: INFO: (3) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 9.636102ms)
  Jul 29 12:58:27.219: INFO: (3) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 10.3264ms)
  Jul 29 12:58:27.219: INFO: (3) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 10.331365ms)
  Jul 29 12:58:27.220: INFO: (3) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 11.162179ms)
  Jul 29 12:58:27.224: INFO: (4) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 4.266239ms)
  Jul 29 12:58:27.226: INFO: (4) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 6.171015ms)
  Jul 29 12:58:27.226: INFO: (4) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 6.10329ms)
  Jul 29 12:58:27.227: INFO: (4) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 6.706036ms)
  Jul 29 12:58:27.227: INFO: (4) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 7.070915ms)
  Jul 29 12:58:27.228: INFO: (4) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 8.420544ms)
  Jul 29 12:58:27.228: INFO: (4) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 8.113737ms)
  Jul 29 12:58:27.228: INFO: (4) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 8.055653ms)
  Jul 29 12:58:27.228: INFO: (4) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 8.257472ms)
  Jul 29 12:58:27.230: INFO: (4) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 10.070256ms)
  Jul 29 12:58:27.231: INFO: (4) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 10.170857ms)
  Jul 29 12:58:27.231: INFO: (4) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 10.574531ms)
  Jul 29 12:58:27.231: INFO: (4) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 10.629797ms)
  Jul 29 12:58:27.231: INFO: (4) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 10.699021ms)
  Jul 29 12:58:27.231: INFO: (4) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 10.985912ms)
  Jul 29 12:58:27.232: INFO: (4) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 11.743814ms)
  Jul 29 12:58:27.237: INFO: (5) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 4.639214ms)
  Jul 29 12:58:27.238: INFO: (5) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 5.557147ms)
  Jul 29 12:58:27.238: INFO: (5) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 5.530618ms)
  Jul 29 12:58:27.238: INFO: (5) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 5.666978ms)
  Jul 29 12:58:27.238: INFO: (5) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 5.843786ms)
  Jul 29 12:58:27.239: INFO: (5) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 6.501776ms)
  Jul 29 12:58:27.239: INFO: (5) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 7.377167ms)
  Jul 29 12:58:27.240: INFO: (5) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 7.753433ms)
  Jul 29 12:58:27.241: INFO: (5) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 8.235843ms)
  Jul 29 12:58:27.241: INFO: (5) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 9.199399ms)
  Jul 29 12:58:27.241: INFO: (5) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 8.920044ms)
  Jul 29 12:58:27.241: INFO: (5) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 8.818249ms)
  Jul 29 12:58:27.242: INFO: (5) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 9.518561ms)
  Jul 29 12:58:27.242: INFO: (5) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 10.078021ms)
  Jul 29 12:58:27.242: INFO: (5) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 10.088838ms)
  Jul 29 12:58:27.243: INFO: (5) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 10.435346ms)
  Jul 29 12:58:27.247: INFO: (6) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 4.291974ms)
  Jul 29 12:58:27.249: INFO: (6) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 5.733951ms)
  Jul 29 12:58:27.250: INFO: (6) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 6.65366ms)
  Jul 29 12:58:27.250: INFO: (6) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 6.758666ms)
  Jul 29 12:58:27.251: INFO: (6) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 7.673509ms)
  Jul 29 12:58:27.251: INFO: (6) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 7.474694ms)
  Jul 29 12:58:27.251: INFO: (6) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 7.781857ms)
  Jul 29 12:58:27.252: INFO: (6) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 8.894239ms)
  Jul 29 12:58:27.252: INFO: (6) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 9.359557ms)
  Jul 29 12:58:27.253: INFO: (6) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 9.38838ms)
  Jul 29 12:58:27.253: INFO: (6) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 9.622968ms)
  Jul 29 12:58:27.253: INFO: (6) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 9.608873ms)
  Jul 29 12:58:27.254: INFO: (6) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 11.43402ms)
  Jul 29 12:58:27.254: INFO: (6) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 11.262598ms)
  Jul 29 12:58:27.254: INFO: (6) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 11.197273ms)
  Jul 29 12:58:27.254: INFO: (6) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 11.247709ms)
  Jul 29 12:58:27.260: INFO: (7) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 4.988976ms)
  Jul 29 12:58:27.260: INFO: (7) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 5.632653ms)
  Jul 29 12:58:27.261: INFO: (7) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 5.649261ms)
  Jul 29 12:58:27.261: INFO: (7) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 5.975296ms)
  Jul 29 12:58:27.262: INFO: (7) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 6.665307ms)
  Jul 29 12:58:27.263: INFO: (7) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 7.550107ms)
  Jul 29 12:58:27.263: INFO: (7) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 8.307432ms)
  Jul 29 12:58:27.264: INFO: (7) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 9.182229ms)
  Jul 29 12:58:27.264: INFO: (7) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 8.911898ms)
  Jul 29 12:58:27.265: INFO: (7) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 10.038249ms)
  Jul 29 12:58:27.265: INFO: (7) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 10.497892ms)
  Jul 29 12:58:27.266: INFO: (7) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 10.968172ms)
  Jul 29 12:58:27.266: INFO: (7) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 11.642802ms)
  Jul 29 12:58:27.268: INFO: (7) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 12.599375ms)
  Jul 29 12:58:27.268: INFO: (7) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 12.675984ms)
  Jul 29 12:58:27.270: INFO: (7) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 14.96181ms)
  Jul 29 12:58:27.276: INFO: (8) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 5.488735ms)
  Jul 29 12:58:27.277: INFO: (8) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 6.54525ms)
  Jul 29 12:58:27.277: INFO: (8) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 6.465769ms)
  Jul 29 12:58:27.279: INFO: (8) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 8.320318ms)
  Jul 29 12:58:27.280: INFO: (8) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 8.903664ms)
  Jul 29 12:58:27.280: INFO: (8) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 9.408791ms)
  Jul 29 12:58:27.280: INFO: (8) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 9.665643ms)
  Jul 29 12:58:27.281: INFO: (8) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 10.307947ms)
  Jul 29 12:58:27.281: INFO: (8) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 10.419548ms)
  Jul 29 12:58:27.281: INFO: (8) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 10.506352ms)
  Jul 29 12:58:27.281: INFO: (8) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 10.63419ms)
  Jul 29 12:58:27.281: INFO: (8) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 10.781274ms)
  Jul 29 12:58:27.281: INFO: (8) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 10.777258ms)
  Jul 29 12:58:27.282: INFO: (8) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 11.008484ms)
  Jul 29 12:58:27.282: INFO: (8) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 11.558391ms)
  Jul 29 12:58:27.283: INFO: (8) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 12.315648ms)
  Jul 29 12:58:27.288: INFO: (9) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 4.841111ms)
  Jul 29 12:58:27.290: INFO: (9) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 6.558167ms)
  Jul 29 12:58:27.290: INFO: (9) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 6.512542ms)
  Jul 29 12:58:27.290: INFO: (9) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 6.615117ms)
  Jul 29 12:58:27.290: INFO: (9) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 6.479856ms)
  Jul 29 12:58:27.291: INFO: (9) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 7.168873ms)
  Jul 29 12:58:27.292: INFO: (9) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 8.015258ms)
  Jul 29 12:58:27.292: INFO: (9) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 7.995385ms)
  Jul 29 12:58:27.292: INFO: (9) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 8.825763ms)
  Jul 29 12:58:27.292: INFO: (9) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 9.055497ms)
  Jul 29 12:58:27.293: INFO: (9) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 8.856435ms)
  Jul 29 12:58:27.293: INFO: (9) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 9.25233ms)
  Jul 29 12:58:27.293: INFO: (9) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 9.516225ms)
  Jul 29 12:58:27.294: INFO: (9) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 10.484265ms)
  Jul 29 12:58:27.295: INFO: (9) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 11.210793ms)
  Jul 29 12:58:27.295: INFO: (9) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 11.16821ms)
  Jul 29 12:58:27.300: INFO: (10) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 4.563839ms)
  Jul 29 12:58:27.300: INFO: (10) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 4.73892ms)
  Jul 29 12:58:27.300: INFO: (10) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 5.190998ms)
  Jul 29 12:58:27.301: INFO: (10) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 5.992548ms)
  Jul 29 12:58:27.302: INFO: (10) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 6.940544ms)
  Jul 29 12:58:27.302: INFO: (10) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 7.04366ms)
  Jul 29 12:58:27.302: INFO: (10) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 7.135019ms)
  Jul 29 12:58:27.303: INFO: (10) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 7.619226ms)
  Jul 29 12:58:27.304: INFO: (10) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 9.010406ms)
  Jul 29 12:58:27.305: INFO: (10) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 9.286882ms)
  Jul 29 12:58:27.305: INFO: (10) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 9.357557ms)
  Jul 29 12:58:27.305: INFO: (10) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 9.579246ms)
  Jul 29 12:58:27.306: INFO: (10) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 9.82604ms)
  Jul 29 12:58:27.306: INFO: (10) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 10.285106ms)
  Jul 29 12:58:27.306: INFO: (10) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 10.894945ms)
  Jul 29 12:58:27.307: INFO: (10) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 11.552935ms)
  Jul 29 12:58:27.312: INFO: (11) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 5.624355ms)
  Jul 29 12:58:27.313: INFO: (11) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 6.794889ms)
  Jul 29 12:58:27.314: INFO: (11) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 7.133512ms)
  Jul 29 12:58:27.314: INFO: (11) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 7.25137ms)
  Jul 29 12:58:27.314: INFO: (11) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 7.118903ms)
  Jul 29 12:58:27.315: INFO: (11) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 8.437698ms)
  Jul 29 12:58:27.315: INFO: (11) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 8.287288ms)
  Jul 29 12:58:27.316: INFO: (11) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 9.161938ms)
  Jul 29 12:58:27.317: INFO: (11) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 9.35823ms)
  Jul 29 12:58:27.317: INFO: (11) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 10.040079ms)
  Jul 29 12:58:27.317: INFO: (11) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 10.563886ms)
  Jul 29 12:58:27.318: INFO: (11) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 10.588881ms)
  Jul 29 12:58:27.318: INFO: (11) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 11.227696ms)
  Jul 29 12:58:27.318: INFO: (11) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 10.919048ms)
  Jul 29 12:58:27.318: INFO: (11) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 11.598586ms)
  Jul 29 12:58:27.319: INFO: (11) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 12.494053ms)
  Jul 29 12:58:27.324: INFO: (12) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 4.21002ms)
  Jul 29 12:58:27.325: INFO: (12) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 5.49387ms)
  Jul 29 12:58:27.326: INFO: (12) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 6.154066ms)
  Jul 29 12:58:27.326: INFO: (12) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 6.173147ms)
  Jul 29 12:58:27.327: INFO: (12) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 6.874236ms)
  Jul 29 12:58:27.328: INFO: (12) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 7.864289ms)
  Jul 29 12:58:27.328: INFO: (12) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 7.943929ms)
  Jul 29 12:58:27.328: INFO: (12) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 8.308345ms)
  Jul 29 12:58:27.329: INFO: (12) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 9.016229ms)
  Jul 29 12:58:27.329: INFO: (12) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 9.615063ms)
  Jul 29 12:58:27.329: INFO: (12) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 9.572675ms)
  Jul 29 12:58:27.330: INFO: (12) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 10.102628ms)
  Jul 29 12:58:27.330: INFO: (12) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 10.107743ms)
  Jul 29 12:58:27.330: INFO: (12) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 10.447466ms)
  Jul 29 12:58:27.330: INFO: (12) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 10.483574ms)
  Jul 29 12:58:27.331: INFO: (12) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 10.707167ms)
  Jul 29 12:58:27.335: INFO: (13) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 4.390337ms)
  Jul 29 12:58:27.335: INFO: (13) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 4.537468ms)
  Jul 29 12:58:27.337: INFO: (13) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 5.640886ms)
  Jul 29 12:58:27.337: INFO: (13) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 5.792445ms)
  Jul 29 12:58:27.337: INFO: (13) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 5.95149ms)
  Jul 29 12:58:27.337: INFO: (13) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 6.245189ms)
  Jul 29 12:58:27.338: INFO: (13) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 7.046425ms)
  Jul 29 12:58:27.338: INFO: (13) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 6.932016ms)
  Jul 29 12:58:27.339: INFO: (13) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 7.58397ms)
  Jul 29 12:58:27.339: INFO: (13) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 7.922584ms)
  Jul 29 12:58:27.340: INFO: (13) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 8.882954ms)
  Jul 29 12:58:27.340: INFO: (13) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 8.931078ms)
  Jul 29 12:58:27.340: INFO: (13) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 9.200399ms)
  Jul 29 12:58:27.341: INFO: (13) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 10.086504ms)
  Jul 29 12:58:27.341: INFO: (13) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 10.170846ms)
  Jul 29 12:58:27.341: INFO: (13) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 10.339293ms)
  Jul 29 12:58:27.346: INFO: (14) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 4.281557ms)
  Jul 29 12:58:27.346: INFO: (14) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 4.797686ms)
  Jul 29 12:58:27.347: INFO: (14) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 5.632662ms)
  Jul 29 12:58:27.348: INFO: (14) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 5.732969ms)
  Jul 29 12:58:27.348: INFO: (14) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 6.166883ms)
  Jul 29 12:58:27.349: INFO: (14) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 7.164068ms)
  Jul 29 12:58:27.350: INFO: (14) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 7.979788ms)
  Jul 29 12:58:27.350: INFO: (14) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 7.971274ms)
  Jul 29 12:58:27.350: INFO: (14) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 8.464672ms)
  Jul 29 12:58:27.351: INFO: (14) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 8.906318ms)
  Jul 29 12:58:27.351: INFO: (14) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 8.868742ms)
  Jul 29 12:58:27.351: INFO: (14) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 9.308463ms)
  Jul 29 12:58:27.352: INFO: (14) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 9.625346ms)
  Jul 29 12:58:27.352: INFO: (14) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 10.131724ms)
  Jul 29 12:58:27.352: INFO: (14) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 10.188363ms)
  Jul 29 12:58:27.353: INFO: (14) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 10.996079ms)
  Jul 29 12:58:27.357: INFO: (15) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 4.151632ms)
  Jul 29 12:58:27.357: INFO: (15) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 4.23527ms)
  Jul 29 12:58:27.359: INFO: (15) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 5.619754ms)
  Jul 29 12:58:27.359: INFO: (15) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 5.831551ms)
  Jul 29 12:58:27.359: INFO: (15) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 6.116343ms)
  Jul 29 12:58:27.360: INFO: (15) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 7.393506ms)
  Jul 29 12:58:27.361: INFO: (15) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 7.999048ms)
  Jul 29 12:58:27.362: INFO: (15) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 7.993977ms)
  Jul 29 12:58:27.361: INFO: (15) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 8.18383ms)
  Jul 29 12:58:27.362: INFO: (15) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 8.818105ms)
  Jul 29 12:58:27.363: INFO: (15) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 9.310851ms)
  Jul 29 12:58:27.363: INFO: (15) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 9.911035ms)
  Jul 29 12:58:27.363: INFO: (15) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 9.511046ms)
  Jul 29 12:58:27.363: INFO: (15) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 10.228633ms)
  Jul 29 12:58:27.364: INFO: (15) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 10.317518ms)
  Jul 29 12:58:27.364: INFO: (15) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 10.521729ms)
  Jul 29 12:58:27.368: INFO: (16) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 4.206897ms)
  Jul 29 12:58:27.369: INFO: (16) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 4.785613ms)
  Jul 29 12:58:27.370: INFO: (16) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 5.355224ms)
  Jul 29 12:58:27.370: INFO: (16) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 6.307169ms)
  Jul 29 12:58:27.371: INFO: (16) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 7.347719ms)
  Jul 29 12:58:27.371: INFO: (16) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 7.17291ms)
  Jul 29 12:58:27.372: INFO: (16) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 8.018677ms)
  Jul 29 12:58:27.372: INFO: (16) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 8.366676ms)
  Jul 29 12:58:27.373: INFO: (16) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 8.217771ms)
  Jul 29 12:58:27.373: INFO: (16) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 8.892897ms)
  Jul 29 12:58:27.374: INFO: (16) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 9.296114ms)
  Jul 29 12:58:27.374: INFO: (16) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 9.505891ms)
  Jul 29 12:58:27.374: INFO: (16) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 9.580509ms)
  Jul 29 12:58:27.374: INFO: (16) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 9.736615ms)
  Jul 29 12:58:27.374: INFO: (16) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 10.350177ms)
  Jul 29 12:58:27.375: INFO: (16) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 10.36689ms)
  Jul 29 12:58:27.379: INFO: (17) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 4.204347ms)
  Jul 29 12:58:27.381: INFO: (17) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 5.832224ms)
  Jul 29 12:58:27.381: INFO: (17) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 5.876391ms)
  Jul 29 12:58:27.381: INFO: (17) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 5.909161ms)
  Jul 29 12:58:27.381: INFO: (17) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 6.032343ms)
  Jul 29 12:58:27.382: INFO: (17) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 7.175666ms)
  Jul 29 12:58:27.383: INFO: (17) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 7.835792ms)
  Jul 29 12:58:27.383: INFO: (17) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 7.878451ms)
  Jul 29 12:58:27.385: INFO: (17) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 9.632107ms)
  Jul 29 12:58:27.385: INFO: (17) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 9.691091ms)
  Jul 29 12:58:27.385: INFO: (17) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 9.650266ms)
  Jul 29 12:58:27.385: INFO: (17) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 9.90943ms)
  Jul 29 12:58:27.386: INFO: (17) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 10.692277ms)
  Jul 29 12:58:27.386: INFO: (17) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 11.056921ms)
  Jul 29 12:58:27.386: INFO: (17) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 10.676277ms)
  Jul 29 12:58:27.387: INFO: (17) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 11.376249ms)
  Jul 29 12:58:27.391: INFO: (18) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 4.335994ms)
  Jul 29 12:58:27.392: INFO: (18) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 5.130987ms)
  Jul 29 12:58:27.394: INFO: (18) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 6.69829ms)
  Jul 29 12:58:27.394: INFO: (18) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 6.288598ms)
  Jul 29 12:58:27.394: INFO: (18) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 6.387214ms)
  Jul 29 12:58:27.394: INFO: (18) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 7.358911ms)
  Jul 29 12:58:27.395: INFO: (18) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 8.552055ms)
  Jul 29 12:58:27.395: INFO: (18) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 8.254608ms)
  Jul 29 12:58:27.397: INFO: (18) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 9.269419ms)
  Jul 29 12:58:27.397: INFO: (18) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 9.329798ms)
  Jul 29 12:58:27.397: INFO: (18) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 9.528049ms)
  Jul 29 12:58:27.397: INFO: (18) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 10.30778ms)
  Jul 29 12:58:27.398: INFO: (18) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 10.518955ms)
  Jul 29 12:58:27.398: INFO: (18) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 10.500011ms)
  Jul 29 12:58:27.398: INFO: (18) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 10.81328ms)
  Jul 29 12:58:27.398: INFO: (18) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 10.965435ms)
  Jul 29 12:58:27.403: INFO: (19) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:460/proxy/: tls baz (200; 4.56945ms)
  Jul 29 12:58:27.404: INFO: (19) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 5.734619ms)
  Jul 29 12:58:27.405: INFO: (19) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:160/proxy/: foo (200; 6.238483ms)
  Jul 29 12:58:27.405: INFO: (19) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">test<... (200; 6.643571ms)
  Jul 29 12:58:27.406: INFO: (19) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 7.653558ms)
  Jul 29 12:58:27.408: INFO: (19) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:1080/proxy/rewriteme">... (200; 9.823252ms)
  Jul 29 12:58:27.408: INFO: (19) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname1/proxy/: tls baz (200; 9.592348ms)
  Jul 29 12:58:27.409: INFO: (19) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname1/proxy/: foo (200; 9.811069ms)
  Jul 29 12:58:27.409: INFO: (19) /api/v1/namespaces/proxy-2426/pods/http:proxy-service-mwgmg-l79gt:162/proxy/: bar (200; 10.123456ms)
  Jul 29 12:58:27.409: INFO: (19) /api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/proxy-service-mwgmg-l79gt/proxy/rewriteme">test</a> (200; 9.859603ms)
  Jul 29 12:58:27.409: INFO: (19) /api/v1/namespaces/proxy-2426/services/proxy-service-mwgmg:portname2/proxy/: bar (200; 10.358744ms)
  Jul 29 12:58:27.409: INFO: (19) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:462/proxy/: tls qux (200; 10.743195ms)
  Jul 29 12:58:27.409: INFO: (19) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname1/proxy/: foo (200; 10.801544ms)
  Jul 29 12:58:27.409: INFO: (19) /api/v1/namespaces/proxy-2426/services/https:proxy-service-mwgmg:tlsportname2/proxy/: tls qux (200; 11.020367ms)
  Jul 29 12:58:27.410: INFO: (19) /api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/: <a href="/api/v1/namespaces/proxy-2426/pods/https:proxy-service-mwgmg-l79gt:443/proxy/tlsrewritem... (200; 11.428264ms)
  Jul 29 12:58:27.410: INFO: (19) /api/v1/namespaces/proxy-2426/services/http:proxy-service-mwgmg:portname2/proxy/: bar (200; 11.226714ms)
  Jul 29 12:58:27.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-mwgmg in namespace proxy-2426, will wait for the garbage collector to delete the pods @ 07/29/23 12:58:27.413
  Jul 29 12:58:27.473: INFO: Deleting ReplicationController proxy-service-mwgmg took: 5.471547ms
  Jul 29 12:58:27.574: INFO: Terminating ReplicationController proxy-service-mwgmg pods took: 100.606575ms
  STEP: Destroying namespace "proxy-2426" for this suite. @ 07/29/23 12:58:30.375
• [5.321 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 07/29/23 12:58:30.389
  Jul 29 12:58:30.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 12:58:30.39
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:58:30.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:58:30.417
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 07/29/23 12:58:30.421
  Jul 29 12:58:30.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:58:31.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:58:37.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7528" for this suite. @ 07/29/23 12:58:37.056
• [6.674 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 07/29/23 12:58:37.063
  Jul 29 12:58:37.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename svc-latency @ 07/29/23 12:58:37.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:58:37.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:58:37.081
  Jul 29 12:58:37.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-2642 @ 07/29/23 12:58:37.084
  I0729 12:58:37.091316      18 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2642, replica count: 1
  I0729 12:58:38.143233      18 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0729 12:58:39.143908      18 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 12:58:39.255: INFO: Created: latency-svc-rftzp
  Jul 29 12:58:39.265: INFO: Got endpoints: latency-svc-rftzp [20.740558ms]
  Jul 29 12:58:39.281: INFO: Created: latency-svc-l9tbp
  Jul 29 12:58:39.287: INFO: Created: latency-svc-2x7nj
  Jul 29 12:58:39.289: INFO: Got endpoints: latency-svc-l9tbp [23.107017ms]
  Jul 29 12:58:39.299: INFO: Got endpoints: latency-svc-2x7nj [33.446568ms]
  Jul 29 12:58:39.302: INFO: Created: latency-svc-4nnxv
  Jul 29 12:58:39.307: INFO: Got endpoints: latency-svc-4nnxv [41.020923ms]
  Jul 29 12:58:39.312: INFO: Created: latency-svc-qmmz8
  Jul 29 12:58:39.316: INFO: Created: latency-svc-5pmlc
  Jul 29 12:58:39.318: INFO: Got endpoints: latency-svc-qmmz8 [52.297996ms]
  Jul 29 12:58:39.325: INFO: Got endpoints: latency-svc-5pmlc [58.213505ms]
  Jul 29 12:58:39.330: INFO: Created: latency-svc-2xv58
  Jul 29 12:58:39.333: INFO: Got endpoints: latency-svc-2xv58 [66.99177ms]
  Jul 29 12:58:39.338: INFO: Created: latency-svc-qz5cw
  Jul 29 12:58:39.342: INFO: Created: latency-svc-hjgv2
  Jul 29 12:58:39.344: INFO: Got endpoints: latency-svc-qz5cw [78.363319ms]
  Jul 29 12:58:39.357: INFO: Created: latency-svc-tvwb5
  Jul 29 12:58:39.359: INFO: Got endpoints: latency-svc-hjgv2 [93.815652ms]
  Jul 29 12:58:39.370: INFO: Got endpoints: latency-svc-tvwb5 [104.694095ms]
  Jul 29 12:58:39.372: INFO: Created: latency-svc-qtzf8
  Jul 29 12:58:39.377: INFO: Created: latency-svc-88hf2
  Jul 29 12:58:39.380: INFO: Got endpoints: latency-svc-qtzf8 [114.129272ms]
  Jul 29 12:58:39.383: INFO: Got endpoints: latency-svc-88hf2 [116.291976ms]
  Jul 29 12:58:39.387: INFO: Created: latency-svc-fw7lt
  Jul 29 12:58:39.394: INFO: Got endpoints: latency-svc-fw7lt [127.732517ms]
  Jul 29 12:58:39.396: INFO: Created: latency-svc-l6ftr
  Jul 29 12:58:39.401: INFO: Created: latency-svc-m4vpx
  Jul 29 12:58:39.404: INFO: Got endpoints: latency-svc-l6ftr [137.901292ms]
  Jul 29 12:58:39.407: INFO: Got endpoints: latency-svc-m4vpx [141.138962ms]
  Jul 29 12:58:39.412: INFO: Created: latency-svc-gg8q2
  Jul 29 12:58:39.419: INFO: Got endpoints: latency-svc-gg8q2 [152.088109ms]
  Jul 29 12:58:39.423: INFO: Created: latency-svc-dbb85
  Jul 29 12:58:39.424: INFO: Created: latency-svc-kr2cg
  Jul 29 12:58:39.426: INFO: Got endpoints: latency-svc-dbb85 [136.608281ms]
  Jul 29 12:58:39.431: INFO: Got endpoints: latency-svc-kr2cg [130.247234ms]
  Jul 29 12:58:39.435: INFO: Created: latency-svc-c5tsv
  Jul 29 12:58:39.443: INFO: Got endpoints: latency-svc-c5tsv [135.486519ms]
  Jul 29 12:58:39.515: INFO: Created: latency-svc-5wtjc
  Jul 29 12:58:39.515: INFO: Created: latency-svc-wbh4s
  Jul 29 12:58:39.517: INFO: Created: latency-svc-x7mjv
  Jul 29 12:58:39.519: INFO: Created: latency-svc-cbk9n
  Jul 29 12:58:39.519: INFO: Created: latency-svc-kk2n4
  Jul 29 12:58:39.528: INFO: Created: latency-svc-5flpb
  Jul 29 12:58:39.528: INFO: Created: latency-svc-x744c
  Jul 29 12:58:39.536: INFO: Created: latency-svc-jth6s
  Jul 29 12:58:39.537: INFO: Created: latency-svc-vzcmh
  Jul 29 12:58:39.537: INFO: Created: latency-svc-j47ng
  Jul 29 12:58:39.537: INFO: Created: latency-svc-xqbq6
  Jul 29 12:58:39.537: INFO: Got endpoints: latency-svc-x7mjv [143.37332ms]
  Jul 29 12:58:39.538: INFO: Created: latency-svc-9vmkw
  Jul 29 12:58:39.538: INFO: Got endpoints: latency-svc-5wtjc [204.242261ms]
  Jul 29 12:58:39.538: INFO: Got endpoints: latency-svc-wbh4s [219.804727ms]
  Jul 29 12:58:39.539: INFO: Got endpoints: latency-svc-kk2n4 [158.214808ms]
  Jul 29 12:58:39.541: INFO: Created: latency-svc-zqcqz
  Jul 29 12:58:39.541: INFO: Created: latency-svc-92bcp
  Jul 29 12:58:39.542: INFO: Created: latency-svc-b6tkk
  Jul 29 12:58:39.557: INFO: Created: latency-svc-sl8rx
  Jul 29 12:58:39.561: INFO: Got endpoints: latency-svc-vzcmh [135.233276ms]
  Jul 29 12:58:39.562: INFO: Got endpoints: latency-svc-5flpb [118.986457ms]
  Jul 29 12:58:39.562: INFO: Got endpoints: latency-svc-xqbq6 [216.892404ms]
  Jul 29 12:58:39.563: INFO: Got endpoints: latency-svc-cbk9n [192.160756ms]
  Jul 29 12:58:39.563: INFO: Got endpoints: latency-svc-b6tkk [180.343236ms]
  Jul 29 12:58:39.573: INFO: Created: latency-svc-9ncxr
  Jul 29 12:58:39.576: INFO: Got endpoints: latency-svc-x744c [168.905141ms]
  Jul 29 12:58:39.579: INFO: Got endpoints: latency-svc-j47ng [160.807836ms]
  Jul 29 12:58:39.580: INFO: Got endpoints: latency-svc-92bcp [254.78474ms]
  Jul 29 12:58:39.580: INFO: Got endpoints: latency-svc-jth6s [220.079645ms]
  Jul 29 12:58:39.581: INFO: Got endpoints: latency-svc-zqcqz [150.943943ms]
  Jul 29 12:58:39.585: INFO: Created: latency-svc-vwsnb
  Jul 29 12:58:39.591: INFO: Got endpoints: latency-svc-sl8rx [53.900528ms]
  Jul 29 12:58:39.593: INFO: Got endpoints: latency-svc-9vmkw [189.074389ms]
  Jul 29 12:58:39.594: INFO: Got endpoints: latency-svc-9ncxr [55.139782ms]
  Jul 29 12:58:39.598: INFO: Created: latency-svc-vjfms
  Jul 29 12:58:39.601: INFO: Created: latency-svc-2drh9
  Jul 29 12:58:39.612: INFO: Got endpoints: latency-svc-vwsnb [73.976677ms]
  Jul 29 12:58:39.665: INFO: Created: latency-svc-fksf7
  Jul 29 12:58:39.671: INFO: Created: latency-svc-d6qjb
  Jul 29 12:58:39.672: INFO: Created: latency-svc-gpjzj
  Jul 29 12:58:39.672: INFO: Created: latency-svc-pz2sh
  Jul 29 12:58:39.676: INFO: Got endpoints: latency-svc-vjfms [138.307784ms]
  Jul 29 12:58:39.677: INFO: Created: latency-svc-xcgth
  Jul 29 12:58:39.684: INFO: Created: latency-svc-8kvwg
  Jul 29 12:58:39.685: INFO: Created: latency-svc-fk24h
  Jul 29 12:58:39.685: INFO: Created: latency-svc-hhwhs
  Jul 29 12:58:39.686: INFO: Created: latency-svc-t54fp
  Jul 29 12:58:39.686: INFO: Created: latency-svc-8hjm7
  Jul 29 12:58:39.687: INFO: Created: latency-svc-kzr9w
  Jul 29 12:58:39.687: INFO: Created: latency-svc-7t6s9
  Jul 29 12:58:39.687: INFO: Created: latency-svc-zz6b5
  Jul 29 12:58:39.694: INFO: Created: latency-svc-r4t7n
  Jul 29 12:58:39.713: INFO: Got endpoints: latency-svc-2drh9 [151.228986ms]
  Jul 29 12:58:39.721: INFO: Created: latency-svc-wzmzc
  Jul 29 12:58:39.764: INFO: Got endpoints: latency-svc-fksf7 [201.706831ms]
  Jul 29 12:58:39.774: INFO: Created: latency-svc-wk82f
  Jul 29 12:58:39.814: INFO: Got endpoints: latency-svc-hhwhs [252.24513ms]
  Jul 29 12:58:39.825: INFO: Created: latency-svc-hrdd5
  Jul 29 12:58:39.862: INFO: Got endpoints: latency-svc-gpjzj [281.529296ms]
  Jul 29 12:58:39.872: INFO: Created: latency-svc-dtqr7
  Jul 29 12:58:39.913: INFO: Got endpoints: latency-svc-pz2sh [336.922933ms]
  Jul 29 12:58:39.923: INFO: Created: latency-svc-mgcx6
  Jul 29 12:58:39.965: INFO: Got endpoints: latency-svc-d6qjb [373.989662ms]
  Jul 29 12:58:39.976: INFO: Created: latency-svc-9mzm8
  Jul 29 12:58:40.014: INFO: Got endpoints: latency-svc-t54fp [451.156168ms]
  Jul 29 12:58:40.046: INFO: Created: latency-svc-strll
  Jul 29 12:58:40.078: INFO: Got endpoints: latency-svc-zz6b5 [496.708203ms]
  Jul 29 12:58:40.115: INFO: Got endpoints: latency-svc-fk24h [552.268678ms]
  Jul 29 12:58:40.118: INFO: Created: latency-svc-r74s5
  Jul 29 12:58:40.129: INFO: Created: latency-svc-jl629
  Jul 29 12:58:40.163: INFO: Got endpoints: latency-svc-xcgth [568.776088ms]
  Jul 29 12:58:40.172: INFO: Created: latency-svc-lg8xn
  Jul 29 12:58:40.215: INFO: Got endpoints: latency-svc-8kvwg [603.435418ms]
  Jul 29 12:58:40.225: INFO: Created: latency-svc-7xpt9
  Jul 29 12:58:40.265: INFO: Got endpoints: latency-svc-7t6s9 [685.205035ms]
  Jul 29 12:58:40.277: INFO: Created: latency-svc-jc4qr
  Jul 29 12:58:40.313: INFO: Got endpoints: latency-svc-8hjm7 [719.620597ms]
  Jul 29 12:58:40.323: INFO: Created: latency-svc-9bmqp
  Jul 29 12:58:40.364: INFO: Got endpoints: latency-svc-kzr9w [783.926373ms]
  Jul 29 12:58:40.381: INFO: Created: latency-svc-fvp7q
  Jul 29 12:58:40.419: INFO: Got endpoints: latency-svc-r4t7n [742.543822ms]
  Jul 29 12:58:40.432: INFO: Created: latency-svc-tzz8m
  Jul 29 12:58:40.464: INFO: Got endpoints: latency-svc-wzmzc [751.800649ms]
  Jul 29 12:58:40.473: INFO: Created: latency-svc-jprl2
  Jul 29 12:58:40.513: INFO: Got endpoints: latency-svc-wk82f [749.37824ms]
  Jul 29 12:58:40.525: INFO: Created: latency-svc-55frt
  Jul 29 12:58:40.564: INFO: Got endpoints: latency-svc-hrdd5 [749.850868ms]
  Jul 29 12:58:40.577: INFO: Created: latency-svc-2zp8l
  Jul 29 12:58:40.614: INFO: Got endpoints: latency-svc-dtqr7 [751.688399ms]
  Jul 29 12:58:40.625: INFO: Created: latency-svc-htfr4
  Jul 29 12:58:40.663: INFO: Got endpoints: latency-svc-mgcx6 [749.816431ms]
  Jul 29 12:58:40.672: INFO: Created: latency-svc-rbtdp
  Jul 29 12:58:40.716: INFO: Got endpoints: latency-svc-9mzm8 [750.261164ms]
  Jul 29 12:58:40.728: INFO: Created: latency-svc-b64d2
  Jul 29 12:58:40.763: INFO: Got endpoints: latency-svc-strll [749.119454ms]
  Jul 29 12:58:40.772: INFO: Created: latency-svc-7k4m5
  Jul 29 12:58:40.813: INFO: Got endpoints: latency-svc-r74s5 [734.385482ms]
  Jul 29 12:58:40.823: INFO: Created: latency-svc-j9gsv
  Jul 29 12:58:40.864: INFO: Got endpoints: latency-svc-jl629 [748.171015ms]
  Jul 29 12:58:40.877: INFO: Created: latency-svc-wft2h
  Jul 29 12:58:40.914: INFO: Got endpoints: latency-svc-lg8xn [751.398057ms]
  Jul 29 12:58:40.922: INFO: Created: latency-svc-ltvt9
  Jul 29 12:58:40.965: INFO: Got endpoints: latency-svc-7xpt9 [750.12159ms]
  Jul 29 12:58:40.975: INFO: Created: latency-svc-k45xd
  Jul 29 12:58:41.014: INFO: Got endpoints: latency-svc-jc4qr [748.131872ms]
  Jul 29 12:58:41.027: INFO: Created: latency-svc-frfqr
  Jul 29 12:58:41.062: INFO: Got endpoints: latency-svc-9bmqp [749.462728ms]
  Jul 29 12:58:41.072: INFO: Created: latency-svc-tffb2
  Jul 29 12:58:41.118: INFO: Got endpoints: latency-svc-fvp7q [754.378101ms]
  Jul 29 12:58:41.156: INFO: Created: latency-svc-wj7dv
  Jul 29 12:58:41.225: INFO: Got endpoints: latency-svc-tzz8m [805.834219ms]
  Jul 29 12:58:41.258: INFO: Got endpoints: latency-svc-jprl2 [793.616533ms]
  Jul 29 12:58:41.267: INFO: Got endpoints: latency-svc-55frt [753.804829ms]
  Jul 29 12:58:41.279: INFO: Created: latency-svc-dpcq9
  Jul 29 12:58:41.280: INFO: Created: latency-svc-pzb74
  Jul 29 12:58:41.285: INFO: Created: latency-svc-dmbv8
  Jul 29 12:58:41.313: INFO: Got endpoints: latency-svc-2zp8l [749.143849ms]
  Jul 29 12:58:41.324: INFO: Created: latency-svc-rrj6c
  Jul 29 12:58:41.364: INFO: Got endpoints: latency-svc-htfr4 [750.005137ms]
  Jul 29 12:58:41.374: INFO: Created: latency-svc-tpkdl
  Jul 29 12:58:41.413: INFO: Got endpoints: latency-svc-rbtdp [750.581981ms]
  Jul 29 12:58:41.423: INFO: Created: latency-svc-n6kls
  Jul 29 12:58:41.465: INFO: Got endpoints: latency-svc-b64d2 [749.30696ms]
  Jul 29 12:58:41.482: INFO: Created: latency-svc-7jc6b
  Jul 29 12:58:41.513: INFO: Got endpoints: latency-svc-7k4m5 [749.660302ms]
  Jul 29 12:58:41.522: INFO: Created: latency-svc-nbvk9
  Jul 29 12:58:41.564: INFO: Got endpoints: latency-svc-j9gsv [750.995246ms]
  Jul 29 12:58:41.574: INFO: Created: latency-svc-ndmzc
  Jul 29 12:58:41.613: INFO: Got endpoints: latency-svc-wft2h [749.292978ms]
  Jul 29 12:58:41.626: INFO: Created: latency-svc-slcrj
  Jul 29 12:58:41.662: INFO: Got endpoints: latency-svc-ltvt9 [747.543796ms]
  Jul 29 12:58:41.670: INFO: Created: latency-svc-sgqbh
  Jul 29 12:58:41.715: INFO: Got endpoints: latency-svc-k45xd [749.132345ms]
  Jul 29 12:58:41.732: INFO: Created: latency-svc-rc9fd
  Jul 29 12:58:41.765: INFO: Got endpoints: latency-svc-frfqr [750.893285ms]
  Jul 29 12:58:41.776: INFO: Created: latency-svc-6wn7h
  Jul 29 12:58:41.812: INFO: Got endpoints: latency-svc-tffb2 [750.163108ms]
  Jul 29 12:58:41.821: INFO: Created: latency-svc-qx7wg
  Jul 29 12:58:41.862: INFO: Got endpoints: latency-svc-wj7dv [743.906939ms]
  Jul 29 12:58:41.872: INFO: Created: latency-svc-4d89c
  Jul 29 12:58:41.913: INFO: Got endpoints: latency-svc-dpcq9 [688.329604ms]
  Jul 29 12:58:41.924: INFO: Created: latency-svc-pxs2q
  Jul 29 12:58:41.964: INFO: Got endpoints: latency-svc-pzb74 [705.063075ms]
  Jul 29 12:58:41.972: INFO: Created: latency-svc-9pkfx
  Jul 29 12:58:42.019: INFO: Got endpoints: latency-svc-dmbv8 [751.298018ms]
  Jul 29 12:58:42.031: INFO: Created: latency-svc-p9727
  Jul 29 12:58:42.063: INFO: Got endpoints: latency-svc-rrj6c [749.501864ms]
  Jul 29 12:58:42.075: INFO: Created: latency-svc-ptrrh
  Jul 29 12:58:42.113: INFO: Got endpoints: latency-svc-tpkdl [748.775112ms]
  Jul 29 12:58:42.123: INFO: Created: latency-svc-vtfjw
  Jul 29 12:58:42.163: INFO: Got endpoints: latency-svc-n6kls [749.614909ms]
  Jul 29 12:58:42.175: INFO: Created: latency-svc-dv24v
  Jul 29 12:58:42.216: INFO: Got endpoints: latency-svc-7jc6b [751.125421ms]
  Jul 29 12:58:42.243: INFO: Created: latency-svc-clqbx
  Jul 29 12:58:42.263: INFO: Got endpoints: latency-svc-nbvk9 [749.861199ms]
  Jul 29 12:58:42.276: INFO: Created: latency-svc-9p576
  Jul 29 12:58:42.314: INFO: Got endpoints: latency-svc-ndmzc [750.763261ms]
  Jul 29 12:58:42.324: INFO: Created: latency-svc-qbjg8
  Jul 29 12:58:42.364: INFO: Got endpoints: latency-svc-slcrj [750.949616ms]
  Jul 29 12:58:42.376: INFO: Created: latency-svc-skx5p
  Jul 29 12:58:42.413: INFO: Got endpoints: latency-svc-sgqbh [750.830357ms]
  Jul 29 12:58:42.422: INFO: Created: latency-svc-6fx8w
  Jul 29 12:58:42.463: INFO: Got endpoints: latency-svc-rc9fd [748.487324ms]
  Jul 29 12:58:42.476: INFO: Created: latency-svc-5pqg5
  Jul 29 12:58:42.513: INFO: Got endpoints: latency-svc-6wn7h [748.916977ms]
  Jul 29 12:58:42.526: INFO: Created: latency-svc-mrphc
  Jul 29 12:58:42.563: INFO: Got endpoints: latency-svc-qx7wg [750.70836ms]
  Jul 29 12:58:42.572: INFO: Created: latency-svc-l4d2w
  Jul 29 12:58:42.613: INFO: Got endpoints: latency-svc-4d89c [750.755901ms]
  Jul 29 12:58:42.621: INFO: Created: latency-svc-xcj7n
  Jul 29 12:58:42.665: INFO: Got endpoints: latency-svc-pxs2q [751.082817ms]
  Jul 29 12:58:42.675: INFO: Created: latency-svc-skd4l
  Jul 29 12:58:42.713: INFO: Got endpoints: latency-svc-9pkfx [749.597957ms]
  Jul 29 12:58:42.724: INFO: Created: latency-svc-m7klg
  Jul 29 12:58:42.763: INFO: Got endpoints: latency-svc-p9727 [744.064394ms]
  Jul 29 12:58:42.777: INFO: Created: latency-svc-c5gq9
  Jul 29 12:58:42.813: INFO: Got endpoints: latency-svc-ptrrh [750.53971ms]
  Jul 29 12:58:42.827: INFO: Created: latency-svc-65mkv
  Jul 29 12:58:42.862: INFO: Got endpoints: latency-svc-vtfjw [749.346368ms]
  Jul 29 12:58:42.870: INFO: Created: latency-svc-7q8l6
  Jul 29 12:58:42.913: INFO: Got endpoints: latency-svc-dv24v [750.342494ms]
  Jul 29 12:58:42.924: INFO: Created: latency-svc-2dkz8
  Jul 29 12:58:42.965: INFO: Got endpoints: latency-svc-clqbx [748.351849ms]
  Jul 29 12:58:42.976: INFO: Created: latency-svc-pfdwk
  Jul 29 12:58:43.012: INFO: Got endpoints: latency-svc-9p576 [749.127159ms]
  Jul 29 12:58:43.023: INFO: Created: latency-svc-qw5dx
  Jul 29 12:58:43.065: INFO: Got endpoints: latency-svc-qbjg8 [750.632667ms]
  Jul 29 12:58:43.077: INFO: Created: latency-svc-jhznq
  Jul 29 12:58:43.115: INFO: Got endpoints: latency-svc-skx5p [750.808083ms]
  Jul 29 12:58:43.131: INFO: Created: latency-svc-n7492
  Jul 29 12:58:43.165: INFO: Got endpoints: latency-svc-6fx8w [751.949728ms]
  Jul 29 12:58:43.174: INFO: Created: latency-svc-t5j2r
  Jul 29 12:58:43.214: INFO: Got endpoints: latency-svc-5pqg5 [750.31008ms]
  Jul 29 12:58:43.226: INFO: Created: latency-svc-9pkht
  Jul 29 12:58:43.287: INFO: Got endpoints: latency-svc-mrphc [773.58863ms]
  Jul 29 12:58:43.301: INFO: Created: latency-svc-4f582
  Jul 29 12:58:43.312: INFO: Got endpoints: latency-svc-l4d2w [749.450916ms]
  Jul 29 12:58:43.321: INFO: Created: latency-svc-tm9xz
  Jul 29 12:58:43.363: INFO: Got endpoints: latency-svc-xcj7n [750.741692ms]
  Jul 29 12:58:43.379: INFO: Created: latency-svc-h658g
  Jul 29 12:58:43.415: INFO: Got endpoints: latency-svc-skd4l [749.73728ms]
  Jul 29 12:58:43.434: INFO: Created: latency-svc-gthcl
  Jul 29 12:58:43.464: INFO: Got endpoints: latency-svc-m7klg [750.441353ms]
  Jul 29 12:58:43.473: INFO: Created: latency-svc-9kddq
  Jul 29 12:58:43.515: INFO: Got endpoints: latency-svc-c5gq9 [751.570609ms]
  Jul 29 12:58:43.525: INFO: Created: latency-svc-4rf5c
  Jul 29 12:58:43.563: INFO: Got endpoints: latency-svc-65mkv [749.80932ms]
  Jul 29 12:58:43.575: INFO: Created: latency-svc-5lpxm
  Jul 29 12:58:43.613: INFO: Got endpoints: latency-svc-7q8l6 [751.353012ms]
  Jul 29 12:58:43.622: INFO: Created: latency-svc-87lfl
  Jul 29 12:58:43.761: INFO: Got endpoints: latency-svc-2dkz8 [847.41294ms]
  Jul 29 12:58:43.766: INFO: Got endpoints: latency-svc-pfdwk [801.13233ms]
  Jul 29 12:58:43.766: INFO: Got endpoints: latency-svc-qw5dx [754.001133ms]
  Jul 29 12:58:43.774: INFO: Created: latency-svc-7kjwx
  Jul 29 12:58:43.781: INFO: Created: latency-svc-vp72r
  Jul 29 12:58:43.787: INFO: Created: latency-svc-j2k78
  Jul 29 12:58:43.814: INFO: Got endpoints: latency-svc-jhznq [749.160858ms]
  Jul 29 12:58:43.830: INFO: Created: latency-svc-xq9x6
  Jul 29 12:58:43.866: INFO: Got endpoints: latency-svc-n7492 [751.451925ms]
  Jul 29 12:58:43.887: INFO: Created: latency-svc-2zfdc
  Jul 29 12:58:43.915: INFO: Got endpoints: latency-svc-t5j2r [750.731407ms]
  Jul 29 12:58:43.926: INFO: Created: latency-svc-bzhsz
  Jul 29 12:58:43.968: INFO: Got endpoints: latency-svc-9pkht [754.180601ms]
  Jul 29 12:58:43.978: INFO: Created: latency-svc-xmg99
  Jul 29 12:58:44.016: INFO: Got endpoints: latency-svc-4f582 [728.859339ms]
  Jul 29 12:58:44.034: INFO: Created: latency-svc-k46g5
  Jul 29 12:58:44.065: INFO: Got endpoints: latency-svc-tm9xz [752.077568ms]
  Jul 29 12:58:44.081: INFO: Created: latency-svc-9z564
  Jul 29 12:58:44.115: INFO: Got endpoints: latency-svc-h658g [751.921488ms]
  Jul 29 12:58:44.126: INFO: Created: latency-svc-nqkhh
  Jul 29 12:58:44.164: INFO: Got endpoints: latency-svc-gthcl [749.485605ms]
  Jul 29 12:58:44.182: INFO: Created: latency-svc-vmswz
  Jul 29 12:58:44.215: INFO: Got endpoints: latency-svc-9kddq [750.458851ms]
  Jul 29 12:58:44.224: INFO: Created: latency-svc-9zq4x
  Jul 29 12:58:44.269: INFO: Got endpoints: latency-svc-4rf5c [754.082105ms]
  Jul 29 12:58:44.282: INFO: Created: latency-svc-62hpt
  Jul 29 12:58:44.314: INFO: Got endpoints: latency-svc-5lpxm [750.715012ms]
  Jul 29 12:58:44.327: INFO: Created: latency-svc-66nlh
  Jul 29 12:58:44.367: INFO: Got endpoints: latency-svc-87lfl [753.988795ms]
  Jul 29 12:58:44.378: INFO: Created: latency-svc-jhnzx
  Jul 29 12:58:44.415: INFO: Got endpoints: latency-svc-7kjwx [653.672694ms]
  Jul 29 12:58:44.429: INFO: Created: latency-svc-vsbvz
  Jul 29 12:58:44.465: INFO: Got endpoints: latency-svc-vp72r [699.205969ms]
  Jul 29 12:58:44.480: INFO: Created: latency-svc-gsjjr
  Jul 29 12:58:44.515: INFO: Got endpoints: latency-svc-j2k78 [749.148714ms]
  Jul 29 12:58:44.528: INFO: Created: latency-svc-hgq9l
  Jul 29 12:58:44.566: INFO: Got endpoints: latency-svc-xq9x6 [751.454895ms]
  Jul 29 12:58:44.576: INFO: Created: latency-svc-vnwj9
  Jul 29 12:58:44.615: INFO: Got endpoints: latency-svc-2zfdc [748.103086ms]
  Jul 29 12:58:44.631: INFO: Created: latency-svc-pkgfr
  Jul 29 12:58:44.665: INFO: Got endpoints: latency-svc-bzhsz [749.783154ms]
  Jul 29 12:58:44.678: INFO: Created: latency-svc-89st2
  Jul 29 12:58:44.714: INFO: Got endpoints: latency-svc-xmg99 [746.149565ms]
  Jul 29 12:58:44.724: INFO: Created: latency-svc-nz87f
  Jul 29 12:58:44.765: INFO: Got endpoints: latency-svc-k46g5 [749.316884ms]
  Jul 29 12:58:44.782: INFO: Created: latency-svc-rwqtz
  Jul 29 12:58:44.814: INFO: Got endpoints: latency-svc-9z564 [749.548717ms]
  Jul 29 12:58:44.826: INFO: Created: latency-svc-ghtxz
  Jul 29 12:58:44.863: INFO: Got endpoints: latency-svc-nqkhh [747.422078ms]
  Jul 29 12:58:44.877: INFO: Created: latency-svc-bskb7
  Jul 29 12:58:44.915: INFO: Got endpoints: latency-svc-vmswz [750.302855ms]
  Jul 29 12:58:44.928: INFO: Created: latency-svc-z67nr
  Jul 29 12:58:44.967: INFO: Got endpoints: latency-svc-9zq4x [752.279472ms]
  Jul 29 12:58:44.977: INFO: Created: latency-svc-cdzrb
  Jul 29 12:58:45.015: INFO: Got endpoints: latency-svc-62hpt [746.645765ms]
  Jul 29 12:58:45.037: INFO: Created: latency-svc-qzjsz
  Jul 29 12:58:45.065: INFO: Got endpoints: latency-svc-66nlh [751.632566ms]
  Jul 29 12:58:45.079: INFO: Created: latency-svc-td2bt
  Jul 29 12:58:45.116: INFO: Got endpoints: latency-svc-jhnzx [748.217726ms]
  Jul 29 12:58:45.131: INFO: Created: latency-svc-bc4b5
  Jul 29 12:58:45.165: INFO: Got endpoints: latency-svc-vsbvz [750.517526ms]
  Jul 29 12:58:45.178: INFO: Created: latency-svc-62smm
  Jul 29 12:58:45.215: INFO: Got endpoints: latency-svc-gsjjr [749.725325ms]
  Jul 29 12:58:45.234: INFO: Created: latency-svc-t5vsh
  Jul 29 12:58:45.268: INFO: Got endpoints: latency-svc-hgq9l [752.425002ms]
  Jul 29 12:58:45.284: INFO: Created: latency-svc-8zm2l
  Jul 29 12:58:45.329: INFO: Got endpoints: latency-svc-vnwj9 [762.750225ms]
  Jul 29 12:58:45.395: INFO: Got endpoints: latency-svc-pkgfr [780.133074ms]
  Jul 29 12:58:45.401: INFO: Created: latency-svc-qdrhn
  Jul 29 12:58:45.415: INFO: Created: latency-svc-qr6qb
  Jul 29 12:58:45.417: INFO: Got endpoints: latency-svc-89st2 [751.722952ms]
  Jul 29 12:58:45.426: INFO: Created: latency-svc-b8hmf
  Jul 29 12:58:45.467: INFO: Got endpoints: latency-svc-nz87f [752.415364ms]
  Jul 29 12:58:45.479: INFO: Created: latency-svc-5p8mz
  Jul 29 12:58:45.515: INFO: Got endpoints: latency-svc-rwqtz [749.276798ms]
  Jul 29 12:58:45.531: INFO: Created: latency-svc-nwhkz
  Jul 29 12:58:45.564: INFO: Got endpoints: latency-svc-ghtxz [749.818114ms]
  Jul 29 12:58:45.574: INFO: Created: latency-svc-qb754
  Jul 29 12:58:45.613: INFO: Got endpoints: latency-svc-bskb7 [750.465591ms]
  Jul 29 12:58:45.629: INFO: Created: latency-svc-zrq6q
  Jul 29 12:58:45.666: INFO: Got endpoints: latency-svc-z67nr [751.026867ms]
  Jul 29 12:58:45.682: INFO: Created: latency-svc-wqw9b
  Jul 29 12:58:45.714: INFO: Got endpoints: latency-svc-cdzrb [746.875596ms]
  Jul 29 12:58:45.725: INFO: Created: latency-svc-zqgch
  Jul 29 12:58:45.765: INFO: Got endpoints: latency-svc-qzjsz [749.896213ms]
  Jul 29 12:58:45.775: INFO: Created: latency-svc-l76bl
  Jul 29 12:58:45.814: INFO: Got endpoints: latency-svc-td2bt [748.781061ms]
  Jul 29 12:58:45.832: INFO: Created: latency-svc-nsnmn
  Jul 29 12:58:45.863: INFO: Got endpoints: latency-svc-bc4b5 [747.126797ms]
  Jul 29 12:58:45.874: INFO: Created: latency-svc-6sbb9
  Jul 29 12:58:45.915: INFO: Got endpoints: latency-svc-62smm [749.612309ms]
  Jul 29 12:58:45.927: INFO: Created: latency-svc-gzvvg
  Jul 29 12:58:45.965: INFO: Got endpoints: latency-svc-t5vsh [750.180276ms]
  Jul 29 12:58:45.979: INFO: Created: latency-svc-vzpwq
  Jul 29 12:58:46.017: INFO: Got endpoints: latency-svc-8zm2l [748.768885ms]
  Jul 29 12:58:46.026: INFO: Created: latency-svc-xxthc
  Jul 29 12:58:46.063: INFO: Got endpoints: latency-svc-qdrhn [734.615908ms]
  Jul 29 12:58:46.077: INFO: Created: latency-svc-m6ngf
  Jul 29 12:58:46.114: INFO: Got endpoints: latency-svc-qr6qb [718.813063ms]
  Jul 29 12:58:46.126: INFO: Created: latency-svc-chfb5
  Jul 29 12:58:46.164: INFO: Got endpoints: latency-svc-b8hmf [746.807622ms]
  Jul 29 12:58:46.174: INFO: Created: latency-svc-7tz8p
  Jul 29 12:58:46.219: INFO: Got endpoints: latency-svc-5p8mz [751.710888ms]
  Jul 29 12:58:46.228: INFO: Created: latency-svc-lqpnk
  Jul 29 12:58:46.264: INFO: Got endpoints: latency-svc-nwhkz [749.261739ms]
  Jul 29 12:58:46.277: INFO: Created: latency-svc-mprsj
  Jul 29 12:58:46.315: INFO: Got endpoints: latency-svc-qb754 [750.793877ms]
  Jul 29 12:58:46.331: INFO: Created: latency-svc-7hgcg
  Jul 29 12:58:46.365: INFO: Got endpoints: latency-svc-zrq6q [751.899956ms]
  Jul 29 12:58:46.379: INFO: Created: latency-svc-tkl9d
  Jul 29 12:58:46.413: INFO: Got endpoints: latency-svc-wqw9b [747.660907ms]
  Jul 29 12:58:46.426: INFO: Created: latency-svc-t2tzr
  Jul 29 12:58:46.465: INFO: Got endpoints: latency-svc-zqgch [751.112196ms]
  Jul 29 12:58:46.475: INFO: Created: latency-svc-7pll4
  Jul 29 12:58:46.515: INFO: Got endpoints: latency-svc-l76bl [749.616687ms]
  Jul 29 12:58:46.526: INFO: Created: latency-svc-cvn7h
  Jul 29 12:58:46.564: INFO: Got endpoints: latency-svc-nsnmn [749.834499ms]
  Jul 29 12:58:46.580: INFO: Created: latency-svc-9nxbh
  Jul 29 12:58:46.614: INFO: Got endpoints: latency-svc-6sbb9 [750.884618ms]
  Jul 29 12:58:46.624: INFO: Created: latency-svc-fxl89
  Jul 29 12:58:46.666: INFO: Got endpoints: latency-svc-gzvvg [751.036088ms]
  Jul 29 12:58:46.684: INFO: Created: latency-svc-xlqfl
  Jul 29 12:58:46.716: INFO: Got endpoints: latency-svc-vzpwq [751.191705ms]
  Jul 29 12:58:46.731: INFO: Created: latency-svc-qzwpr
  Jul 29 12:58:46.764: INFO: Got endpoints: latency-svc-xxthc [746.577532ms]
  Jul 29 12:58:46.776: INFO: Created: latency-svc-dndzn
  Jul 29 12:58:46.814: INFO: Got endpoints: latency-svc-m6ngf [750.892013ms]
  Jul 29 12:58:46.826: INFO: Created: latency-svc-vcm4f
  Jul 29 12:58:46.865: INFO: Got endpoints: latency-svc-chfb5 [751.228183ms]
  Jul 29 12:58:46.877: INFO: Created: latency-svc-zmcp8
  Jul 29 12:58:46.916: INFO: Got endpoints: latency-svc-7tz8p [751.834923ms]
  Jul 29 12:58:46.930: INFO: Created: latency-svc-hwk84
  Jul 29 12:58:46.965: INFO: Got endpoints: latency-svc-lqpnk [746.097919ms]
  Jul 29 12:58:46.976: INFO: Created: latency-svc-5rv5p
  Jul 29 12:58:47.018: INFO: Got endpoints: latency-svc-mprsj [753.538722ms]
  Jul 29 12:58:47.031: INFO: Created: latency-svc-7cstx
  Jul 29 12:58:47.065: INFO: Got endpoints: latency-svc-7hgcg [749.994474ms]
  Jul 29 12:58:47.076: INFO: Created: latency-svc-2p2hx
  Jul 29 12:58:47.114: INFO: Got endpoints: latency-svc-tkl9d [748.197559ms]
  Jul 29 12:58:47.164: INFO: Got endpoints: latency-svc-t2tzr [751.069245ms]
  Jul 29 12:58:47.216: INFO: Got endpoints: latency-svc-7pll4 [751.10488ms]
  Jul 29 12:58:47.265: INFO: Got endpoints: latency-svc-cvn7h [749.793146ms]
  Jul 29 12:58:47.314: INFO: Got endpoints: latency-svc-9nxbh [749.788207ms]
  Jul 29 12:58:47.363: INFO: Got endpoints: latency-svc-fxl89 [749.494947ms]
  Jul 29 12:58:47.415: INFO: Got endpoints: latency-svc-xlqfl [749.297607ms]
  Jul 29 12:58:47.467: INFO: Got endpoints: latency-svc-qzwpr [749.661314ms]
  Jul 29 12:58:47.514: INFO: Got endpoints: latency-svc-dndzn [749.833736ms]
  Jul 29 12:58:47.565: INFO: Got endpoints: latency-svc-vcm4f [750.939513ms]
  Jul 29 12:58:47.613: INFO: Got endpoints: latency-svc-zmcp8 [748.60196ms]
  Jul 29 12:58:47.665: INFO: Got endpoints: latency-svc-hwk84 [748.614446ms]
  Jul 29 12:58:47.714: INFO: Got endpoints: latency-svc-5rv5p [749.058316ms]
  Jul 29 12:58:47.766: INFO: Got endpoints: latency-svc-7cstx [747.832587ms]
  Jul 29 12:58:47.815: INFO: Got endpoints: latency-svc-2p2hx [750.038355ms]
  Jul 29 12:58:47.815: INFO: Latencies: [23.107017ms 33.446568ms 41.020923ms 52.297996ms 53.900528ms 55.139782ms 58.213505ms 66.99177ms 73.976677ms 78.363319ms 93.815652ms 104.694095ms 114.129272ms 116.291976ms 118.986457ms 127.732517ms 130.247234ms 135.233276ms 135.486519ms 136.608281ms 137.901292ms 138.307784ms 141.138962ms 143.37332ms 150.943943ms 151.228986ms 152.088109ms 158.214808ms 160.807836ms 168.905141ms 180.343236ms 189.074389ms 192.160756ms 201.706831ms 204.242261ms 216.892404ms 219.804727ms 220.079645ms 252.24513ms 254.78474ms 281.529296ms 336.922933ms 373.989662ms 451.156168ms 496.708203ms 552.268678ms 568.776088ms 603.435418ms 653.672694ms 685.205035ms 688.329604ms 699.205969ms 705.063075ms 718.813063ms 719.620597ms 728.859339ms 734.385482ms 734.615908ms 742.543822ms 743.906939ms 744.064394ms 746.097919ms 746.149565ms 746.577532ms 746.645765ms 746.807622ms 746.875596ms 747.126797ms 747.422078ms 747.543796ms 747.660907ms 747.832587ms 748.103086ms 748.131872ms 748.171015ms 748.197559ms 748.217726ms 748.351849ms 748.487324ms 748.60196ms 748.614446ms 748.768885ms 748.775112ms 748.781061ms 748.916977ms 749.058316ms 749.119454ms 749.127159ms 749.132345ms 749.143849ms 749.148714ms 749.160858ms 749.261739ms 749.276798ms 749.292978ms 749.297607ms 749.30696ms 749.316884ms 749.346368ms 749.37824ms 749.450916ms 749.462728ms 749.485605ms 749.494947ms 749.501864ms 749.548717ms 749.597957ms 749.612309ms 749.614909ms 749.616687ms 749.660302ms 749.661314ms 749.725325ms 749.73728ms 749.783154ms 749.788207ms 749.793146ms 749.80932ms 749.816431ms 749.818114ms 749.833736ms 749.834499ms 749.850868ms 749.861199ms 749.896213ms 749.994474ms 750.005137ms 750.038355ms 750.12159ms 750.163108ms 750.180276ms 750.261164ms 750.302855ms 750.31008ms 750.342494ms 750.441353ms 750.458851ms 750.465591ms 750.517526ms 750.53971ms 750.581981ms 750.632667ms 750.70836ms 750.715012ms 750.731407ms 750.741692ms 750.755901ms 750.763261ms 750.793877ms 750.808083ms 750.830357ms 750.884618ms 750.892013ms 750.893285ms 750.939513ms 750.949616ms 750.995246ms 751.026867ms 751.036088ms 751.069245ms 751.082817ms 751.10488ms 751.112196ms 751.125421ms 751.191705ms 751.228183ms 751.298018ms 751.353012ms 751.398057ms 751.451925ms 751.454895ms 751.570609ms 751.632566ms 751.688399ms 751.710888ms 751.722952ms 751.800649ms 751.834923ms 751.899956ms 751.921488ms 751.949728ms 752.077568ms 752.279472ms 752.415364ms 752.425002ms 753.538722ms 753.804829ms 753.988795ms 754.001133ms 754.082105ms 754.180601ms 754.378101ms 762.750225ms 773.58863ms 780.133074ms 783.926373ms 793.616533ms 801.13233ms 805.834219ms 847.41294ms]
  Jul 29 12:58:47.815: INFO: 50 %ile: 749.450916ms
  Jul 29 12:58:47.815: INFO: 90 %ile: 751.949728ms
  Jul 29 12:58:47.815: INFO: 99 %ile: 805.834219ms
  Jul 29 12:58:47.815: INFO: Total sample count: 200
  Jul 29 12:58:47.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-2642" for this suite. @ 07/29/23 12:58:47.822
• [10.765 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 07/29/23 12:58:47.834
  Jul 29 12:58:47.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pods @ 07/29/23 12:58:47.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:58:47.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:58:47.866
  STEP: creating the pod @ 07/29/23 12:58:47.87
  STEP: submitting the pod to kubernetes @ 07/29/23 12:58:47.87
  STEP: verifying QOS class is set on the pod @ 07/29/23 12:58:47.878
  Jul 29 12:58:47.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-482" for this suite. @ 07/29/23 12:58:47.889
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 07/29/23 12:58:47.904
  Jul 29 12:58:47.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 12:58:47.905
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:58:47.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:58:47.935
  STEP: Creating service test in namespace statefulset-6865 @ 07/29/23 12:58:47.939
  STEP: Looking for a node to schedule stateful set and pod @ 07/29/23 12:58:47.944
  STEP: Creating pod with conflicting port in namespace statefulset-6865 @ 07/29/23 12:58:47.953
  STEP: Waiting until pod test-pod will start running in namespace statefulset-6865 @ 07/29/23 12:58:47.978
  STEP: Creating statefulset with conflicting port in namespace statefulset-6865 @ 07/29/23 12:58:49.986
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6865 @ 07/29/23 12:58:49.996
  Jul 29 12:58:50.012: INFO: Observed stateful pod in namespace: statefulset-6865, name: ss-0, uid: d7149d5b-b7f2-47ad-9a7e-d22fb2c9c192, status phase: Pending. Waiting for statefulset controller to delete.
  Jul 29 12:58:50.030: INFO: Observed stateful pod in namespace: statefulset-6865, name: ss-0, uid: d7149d5b-b7f2-47ad-9a7e-d22fb2c9c192, status phase: Failed. Waiting for statefulset controller to delete.
  Jul 29 12:58:50.049: INFO: Observed stateful pod in namespace: statefulset-6865, name: ss-0, uid: d7149d5b-b7f2-47ad-9a7e-d22fb2c9c192, status phase: Failed. Waiting for statefulset controller to delete.
  Jul 29 12:58:50.056: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6865
  STEP: Removing pod with conflicting port in namespace statefulset-6865 @ 07/29/23 12:58:50.056
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6865 and will be in running state @ 07/29/23 12:58:50.083
  Jul 29 12:58:52.092: INFO: Deleting all statefulset in ns statefulset-6865
  Jul 29 12:58:52.095: INFO: Scaling statefulset ss to 0
  Jul 29 12:59:02.116: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 12:59:02.120: INFO: Deleting statefulset ss
  Jul 29 12:59:02.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6865" for this suite. @ 07/29/23 12:59:02.136
• [14.237 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 07/29/23 12:59:02.143
  Jul 29 12:59:02.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 12:59:02.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:59:02.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:59:02.162
  Jul 29 12:59:02.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:59:03.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5558" for this suite. @ 07/29/23 12:59:03.191
• [1.055 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 07/29/23 12:59:03.199
  Jul 29 12:59:03.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pod-network-test @ 07/29/23 12:59:03.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:59:03.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:59:03.216
  STEP: Performing setup for networking test in namespace pod-network-test-3754 @ 07/29/23 12:59:03.223
  STEP: creating a selector @ 07/29/23 12:59:03.223
  STEP: Creating the service pods in kubernetes @ 07/29/23 12:59:03.223
  Jul 29 12:59:03.223: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 07/29/23 12:59:15.292
  Jul 29 12:59:17.327: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 29 12:59:17.327: INFO: Going to poll 192.168.10.46 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 12:59:17.330: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.10.46 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3754 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:59:17.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:59:17.331: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:59:17.331: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3754/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.10.46+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 12:59:18.407: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul 29 12:59:18.407: INFO: Going to poll 192.168.129.92 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 12:59:18.411: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.129.92 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3754 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:59:18.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:59:18.411: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:59:18.411: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3754/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.129.92+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 12:59:19.470: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul 29 12:59:19.470: INFO: Going to poll 192.168.8.152 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 12:59:19.474: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.8.152 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3754 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 12:59:19.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 12:59:19.474: INFO: ExecWithOptions: Clientset creation
  Jul 29 12:59:19.474: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3754/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.8.152+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 12:59:20.531: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul 29 12:59:20.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3754" for this suite. @ 07/29/23 12:59:20.536
• [17.343 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 07/29/23 12:59:20.542
  Jul 29 12:59:20.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename gc @ 07/29/23 12:59:20.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:59:20.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:59:20.561
  STEP: create the rc @ 07/29/23 12:59:20.567
  W0729 12:59:20.574233      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 07/29/23 12:59:24.579
  STEP: wait for the rc to be deleted @ 07/29/23 12:59:24.588
  Jul 29 12:59:25.614: INFO: 80 pods remaining
  Jul 29 12:59:25.614: INFO: 80 pods has nil DeletionTimestamp
  Jul 29 12:59:25.614: INFO: 
  Jul 29 12:59:26.608: INFO: 71 pods remaining
  Jul 29 12:59:26.608: INFO: 70 pods has nil DeletionTimestamp
  Jul 29 12:59:26.608: INFO: 
  Jul 29 12:59:27.621: INFO: 60 pods remaining
  Jul 29 12:59:27.621: INFO: 60 pods has nil DeletionTimestamp
  Jul 29 12:59:27.621: INFO: 
  Jul 29 12:59:28.598: INFO: 40 pods remaining
  Jul 29 12:59:28.599: INFO: 40 pods has nil DeletionTimestamp
  Jul 29 12:59:28.599: INFO: 
  Jul 29 12:59:29.606: INFO: 31 pods remaining
  Jul 29 12:59:29.606: INFO: 30 pods has nil DeletionTimestamp
  Jul 29 12:59:29.606: INFO: 
  Jul 29 12:59:30.600: INFO: 20 pods remaining
  Jul 29 12:59:30.600: INFO: 20 pods has nil DeletionTimestamp
  Jul 29 12:59:30.601: INFO: 
  STEP: Gathering metrics @ 07/29/23 12:59:31.603
  W0729 12:59:31.607207      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 29 12:59:31.607: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 12:59:31.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3282" for this suite. @ 07/29/23 12:59:31.612
• [11.077 seconds]
------------------------------
SSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 07/29/23 12:59:31.62
  Jul 29 12:59:31.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 12:59:31.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:59:31.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:59:31.641
  STEP: Creating configMap that has name configmap-test-emptyKey-000307e2-fbdd-41e0-9c50-b06c771b982f @ 07/29/23 12:59:31.652
  Jul 29 12:59:31.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5526" for this suite. @ 07/29/23 12:59:31.658
• [0.045 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 07/29/23 12:59:31.666
  Jul 29 12:59:31.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 12:59:31.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:59:31.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:59:31.687
  STEP: Creating projection with secret that has name projected-secret-test-map-16902dd7-37a7-4c8c-8068-974aa4030dd1 @ 07/29/23 12:59:31.689
  STEP: Creating a pod to test consume secrets @ 07/29/23 12:59:31.693
  STEP: Saw pod success @ 07/29/23 12:59:41.736
  Jul 29 12:59:41.739: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-projected-secrets-1ae370d6-3045-4c6e-8776-3c4083d9c903 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 12:59:41.757
  Jul 29 12:59:41.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4251" for this suite. @ 07/29/23 12:59:41.777
• [10.118 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 07/29/23 12:59:41.785
  Jul 29 12:59:41.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 12:59:41.786
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:59:41.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:59:41.807
  Jul 29 12:59:43.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 12:59:43.835: INFO: Deleting pod "var-expansion-fc30503f-ec6d-4b38-ab7c-45846e96b019" in namespace "var-expansion-6180"
  Jul 29 12:59:43.842: INFO: Wait up to 5m0s for pod "var-expansion-fc30503f-ec6d-4b38-ab7c-45846e96b019" to be fully deleted
  STEP: Destroying namespace "var-expansion-6180" for this suite. @ 07/29/23 12:59:45.849
• [4.071 seconds]
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 07/29/23 12:59:45.856
  Jul 29 12:59:45.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename job @ 07/29/23 12:59:45.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:59:45.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:59:45.875
  STEP: Creating a job @ 07/29/23 12:59:45.877
  STEP: Ensure pods equal to parallelism count is attached to the job @ 07/29/23 12:59:45.883
  STEP: patching /status @ 07/29/23 12:59:47.888
  STEP: updating /status @ 07/29/23 12:59:47.897
  STEP: get /status @ 07/29/23 12:59:47.927
  Jul 29 12:59:47.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1760" for this suite. @ 07/29/23 12:59:47.933
• [2.084 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 07/29/23 12:59:47.941
  Jul 29 12:59:47.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 12:59:47.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:59:47.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:59:47.958
  STEP: Setting up server cert @ 07/29/23 12:59:47.989
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 12:59:48.297
  STEP: Deploying the webhook pod @ 07/29/23 12:59:48.304
  STEP: Wait for the deployment to be ready @ 07/29/23 12:59:48.318
  Jul 29 12:59:48.323: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 12:59:50.333
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 12:59:50.342
  Jul 29 12:59:51.343: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 07/29/23 12:59:51.404
  STEP: Creating a configMap that should be mutated @ 07/29/23 12:59:51.418
  STEP: Deleting the collection of validation webhooks @ 07/29/23 12:59:51.453
  STEP: Creating a configMap that should not be mutated @ 07/29/23 12:59:51.504
  Jul 29 12:59:51.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9644" for this suite. @ 07/29/23 12:59:51.567
  STEP: Destroying namespace "webhook-markers-7250" for this suite. @ 07/29/23 12:59:51.572
• [3.637 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 07/29/23 12:59:51.578
  Jul 29 12:59:51.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename secrets @ 07/29/23 12:59:51.579
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 12:59:51.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 12:59:51.595
  STEP: Creating secret with name s-test-opt-del-deeecaca-1fbf-49c7-920b-cd090194730d @ 07/29/23 12:59:51.601
  STEP: Creating secret with name s-test-opt-upd-fe54c37c-16f7-4b41-a623-09d3bdcd1287 @ 07/29/23 12:59:51.606
  STEP: Creating the pod @ 07/29/23 12:59:51.611
  STEP: Deleting secret s-test-opt-del-deeecaca-1fbf-49c7-920b-cd090194730d @ 07/29/23 12:59:53.653
  STEP: Updating secret s-test-opt-upd-fe54c37c-16f7-4b41-a623-09d3bdcd1287 @ 07/29/23 12:59:53.659
  STEP: Creating secret with name s-test-opt-create-a13f5445-131f-4361-bf16-cbda6038a85c @ 07/29/23 12:59:53.664
  STEP: waiting to observe update in volume @ 07/29/23 12:59:53.669
  Jul 29 13:01:08.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7419" for this suite. @ 07/29/23 13:01:08.044
• [76.472 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 07/29/23 13:01:08.051
  Jul 29 13:01:08.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:01:08.052
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:01:08.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:01:08.073
  STEP: Creating configMap with name projected-configmap-test-volume-54afefc0-627a-48cc-9e86-c3fc64bdd7fc @ 07/29/23 13:01:08.075
  STEP: Creating a pod to test consume configMaps @ 07/29/23 13:01:08.08
  STEP: Saw pod success @ 07/29/23 13:01:12.1
  Jul 29 13:01:12.103: INFO: Trying to get logs from node ip-172-31-19-67 pod pod-projected-configmaps-bbc88b97-93d3-4ae4-8db1-4a093e4b8c68 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 13:01:12.121
  Jul 29 13:01:12.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3281" for this suite. @ 07/29/23 13:01:12.143
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 07/29/23 13:01:12.152
  Jul 29 13:01:12.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename subpath @ 07/29/23 13:01:12.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:01:12.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:01:12.17
  STEP: Setting up data @ 07/29/23 13:01:12.173
  STEP: Creating pod pod-subpath-test-projected-stwj @ 07/29/23 13:01:12.18
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 13:01:12.18
  STEP: Saw pod success @ 07/29/23 13:01:36.244
  Jul 29 13:01:36.247: INFO: Trying to get logs from node ip-172-31-19-67 pod pod-subpath-test-projected-stwj container test-container-subpath-projected-stwj: <nil>
  STEP: delete the pod @ 07/29/23 13:01:36.253
  STEP: Deleting pod pod-subpath-test-projected-stwj @ 07/29/23 13:01:36.27
  Jul 29 13:01:36.270: INFO: Deleting pod "pod-subpath-test-projected-stwj" in namespace "subpath-8559"
  Jul 29 13:01:36.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8559" for this suite. @ 07/29/23 13:01:36.277
• [24.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 07/29/23 13:01:36.285
  Jul 29 13:01:36.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 13:01:36.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:01:36.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:01:36.303
  STEP: Creating a test namespace @ 07/29/23 13:01:36.305
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:01:36.344
  STEP: Creating a pod in the namespace @ 07/29/23 13:01:36.347
  STEP: Waiting for the pod to have running status @ 07/29/23 13:01:36.355
  STEP: Deleting the namespace @ 07/29/23 13:01:38.364
  STEP: Waiting for the namespace to be removed. @ 07/29/23 13:01:38.39
  STEP: Recreating the namespace @ 07/29/23 13:01:49.394
  STEP: Verifying there are no pods in the namespace @ 07/29/23 13:01:49.428
  Jul 29 13:01:49.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-214" for this suite. @ 07/29/23 13:01:49.446
  STEP: Destroying namespace "nsdeletetest-9956" for this suite. @ 07/29/23 13:01:49.451
  Jul 29 13:01:49.456: INFO: Namespace nsdeletetest-9956 was already deleted
  STEP: Destroying namespace "nsdeletetest-5818" for this suite. @ 07/29/23 13:01:49.456
• [13.177 seconds]
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 07/29/23 13:01:49.462
  Jul 29 13:01:49.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 13:01:49.463
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:01:49.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:01:49.485
  STEP: creating the pod with failed condition @ 07/29/23 13:01:49.488
  STEP: updating the pod @ 07/29/23 13:03:49.496
  Jul 29 13:03:50.008: INFO: Successfully updated pod "var-expansion-a2e6fd38-4ec0-401f-a165-2769becd620d"
  STEP: waiting for pod running @ 07/29/23 13:03:50.008
  STEP: deleting the pod gracefully @ 07/29/23 13:03:52.017
  Jul 29 13:03:52.017: INFO: Deleting pod "var-expansion-a2e6fd38-4ec0-401f-a165-2769becd620d" in namespace "var-expansion-3683"
  Jul 29 13:03:52.027: INFO: Wait up to 5m0s for pod "var-expansion-a2e6fd38-4ec0-401f-a165-2769becd620d" to be fully deleted
  Jul 29 13:04:24.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3683" for this suite. @ 07/29/23 13:04:24.108
• [154.653 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 07/29/23 13:04:24.117
  Jul 29 13:04:24.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 13:04:24.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:04:24.136
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:04:24.14
  STEP: Creating pod busybox-495fcfb8-2cb6-47e9-a25a-f2612ac38fe5 in namespace container-probe-6538 @ 07/29/23 13:04:24.143
  Jul 29 13:04:26.160: INFO: Started pod busybox-495fcfb8-2cb6-47e9-a25a-f2612ac38fe5 in namespace container-probe-6538
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 13:04:26.16
  Jul 29 13:04:26.163: INFO: Initial restart count of pod busybox-495fcfb8-2cb6-47e9-a25a-f2612ac38fe5 is 0
  Jul 29 13:08:26.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 13:08:26.684
  STEP: Destroying namespace "container-probe-6538" for this suite. @ 07/29/23 13:08:26.699
• [242.589 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 07/29/23 13:08:26.708
  Jul 29 13:08:26.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename secrets @ 07/29/23 13:08:26.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:08:26.725
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:08:26.727
  STEP: Creating secret with name secret-test-8103d706-a07c-4cf0-bb0f-d297af2c12be @ 07/29/23 13:08:26.729
  STEP: Creating a pod to test consume secrets @ 07/29/23 13:08:26.734
  STEP: Saw pod success @ 07/29/23 13:08:30.757
  Jul 29 13:08:30.759: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-secrets-a77d0daa-3895-4153-be46-a7ddfb965792 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 13:08:30.778
  Jul 29 13:08:30.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-807" for this suite. @ 07/29/23 13:08:30.799
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 07/29/23 13:08:30.806
  Jul 29 13:08:30.806: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename dns @ 07/29/23 13:08:30.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:08:30.825
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:08:30.827
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 07/29/23 13:08:30.83
  Jul 29 13:08:30.841: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1158  0159a5e8-7955-462a-ad67-977469165307 27996 0 2023-07-29 13:08:30 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-07-29 13:08:30 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8zxlq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8zxlq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 07/29/23 13:08:32.851
  Jul 29 13:08:32.851: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1158 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:08:32.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:08:32.851: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:08:32.851: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-1158/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 07/29/23 13:08:32.934
  Jul 29 13:08:32.934: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1158 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:08:32.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:08:32.935: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:08:32.935: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-1158/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 13:08:33.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 13:08:33.024: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-1158" for this suite. @ 07/29/23 13:08:33.038
• [2.239 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 07/29/23 13:08:33.047
  Jul 29 13:08:33.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 13:08:33.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:08:33.063
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:08:33.065
  STEP: Creating service test in namespace statefulset-5227 @ 07/29/23 13:08:33.068
  STEP: Creating statefulset ss in namespace statefulset-5227 @ 07/29/23 13:08:33.077
  Jul 29 13:08:33.087: INFO: Found 0 stateful pods, waiting for 1
  Jul 29 13:08:43.092: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 07/29/23 13:08:43.099
  STEP: Getting /status @ 07/29/23 13:08:43.113
  Jul 29 13:08:43.117: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 07/29/23 13:08:43.117
  Jul 29 13:08:43.129: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 07/29/23 13:08:43.129
  Jul 29 13:08:43.131: INFO: Observed &StatefulSet event: ADDED
  Jul 29 13:08:43.131: INFO: Found Statefulset ss in namespace statefulset-5227 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 13:08:43.131: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 07/29/23 13:08:43.132
  Jul 29 13:08:43.132: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 29 13:08:43.142: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 07/29/23 13:08:43.142
  Jul 29 13:08:43.143: INFO: Observed &StatefulSet event: ADDED
  Jul 29 13:08:43.143: INFO: Observed Statefulset ss in namespace statefulset-5227 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 13:08:43.143: INFO: Observed &StatefulSet event: MODIFIED
  Jul 29 13:08:43.143: INFO: Deleting all statefulset in ns statefulset-5227
  Jul 29 13:08:43.147: INFO: Scaling statefulset ss to 0
  Jul 29 13:08:53.166: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 13:08:53.168: INFO: Deleting statefulset ss
  Jul 29 13:08:53.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5227" for this suite. @ 07/29/23 13:08:53.187
• [20.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 07/29/23 13:08:53.196
  Jul 29 13:08:53.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 13:08:53.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:08:53.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:08:53.217
  STEP: creating a Namespace @ 07/29/23 13:08:53.22
  STEP: patching the Namespace @ 07/29/23 13:08:53.236
  STEP: get the Namespace and ensuring it has the label @ 07/29/23 13:08:53.241
  Jul 29 13:08:53.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-23" for this suite. @ 07/29/23 13:08:53.249
  STEP: Destroying namespace "nspatchtest-a1417e62-4e00-4798-934d-9f36f2b2b52c-2902" for this suite. @ 07/29/23 13:08:53.254
• [0.064 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 07/29/23 13:08:53.261
  Jul 29 13:08:53.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename watch @ 07/29/23 13:08:53.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:08:53.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:08:53.276
  STEP: creating a new configmap @ 07/29/23 13:08:53.278
  STEP: modifying the configmap once @ 07/29/23 13:08:53.283
  STEP: modifying the configmap a second time @ 07/29/23 13:08:53.29
  STEP: deleting the configmap @ 07/29/23 13:08:53.299
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 07/29/23 13:08:53.305
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 07/29/23 13:08:53.307
  Jul 29 13:08:53.307: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4036  59850e78-6641-4fe6-ad64-8d86b0ccbbe4 28166 0 2023-07-29 13:08:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-29 13:08:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:08:53.307: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4036  59850e78-6641-4fe6-ad64-8d86b0ccbbe4 28167 0 2023-07-29 13:08:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-29 13:08:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:08:53.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4036" for this suite. @ 07/29/23 13:08:53.31
• [0.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 07/29/23 13:08:53.319
  Jul 29 13:08:53.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 13:08:53.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:08:53.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:08:53.336
  STEP: apply creating a deployment @ 07/29/23 13:08:53.339
  Jul 29 13:08:53.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2509" for this suite. @ 07/29/23 13:08:53.356
• [0.044 seconds]
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 07/29/23 13:08:53.363
  Jul 29 13:08:53.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename sched-pred @ 07/29/23 13:08:53.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:08:53.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:08:53.382
  Jul 29 13:08:53.385: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 29 13:08:53.393: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 13:08:53.397: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-19-67 before test
  Jul 29 13:08:53.401: INFO: nginx-ingress-controller-kubernetes-worker-gllr9 from ingress-nginx-kubernetes-worker started at 2023-07-29 12:05:42 +0000 UTC (1 container statuses recorded)
  Jul 29 13:08:53.401: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 29 13:08:53.401: INFO: calico-kube-controllers-66cf5c7c9b-lp29z from kube-system started at 2023-07-29 12:05:47 +0000 UTC (1 container statuses recorded)
  Jul 29 13:08:53.401: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul 29 13:08:53.401: INFO: sonobuoy-e2e-job-fe057c2caff44e9c from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:08:53.401: INFO: 	Container e2e ready: true, restart count 0
  Jul 29 13:08:53.401: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:08:53.401: INFO: sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-7dv7m from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:08:53.401: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:08:53.401: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 13:08:53.401: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-33-37 before test
  Jul 29 13:08:53.405: INFO: nginx-ingress-controller-kubernetes-worker-vg4gz from ingress-nginx-kubernetes-worker started at 2023-07-29 12:18:09 +0000 UTC (1 container statuses recorded)
  Jul 29 13:08:53.405: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 29 13:08:53.405: INFO: sonobuoy from sonobuoy started at 2023-07-29 12:12:09 +0000 UTC (1 container statuses recorded)
  Jul 29 13:08:53.405: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 29 13:08:53.405: INFO: sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-fxbms from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:08:53.405: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:08:53.405: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 13:08:53.405: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-5-66 before test
  Jul 29 13:08:53.411: INFO: default-http-backend-kubernetes-worker-65fc475d49-fwzg8 from ingress-nginx-kubernetes-worker started at 2023-07-29 11:57:02 +0000 UTC (1 container statuses recorded)
  Jul 29 13:08:53.411: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul 29 13:08:53.411: INFO: nginx-ingress-controller-kubernetes-worker-hbv24 from ingress-nginx-kubernetes-worker started at 2023-07-29 11:57:02 +0000 UTC (1 container statuses recorded)
  Jul 29 13:08:53.411: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 29 13:08:53.411: INFO: coredns-5c7f76ccb8-7mf4p from kube-system started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:08:53.411: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 13:08:53.411: INFO: kube-state-metrics-5b95b4459c-849m5 from kube-system started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:08:53.411: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul 29 13:08:53.411: INFO: metrics-server-v0.5.2-6cf8c8b69c-slzlr from kube-system started at 2023-07-29 11:55:52 +0000 UTC (2 container statuses recorded)
  Jul 29 13:08:53.411: INFO: 	Container metrics-server ready: true, restart count 0
  Jul 29 13:08:53.411: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul 29 13:08:53.411: INFO: dashboard-metrics-scraper-6b8586b5c9-6s95z from kubernetes-dashboard started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:08:53.411: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul 29 13:08:53.411: INFO: kubernetes-dashboard-6869f4cd5f-fwg5d from kubernetes-dashboard started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:08:53.411: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul 29 13:08:53.411: INFO: sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-n84sm from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:08:53.411: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:08:53.411: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 07/29/23 13:08:53.412
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.1776589b217fc96b], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 07/29/23 13:08:53.437
  Jul 29 13:08:54.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9064" for this suite. @ 07/29/23 13:08:54.44
• [1.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 07/29/23 13:08:54.447
  Jul 29 13:08:54.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pod-network-test @ 07/29/23 13:08:54.448
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:08:54.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:08:54.466
  STEP: Performing setup for networking test in namespace pod-network-test-136 @ 07/29/23 13:08:54.469
  STEP: creating a selector @ 07/29/23 13:08:54.469
  STEP: Creating the service pods in kubernetes @ 07/29/23 13:08:54.469
  Jul 29 13:08:54.469: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 07/29/23 13:09:16.56
  Jul 29 13:09:18.576: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 29 13:09:18.576: INFO: Breadth first check of 192.168.10.36 on host 172.31.19.67...
  Jul 29 13:09:18.579: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.74:9080/dial?request=hostname&protocol=http&host=192.168.10.36&port=8083&tries=1'] Namespace:pod-network-test-136 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:09:18.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:09:18.579: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:09:18.580: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-136/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.129.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.10.36%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 13:09:18.642: INFO: Waiting for responses: map[]
  Jul 29 13:09:18.642: INFO: reached 192.168.10.36 after 0/1 tries
  Jul 29 13:09:18.642: INFO: Breadth first check of 192.168.129.82 on host 172.31.33.37...
  Jul 29 13:09:18.646: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.74:9080/dial?request=hostname&protocol=http&host=192.168.129.82&port=8083&tries=1'] Namespace:pod-network-test-136 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:09:18.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:09:18.647: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:09:18.647: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-136/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.129.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.129.82%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 13:09:18.708: INFO: Waiting for responses: map[]
  Jul 29 13:09:18.708: INFO: reached 192.168.129.82 after 0/1 tries
  Jul 29 13:09:18.708: INFO: Breadth first check of 192.168.8.128 on host 172.31.5.66...
  Jul 29 13:09:18.711: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.74:9080/dial?request=hostname&protocol=http&host=192.168.8.128&port=8083&tries=1'] Namespace:pod-network-test-136 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:09:18.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:09:18.712: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:09:18.712: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-136/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.129.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.8.128%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 13:09:18.770: INFO: Waiting for responses: map[]
  Jul 29 13:09:18.770: INFO: reached 192.168.8.128 after 0/1 tries
  Jul 29 13:09:18.770: INFO: Going to retry 0 out of 3 pods....
  Jul 29 13:09:18.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-136" for this suite. @ 07/29/23 13:09:18.774
• [24.334 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 07/29/23 13:09:18.783
  Jul 29 13:09:18.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 13:09:18.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:09:18.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:09:18.802
  STEP: Creating a suspended cronjob @ 07/29/23 13:09:18.806
  STEP: Ensuring no jobs are scheduled @ 07/29/23 13:09:18.811
  STEP: Ensuring no job exists by listing jobs explicitly @ 07/29/23 13:14:18.823
  STEP: Removing cronjob @ 07/29/23 13:14:18.827
  Jul 29 13:14:18.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2532" for this suite. @ 07/29/23 13:14:18.838
• [300.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 07/29/23 13:14:18.848
  Jul 29 13:14:18.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 13:14:18.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:14:18.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:14:18.876
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/29/23 13:14:18.88
  STEP: Saw pod success @ 07/29/23 13:14:22.907
  Jul 29 13:14:22.911: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-91a5b1e5-10a3-46c3-8ad9-788c32aaf14e container test-container: <nil>
  STEP: delete the pod @ 07/29/23 13:14:22.935
  Jul 29 13:14:22.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2883" for this suite. @ 07/29/23 13:14:22.96
• [4.118 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 07/29/23 13:14:22.967
  Jul 29 13:14:22.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 13:14:22.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:14:22.989
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:14:22.994
  Jul 29 13:14:22.997: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 07/29/23 13:14:24.009
  STEP: Checking rc "condition-test" has the desired failure condition set @ 07/29/23 13:14:24.017
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 07/29/23 13:14:25.027
  Jul 29 13:14:25.037: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 07/29/23 13:14:25.037
  Jul 29 13:14:26.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5667" for this suite. @ 07/29/23 13:14:26.052
• [3.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 07/29/23 13:14:26.061
  Jul 29 13:14:26.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 13:14:26.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:14:26.076
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:14:26.079
  Jul 29 13:14:26.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/29/23 13:14:27.399
  Jul 29 13:14:27.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-8134 --namespace=crd-publish-openapi-8134 create -f -'
  Jul 29 13:14:28.376: INFO: stderr: ""
  Jul 29 13:14:28.376: INFO: stdout: "e2e-test-crd-publish-openapi-8772-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul 29 13:14:28.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-8134 --namespace=crd-publish-openapi-8134 delete e2e-test-crd-publish-openapi-8772-crds test-cr'
  Jul 29 13:14:28.445: INFO: stderr: ""
  Jul 29 13:14:28.445: INFO: stdout: "e2e-test-crd-publish-openapi-8772-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jul 29 13:14:28.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-8134 --namespace=crd-publish-openapi-8134 apply -f -'
  Jul 29 13:14:28.815: INFO: stderr: ""
  Jul 29 13:14:28.815: INFO: stdout: "e2e-test-crd-publish-openapi-8772-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul 29 13:14:28.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-8134 --namespace=crd-publish-openapi-8134 delete e2e-test-crd-publish-openapi-8772-crds test-cr'
  Jul 29 13:14:28.902: INFO: stderr: ""
  Jul 29 13:14:28.902: INFO: stdout: "e2e-test-crd-publish-openapi-8772-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 07/29/23 13:14:28.902
  Jul 29 13:14:28.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-8134 explain e2e-test-crd-publish-openapi-8772-crds'
  Jul 29 13:14:29.431: INFO: stderr: ""
  Jul 29 13:14:29.431: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-8772-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  Jul 29 13:14:30.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8134" for this suite. @ 07/29/23 13:14:30.724
• [4.672 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 07/29/23 13:14:30.735
  Jul 29 13:14:30.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename podtemplate @ 07/29/23 13:14:30.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:14:30.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:14:30.761
  Jul 29 13:14:30.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-8272" for this suite. @ 07/29/23 13:14:30.796
• [0.069 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 07/29/23 13:14:30.805
  Jul 29 13:14:30.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 13:14:30.806
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:14:30.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:14:30.823
  Jul 29 13:14:30.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/29/23 13:14:32.108
  Jul 29 13:14:32.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-9875 --namespace=crd-publish-openapi-9875 create -f -'
  Jul 29 13:14:32.603: INFO: stderr: ""
  Jul 29 13:14:32.603: INFO: stdout: "e2e-test-crd-publish-openapi-6134-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul 29 13:14:32.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-9875 --namespace=crd-publish-openapi-9875 delete e2e-test-crd-publish-openapi-6134-crds test-cr'
  Jul 29 13:14:32.671: INFO: stderr: ""
  Jul 29 13:14:32.671: INFO: stdout: "e2e-test-crd-publish-openapi-6134-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jul 29 13:14:32.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-9875 --namespace=crd-publish-openapi-9875 apply -f -'
  Jul 29 13:14:32.938: INFO: stderr: ""
  Jul 29 13:14:32.938: INFO: stdout: "e2e-test-crd-publish-openapi-6134-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul 29 13:14:32.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-9875 --namespace=crd-publish-openapi-9875 delete e2e-test-crd-publish-openapi-6134-crds test-cr'
  Jul 29 13:14:33.006: INFO: stderr: ""
  Jul 29 13:14:33.006: INFO: stdout: "e2e-test-crd-publish-openapi-6134-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/29/23 13:14:33.006
  Jul 29 13:14:33.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-9875 explain e2e-test-crd-publish-openapi-6134-crds'
  Jul 29 13:14:33.205: INFO: stderr: ""
  Jul 29 13:14:33.205: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-6134-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Jul 29 13:14:34.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9875" for this suite. @ 07/29/23 13:14:34.5
• [3.701 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 07/29/23 13:14:34.507
  Jul 29 13:14:34.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 13:14:34.508
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:14:34.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:14:34.528
  STEP: creating a ReplicationController @ 07/29/23 13:14:34.533
  STEP: waiting for RC to be added @ 07/29/23 13:14:34.625
  STEP: waiting for available Replicas @ 07/29/23 13:14:34.625
  STEP: patching ReplicationController @ 07/29/23 13:14:35.917
  STEP: waiting for RC to be modified @ 07/29/23 13:14:35.928
  STEP: patching ReplicationController status @ 07/29/23 13:14:35.928
  STEP: waiting for RC to be modified @ 07/29/23 13:14:35.934
  STEP: waiting for available Replicas @ 07/29/23 13:14:35.934
  STEP: fetching ReplicationController status @ 07/29/23 13:14:35.938
  STEP: patching ReplicationController scale @ 07/29/23 13:14:35.943
  STEP: waiting for RC to be modified @ 07/29/23 13:14:35.95
  STEP: waiting for ReplicationController's scale to be the max amount @ 07/29/23 13:14:35.95
  STEP: fetching ReplicationController; ensuring that it's patched @ 07/29/23 13:14:37.136
  STEP: updating ReplicationController status @ 07/29/23 13:14:37.139
  STEP: waiting for RC to be modified @ 07/29/23 13:14:37.144
  STEP: listing all ReplicationControllers @ 07/29/23 13:14:37.144
  STEP: checking that ReplicationController has expected values @ 07/29/23 13:14:37.151
  STEP: deleting ReplicationControllers by collection @ 07/29/23 13:14:37.151
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 07/29/23 13:14:37.162
  Jul 29 13:14:37.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 13:14:37.206625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-9645" for this suite. @ 07/29/23 13:14:37.209
• [2.709 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 07/29/23 13:14:37.216
  Jul 29 13:14:37.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename disruption @ 07/29/23 13:14:37.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:14:37.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:14:37.235
  STEP: creating the pdb @ 07/29/23 13:14:37.237
  STEP: Waiting for the pdb to be processed @ 07/29/23 13:14:37.246
  E0729 13:14:38.206926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:14:39.207014      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 07/29/23 13:14:39.253
  STEP: Waiting for the pdb to be processed @ 07/29/23 13:14:39.261
  STEP: patching the pdb @ 07/29/23 13:14:39.267
  STEP: Waiting for the pdb to be processed @ 07/29/23 13:14:39.277
  STEP: Waiting for the pdb to be deleted @ 07/29/23 13:14:39.29
  Jul 29 13:14:39.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5463" for this suite. @ 07/29/23 13:14:39.297
• [2.087 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 07/29/23 13:14:39.304
  Jul 29 13:14:39.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:14:39.305
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:14:39.32
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:14:39.323
  STEP: Creating the pod @ 07/29/23 13:14:39.327
  E0729 13:14:40.207115      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:14:41.207481      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:14:41.864: INFO: Successfully updated pod "labelsupdatebe731561-b22b-449f-a398-d42f9decac20"
  E0729 13:14:42.207548      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:14:43.207988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:14:43.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7400" for this suite. @ 07/29/23 13:14:43.895
• [4.600 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 07/29/23 13:14:43.905
  Jul 29 13:14:43.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:14:43.905
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:14:43.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:14:43.927
  STEP: Creating the pod @ 07/29/23 13:14:43.929
  E0729 13:14:44.208082      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:14:45.208170      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:14:46.208197      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:14:46.473: INFO: Successfully updated pod "annotationupdate7dff3f7e-a0c9-4c0b-873c-0cbd0e594f6d"
  E0729 13:14:47.208553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:14:48.209197      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:14:48.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9632" for this suite. @ 07/29/23 13:14:48.493
• [4.594 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 07/29/23 13:14:48.499
  Jul 29 13:14:48.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename deployment @ 07/29/23 13:14:48.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:14:48.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:14:48.521
  Jul 29 13:14:48.533: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0729 13:14:49.209323      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:14:50.209604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:14:51.210349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:14:52.210562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:14:53.210696      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:14:53.537: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 13:14:53.537
  Jul 29 13:14:53.537: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 07/29/23 13:14:53.549
  Jul 29 13:14:53.561: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5408  8be91982-091b-4456-be3a-6364bb17c0a7 29410 1 2023-07-29 13:14:53 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-07-29 13:14:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003acb8a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Jul 29 13:14:53.564: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Jul 29 13:14:53.564: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Jul 29 13:14:53.565: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5408  ae18422e-d450-4c0b-9e02-43d28df327da 29412 1 2023-07-29 13:14:48 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 8be91982-091b-4456-be3a-6364bb17c0a7 0xc0051d6837 0xc0051d6838}] [] [{e2e.test Update apps/v1 2023-07-29 13:14:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 13:14:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-29 13:14:53 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"8be91982-091b-4456-be3a-6364bb17c0a7\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0051d6918 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 13:14:53.570: INFO: Pod "test-cleanup-controller-r4bn2" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-r4bn2 test-cleanup-controller- deployment-5408  fe13c263-b180-4806-bc5c-f3845e05ca56 29400 0 2023-07-29 13:14:48 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller ae18422e-d450-4c0b-9e02-43d28df327da 0xc003acbc17 0xc003acbc18}] [] [{kube-controller-manager Update v1 2023-07-29 13:14:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ae18422e-d450-4c0b-9e02-43d28df327da\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 13:14:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.10.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vtjl6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vtjl6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:14:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:14:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:14:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:14:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:192.168.10.40,StartTime:2023-07-29 13:14:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 13:14:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4fcda3efc80d3ed0ad8c02a6bbdc47cd8ce93a4b4375d9e9061613bd5ca22558,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.10.40,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 13:14:53.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5408" for this suite. @ 07/29/23 13:14:53.581
• [5.093 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 07/29/23 13:14:53.592
  Jul 29 13:14:53.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 13:14:53.594
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:14:53.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:14:53.618
  Jul 29 13:14:53.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8658" for this suite. @ 07/29/23 13:14:53.669
• [0.082 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 07/29/23 13:14:53.675
  Jul 29 13:14:53.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 13:14:53.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:14:53.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:14:53.696
  STEP: Creating configMap configmap-2538/configmap-test-564f3d9f-284d-48b1-9612-651bdceca4a1 @ 07/29/23 13:14:53.699
  STEP: Creating a pod to test consume configMaps @ 07/29/23 13:14:53.705
  E0729 13:14:54.211223      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:14:55.211299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:14:56.211853      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:14:57.211923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:14:57.73
  Jul 29 13:14:57.733: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-configmaps-cf06983d-fdd9-439d-899f-5ee9cd017569 container env-test: <nil>
  STEP: delete the pod @ 07/29/23 13:14:57.741
  Jul 29 13:14:57.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2538" for this suite. @ 07/29/23 13:14:57.774
• [4.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 07/29/23 13:14:57.782
  Jul 29 13:14:57.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 13:14:57.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:14:57.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:14:57.801
  STEP: Creating simple DaemonSet "daemon-set" @ 07/29/23 13:14:57.82
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 13:14:57.826
  Jul 29 13:14:57.828: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:14:57.829: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:14:57.833: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 13:14:57.833: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  E0729 13:14:58.212772      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:14:58.841: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:14:58.841: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:14:58.844: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 13:14:58.844: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  E0729 13:14:59.213099      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:14:59.836: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:14:59.837: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:14:59.840: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 13:14:59.840: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 07/29/23 13:14:59.842
  Jul 29 13:14:59.857: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:14:59.857: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:14:59.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 13:14:59.861: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  E0729 13:15:00.214169      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:15:00.866: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:15:00.866: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:15:00.869: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 13:15:00.870: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  E0729 13:15:01.214267      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:15:01.865: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:15:01.865: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:15:01.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 13:15:01.868: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  E0729 13:15:02.214358      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:15:02.867: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:15:02.867: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:15:02.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 13:15:02.871: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 13:15:02.873
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8375, will wait for the garbage collector to delete the pods @ 07/29/23 13:15:02.873
  Jul 29 13:15:02.935: INFO: Deleting DaemonSet.extensions daemon-set took: 7.067913ms
  Jul 29 13:15:03.035: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.666205ms
  E0729 13:15:03.215994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:04.217003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:15:04.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 13:15:04.240: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 13:15:04.243: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29646"},"items":null}

  Jul 29 13:15:04.246: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29646"},"items":null}

  Jul 29 13:15:04.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8375" for this suite. @ 07/29/23 13:15:04.262
• [6.488 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 07/29/23 13:15:04.27
  Jul 29 13:15:04.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 13:15:04.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:15:04.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:15:04.291
  STEP: Setting up server cert @ 07/29/23 13:15:04.312
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 13:15:04.658
  STEP: Deploying the webhook pod @ 07/29/23 13:15:04.668
  STEP: Wait for the deployment to be ready @ 07/29/23 13:15:04.68
  Jul 29 13:15:04.690: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 13:15:05.217760      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:06.218233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 13:15:06.699
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 13:15:06.708
  E0729 13:15:07.218478      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:15:07.709: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 29 13:15:07.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  E0729 13:15:08.218920      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5515-crds.webhook.example.com via the AdmissionRegistration API @ 07/29/23 13:15:08.223
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/29/23 13:15:08.241
  E0729 13:15:09.219993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:10.220189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:15:10.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9101" for this suite. @ 07/29/23 13:15:10.849
  STEP: Destroying namespace "webhook-markers-7252" for this suite. @ 07/29/23 13:15:10.855
• [6.591 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 07/29/23 13:15:10.863
  Jul 29 13:15:10.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 13:15:10.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:15:10.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:15:10.883
  STEP: creating a collection of services @ 07/29/23 13:15:10.886
  Jul 29 13:15:10.886: INFO: Creating e2e-svc-a-jxhzb
  Jul 29 13:15:10.895: INFO: Creating e2e-svc-b-rpjqd
  Jul 29 13:15:10.905: INFO: Creating e2e-svc-c-jff88
  STEP: deleting service collection @ 07/29/23 13:15:10.919
  Jul 29 13:15:10.948: INFO: Collection of services has been deleted
  Jul 29 13:15:10.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8742" for this suite. @ 07/29/23 13:15:10.952
• [0.096 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 07/29/23 13:15:10.96
  Jul 29 13:15:10.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 13:15:10.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:15:10.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:15:10.981
  STEP: Creating configMap with name configmap-test-volume-bce66e27-befd-46b2-aa9e-660cc5540447 @ 07/29/23 13:15:10.983
  STEP: Creating a pod to test consume configMaps @ 07/29/23 13:15:10.988
  E0729 13:15:11.220892      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:12.220979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:13.221363      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:14.222015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:15:15.009
  Jul 29 13:15:15.012: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-configmaps-6bc95e1e-d45d-430e-9604-e89065f6ae65 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 13:15:15.018
  Jul 29 13:15:15.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6901" for this suite. @ 07/29/23 13:15:15.045
• [4.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 07/29/23 13:15:15.053
  Jul 29 13:15:15.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 13:15:15.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:15:15.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:15:15.078
  Jul 29 13:15:15.117: INFO: created pod
  E0729 13:15:15.222860      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:16.223260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:17.223342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:18.224001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:15:19.131
  E0729 13:15:19.225005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:20.225686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:21.226004      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:22.226984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:23.227977      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:24.228727      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:25.228839      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:26.229260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:27.229408      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:28.230148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:29.230241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:30.230940      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:31.231668      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:32.231753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:33.232461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:34.232553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:35.232658      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:36.232987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:37.233116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:38.233177      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:39.233336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:40.234046      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:41.234344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:42.234531      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:43.234625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:44.234823      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:45.234972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:46.235268      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:47.235369      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:48.235448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:15:49.132: INFO: polling logs
  Jul 29 13:15:49.141: INFO: Pod logs: 
  I0729 13:15:15.819177       1 log.go:198] OK: Got token
  I0729 13:15:15.819380       1 log.go:198] validating with in-cluster discovery
  I0729 13:15:15.819713       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0729 13:15:15.819801       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8690:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1690637115, NotBefore:1690636515, IssuedAt:1690636515, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8690", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b86671bb-b65b-4f51-83be-10a100e2a757"}}}
  I0729 13:15:15.830955       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0729 13:15:15.838020       1 log.go:198] OK: Validated signature on JWT
  I0729 13:15:15.838109       1 log.go:198] OK: Got valid claims from token!
  I0729 13:15:15.838190       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8690:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1690637115, NotBefore:1690636515, IssuedAt:1690636515, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8690", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b86671bb-b65b-4f51-83be-10a100e2a757"}}}

  Jul 29 13:15:49.141: INFO: completed pod
  Jul 29 13:15:49.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8690" for this suite. @ 07/29/23 13:15:49.154
• [34.120 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 07/29/23 13:15:49.175
  Jul 29 13:15:49.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:15:49.177
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:15:49.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:15:49.199
  STEP: Creating configMap with name projected-configmap-test-volume-map-7cbb6c58-6e31-4434-b0aa-5f5e71b81499 @ 07/29/23 13:15:49.203
  STEP: Creating a pod to test consume configMaps @ 07/29/23 13:15:49.212
  E0729 13:15:49.236050      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:50.236155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:51.236950      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:52.237118      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:15:53.235
  E0729 13:15:53.237397      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:15:53.238: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-projected-configmaps-7b045f5d-e329-4319-a1a1-658c8034efe8 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 13:15:53.245
  Jul 29 13:15:53.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1773" for this suite. @ 07/29/23 13:15:53.266
• [4.098 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 07/29/23 13:15:53.274
  Jul 29 13:15:53.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename deployment @ 07/29/23 13:15:53.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:15:53.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:15:53.291
  Jul 29 13:15:53.307: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0729 13:15:54.238084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:55.238164      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:56.238225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:57.238461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:15:58.238548      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:15:58.311: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 13:15:58.311
  Jul 29 13:15:58.311: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0729 13:15:59.238655      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:00.238810      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:16:00.315: INFO: Creating deployment "test-rollover-deployment"
  Jul 29 13:16:00.322: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0729 13:16:01.239346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:02.239987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:16:02.330: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jul 29 13:16:02.336: INFO: Ensure that both replica sets have 1 created replica
  Jul 29 13:16:02.342: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jul 29 13:16:02.353: INFO: Updating deployment test-rollover-deployment
  Jul 29 13:16:02.353: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0729 13:16:03.240020      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:04.240104      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:16:04.360: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jul 29 13:16:04.366: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jul 29 13:16:04.372: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 13:16:04.372: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 13, 16, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0729 13:16:05.240196      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:06.240240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:16:06.380: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 13:16:06.380: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 13, 16, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0729 13:16:07.240356      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:08.240439      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:16:08.379: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 13:16:08.380: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 13, 16, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0729 13:16:09.240670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:10.241225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:16:10.381: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 13:16:10.381: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 13, 16, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0729 13:16:11.241267      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:12.241356      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:16:12.380: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 13:16:12.381: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 13, 16, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 13, 16, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0729 13:16:13.241820      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:14.241900      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:16:14.380: INFO: 
  Jul 29 13:16:14.380: INFO: Ensure that both old replica sets have no replicas
  Jul 29 13:16:14.389: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-1061  8d8efe47-1446-4a60-bdd1-508208ac85d0 30123 2 2023-07-29 13:16:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-29 13:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 13:16:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041604c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 13:16:00 +0000 UTC,LastTransitionTime:2023-07-29 13:16:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-07-29 13:16:13 +0000 UTC,LastTransitionTime:2023-07-29 13:16:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 29 13:16:14.392: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-1061  07dbbe3c-0d04-4168-91f4-906193e40641 30113 2 2023-07-29 13:16:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 8d8efe47-1446-4a60-bdd1-508208ac85d0 0xc004160977 0xc004160978}] [] [{kube-controller-manager Update apps/v1 2023-07-29 13:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d8efe47-1446-4a60-bdd1-508208ac85d0\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 13:16:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004160a28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 13:16:14.392: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jul 29 13:16:14.392: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1061  a2eb41b7-70bf-41f4-930d-68b953011c9d 30122 2 2023-07-29 13:15:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 8d8efe47-1446-4a60-bdd1-508208ac85d0 0xc004160847 0xc004160848}] [] [{e2e.test Update apps/v1 2023-07-29 13:15:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 13:16:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d8efe47-1446-4a60-bdd1-508208ac85d0\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-29 13:16:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004160908 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 13:16:14.392: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-1061  64593c3e-d8c7-4f70-9500-7c73701c2180 30070 2 2023-07-29 13:16:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 8d8efe47-1446-4a60-bdd1-508208ac85d0 0xc004160a97 0xc004160a98}] [] [{kube-controller-manager Update apps/v1 2023-07-29 13:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8d8efe47-1446-4a60-bdd1-508208ac85d0\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 13:16:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004160b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 13:16:14.396: INFO: Pod "test-rollover-deployment-57777854c9-zgsnj" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-zgsnj test-rollover-deployment-57777854c9- deployment-1061  6e81342f-37db-47f5-a093-20822a482091 30088 0 2023-07-29 13:16:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 07dbbe3c-0d04-4168-91f4-906193e40641 0xc0041610d7 0xc0041610d8}] [] [{kube-controller-manager Update v1 2023-07-29 13:16:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07dbbe3c-0d04-4168-91f4-906193e40641\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 13:16:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.129.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lp74,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lp74,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:16:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:16:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:16:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:16:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:192.168.129.77,StartTime:2023-07-29 13:16:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 13:16:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://a8c8ee3261cec60eb24729a59ab64a5ddec8a0049e6367c954105cf25e4dbf75,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.129.77,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 13:16:14.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1061" for this suite. @ 07/29/23 13:16:14.399
• [21.132 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 07/29/23 13:16:14.406
  Jul 29 13:16:14.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename watch @ 07/29/23 13:16:14.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:16:14.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:16:14.427
  STEP: creating a watch on configmaps @ 07/29/23 13:16:14.429
  STEP: creating a new configmap @ 07/29/23 13:16:14.43
  STEP: modifying the configmap once @ 07/29/23 13:16:14.435
  STEP: closing the watch once it receives two notifications @ 07/29/23 13:16:14.441
  Jul 29 13:16:14.441: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7306  643a7819-84f5-420b-afc7-744cb2c3d896 30135 0 2023-07-29 13:16:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 13:16:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:16:14.441: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7306  643a7819-84f5-420b-afc7-744cb2c3d896 30136 0 2023-07-29 13:16:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 13:16:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 07/29/23 13:16:14.441
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 07/29/23 13:16:14.449
  STEP: deleting the configmap @ 07/29/23 13:16:14.45
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 07/29/23 13:16:14.457
  Jul 29 13:16:14.457: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7306  643a7819-84f5-420b-afc7-744cb2c3d896 30137 0 2023-07-29 13:16:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 13:16:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:16:14.457: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7306  643a7819-84f5-420b-afc7-744cb2c3d896 30138 0 2023-07-29 13:16:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 13:16:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:16:14.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7306" for this suite. @ 07/29/23 13:16:14.461
• [0.061 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 07/29/23 13:16:14.468
  Jul 29 13:16:14.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 13:16:14.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:16:14.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:16:14.487
  STEP: fetching the /apis discovery document @ 07/29/23 13:16:14.489
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 07/29/23 13:16:14.49
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 07/29/23 13:16:14.49
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 07/29/23 13:16:14.491
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 07/29/23 13:16:14.492
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 07/29/23 13:16:14.492
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 07/29/23 13:16:14.493
  Jul 29 13:16:14.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6877" for this suite. @ 07/29/23 13:16:14.496
• [0.034 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 07/29/23 13:16:14.503
  Jul 29 13:16:14.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 13:16:14.504
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:16:14.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:16:14.521
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/29/23 13:16:14.523
  E0729 13:16:15.242769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:16.243228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:17.243320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:18.243413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:16:18.544
  Jul 29 13:16:18.547: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-6ead9aa2-6c8f-4d85-8f86-2bb1c998215d container test-container: <nil>
  STEP: delete the pod @ 07/29/23 13:16:18.555
  Jul 29 13:16:18.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9736" for this suite. @ 07/29/23 13:16:18.574
• [4.078 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 07/29/23 13:16:18.581
  Jul 29 13:16:18.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename sched-preemption @ 07/29/23 13:16:18.582
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:16:18.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:16:18.602
  Jul 29 13:16:18.619: INFO: Waiting up to 1m0s for all nodes to be ready
  E0729 13:16:19.243509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:20.243696      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:21.244304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:22.244403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:23.245340      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:24.245392      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:25.245518      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:26.246287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:27.246874      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:28.246936      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:29.247706      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:30.247911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:31.248242      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:32.248352      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:33.248438      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:34.248529      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:35.248619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:36.249080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:37.249915      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:38.250022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:39.250717      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:40.250919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:41.251972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:42.252276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:43.253249      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:44.253437      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:45.254296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:46.254533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:47.255538      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:48.255982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:49.256105      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:50.256277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:51.257228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:52.257320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:53.257410      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:54.257491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:55.257597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:56.258181      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:57.258281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:58.258463      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:16:59.258558      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:00.258632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:01.259347      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:02.259455      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:03.259966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:04.260077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:05.260163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:06.260243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:07.260348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:08.260423      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:09.261219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:10.261310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:11.261927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:12.262023      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:13.262808      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:14.263047      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:15.263965      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:16.264289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:17.265281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:18.265376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:17:18.634: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/29/23 13:17:18.637
  Jul 29 13:17:18.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/29/23 13:17:18.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:17:18.653
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:17:18.656
  STEP: Finding an available node @ 07/29/23 13:17:18.658
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/29/23 13:17:18.658
  E0729 13:17:19.265434      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:20.265518      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/29/23 13:17:20.679
  Jul 29 13:17:20.692: INFO: found a healthy node: ip-172-31-33-37
  E0729 13:17:21.265908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:22.265996      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:23.266985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:24.267989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:25.268650      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:26.269172      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:17:26.760: INFO: pods created so far: [1 1 1]
  Jul 29 13:17:26.760: INFO: length of pods created so far: 3
  E0729 13:17:27.269942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:28.270610      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:17:28.770: INFO: pods created so far: [2 2 1]
  E0729 13:17:29.271389      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:30.271505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:31.272378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:32.272555      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:33.272646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:34.272831      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:35.272926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:17:35.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 13:17:35.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-2523" for this suite. @ 07/29/23 13:17:35.845
  STEP: Destroying namespace "sched-preemption-1519" for this suite. @ 07/29/23 13:17:35.851
• [77.277 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 07/29/23 13:17:35.859
  Jul 29 13:17:35.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 13:17:35.86
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:17:35.875
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:17:35.877
  STEP: Creating configMap with name configmap-test-volume-c472c8fc-15e1-4b92-b455-3bac73bce82f @ 07/29/23 13:17:35.879
  STEP: Creating a pod to test consume configMaps @ 07/29/23 13:17:35.884
  E0729 13:17:36.273018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:37.273392      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:38.273507      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:39.273793      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:17:39.914
  Jul 29 13:17:39.917: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-configmaps-82fe2d55-4fb9-4aae-a305-cb504fbba25a container configmap-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 13:17:39.923
  Jul 29 13:17:40.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2" for this suite. @ 07/29/23 13:17:40.023
• [4.171 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 07/29/23 13:17:40.031
  Jul 29 13:17:40.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 13:17:40.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:17:40.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:17:40.052
  Jul 29 13:17:40.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  E0729 13:17:40.273880      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:41.273973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 07/29/23 13:17:41.513
  Jul 29 13:17:41.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 --namespace=crd-publish-openapi-3353 create -f -'
  Jul 29 13:17:42.230: INFO: stderr: ""
  Jul 29 13:17:42.230: INFO: stdout: "e2e-test-crd-publish-openapi-4636-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul 29 13:17:42.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 --namespace=crd-publish-openapi-3353 delete e2e-test-crd-publish-openapi-4636-crds test-foo'
  E0729 13:17:42.274363      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:17:42.297: INFO: stderr: ""
  Jul 29 13:17:42.297: INFO: stdout: "e2e-test-crd-publish-openapi-4636-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jul 29 13:17:42.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 --namespace=crd-publish-openapi-3353 apply -f -'
  Jul 29 13:17:42.553: INFO: stderr: ""
  Jul 29 13:17:42.553: INFO: stdout: "e2e-test-crd-publish-openapi-4636-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul 29 13:17:42.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 --namespace=crd-publish-openapi-3353 delete e2e-test-crd-publish-openapi-4636-crds test-foo'
  Jul 29 13:17:42.672: INFO: stderr: ""
  Jul 29 13:17:42.672: INFO: stdout: "e2e-test-crd-publish-openapi-4636-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 07/29/23 13:17:42.672
  Jul 29 13:17:42.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 --namespace=crd-publish-openapi-3353 create -f -'
  Jul 29 13:17:43.238: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 07/29/23 13:17:43.238
  Jul 29 13:17:43.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 --namespace=crd-publish-openapi-3353 create -f -'
  E0729 13:17:43.274856      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:17:43.508: INFO: rc: 1
  Jul 29 13:17:43.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 --namespace=crd-publish-openapi-3353 apply -f -'
  Jul 29 13:17:43.702: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 07/29/23 13:17:43.702
  Jul 29 13:17:43.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 --namespace=crd-publish-openapi-3353 create -f -'
  Jul 29 13:17:43.895: INFO: rc: 1
  Jul 29 13:17:43.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 --namespace=crd-publish-openapi-3353 apply -f -'
  Jul 29 13:17:44.089: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 07/29/23 13:17:44.089
  Jul 29 13:17:44.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 explain e2e-test-crd-publish-openapi-4636-crds'
  Jul 29 13:17:44.272: INFO: stderr: ""
  Jul 29 13:17:44.272: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4636-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 07/29/23 13:17:44.272
  Jul 29 13:17:44.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 explain e2e-test-crd-publish-openapi-4636-crds.metadata'
  E0729 13:17:44.275353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:17:44.460: INFO: stderr: ""
  Jul 29 13:17:44.460: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4636-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jul 29 13:17:44.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 explain e2e-test-crd-publish-openapi-4636-crds.spec'
  Jul 29 13:17:44.650: INFO: stderr: ""
  Jul 29 13:17:44.650: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4636-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jul 29 13:17:44.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 explain e2e-test-crd-publish-openapi-4636-crds.spec.bars'
  Jul 29 13:17:44.832: INFO: stderr: ""
  Jul 29 13:17:44.832: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4636-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 07/29/23 13:17:44.833
  Jul 29 13:17:44.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=crd-publish-openapi-3353 explain e2e-test-crd-publish-openapi-4636-crds.spec.bars2'
  Jul 29 13:17:45.020: INFO: rc: 1
  E0729 13:17:45.275792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:46.276384      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:17:46.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3353" for this suite. @ 07/29/23 13:17:46.305
• [6.281 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 07/29/23 13:17:46.313
  Jul 29 13:17:46.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 13:17:46.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:17:46.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:17:46.336
  STEP: Creating service test in namespace statefulset-6503 @ 07/29/23 13:17:46.339
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 07/29/23 13:17:46.347
  STEP: Creating stateful set ss in namespace statefulset-6503 @ 07/29/23 13:17:46.353
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6503 @ 07/29/23 13:17:46.359
  Jul 29 13:17:46.361: INFO: Found 0 stateful pods, waiting for 1
  E0729 13:17:47.276537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:48.277480      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:49.277863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:50.278070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:51.278542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:52.278762      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:53.278927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:54.279987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:55.280165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:56.280270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:17:56.367: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 07/29/23 13:17:56.367
  Jul 29 13:17:56.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-6503 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 13:17:56.503: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 13:17:56.503: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 13:17:56.503: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 13:17:56.506: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0729 13:17:57.281270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:58.281525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:17:59.282113      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:00.282275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:01.282387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:02.282482      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:03.282530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:04.282599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:05.282756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:06.283118      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:06.511: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 13:18:06.511: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 13:18:06.526: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999838s
  E0729 13:18:07.283709      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:07.532: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996013907s
  E0729 13:18:08.283814      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:08.536: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990519152s
  E0729 13:18:09.284356      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:09.541: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987174463s
  E0729 13:18:10.284987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:10.544: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982552899s
  E0729 13:18:11.285156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:11.548: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.979425563s
  E0729 13:18:12.285233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:12.553: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.975208942s
  E0729 13:18:13.286311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:13.556: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.970355629s
  E0729 13:18:14.286353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:14.561: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.966106123s
  E0729 13:18:15.287371      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:15.566: INFO: Verifying statefulset ss doesn't scale past 1 for another 962.140835ms
  E0729 13:18:16.288348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6503 @ 07/29/23 13:18:16.566
  Jul 29 13:18:16.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-6503 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 13:18:16.689: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 13:18:16.689: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 13:18:16.689: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 13:18:16.692: INFO: Found 1 stateful pods, waiting for 3
  E0729 13:18:17.288535      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:18.288623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:19.293634      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:20.293687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:21.294695      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:22.294752      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:23.294936      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:24.295012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:25.295977      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:26.296351      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:26.697: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 13:18:26.698: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 13:18:26.698: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 07/29/23 13:18:26.698
  STEP: Scale down will halt with unhealthy stateful pod @ 07/29/23 13:18:26.698
  Jul 29 13:18:26.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-6503 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 13:18:26.830: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 13:18:26.830: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 13:18:26.830: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 13:18:26.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-6503 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 13:18:26.955: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 13:18:26.955: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 13:18:26.955: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 13:18:26.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-6503 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 13:18:27.084: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 13:18:27.084: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 13:18:27.084: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 13:18:27.084: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 13:18:27.092: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0729 13:18:27.297128      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:28.297379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:29.298229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:30.298416      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:31.299402      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:32.299987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:33.300082      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:34.300135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:35.300310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:36.301337      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:37.102: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 13:18:37.102: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 13:18:37.102: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 13:18:37.116: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999822s
  E0729 13:18:37.301657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:38.121: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996078069s
  E0729 13:18:38.301674      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:39.125: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99182907s
  E0729 13:18:39.302432      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:40.129: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988112702s
  E0729 13:18:40.302833      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:41.135: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983548239s
  E0729 13:18:41.303683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:42.139: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977843811s
  E0729 13:18:42.304139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:43.144: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973341193s
  E0729 13:18:43.305077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:44.149: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968463882s
  E0729 13:18:44.305345      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:45.153: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964155781s
  E0729 13:18:45.306249      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:46.158: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.329747ms
  E0729 13:18:46.307159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6503 @ 07/29/23 13:18:47.158
  Jul 29 13:18:47.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-6503 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 13:18:47.290: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 13:18:47.290: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 13:18:47.290: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 13:18:47.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-6503 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0729 13:18:47.307509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:18:47.432: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 13:18:47.432: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 13:18:47.432: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 13:18:47.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-6503 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 13:18:47.558: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 13:18:47.558: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 13:18:47.558: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 13:18:47.558: INFO: Scaling statefulset ss to 0
  E0729 13:18:48.308595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:49.309350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:50.310331      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:51.310365      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:52.310439      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:53.310584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:54.310702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:55.310922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:56.311339      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:57.312195      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 07/29/23 13:18:57.573
  Jul 29 13:18:57.573: INFO: Deleting all statefulset in ns statefulset-6503
  Jul 29 13:18:57.577: INFO: Scaling statefulset ss to 0
  Jul 29 13:18:57.586: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 13:18:57.590: INFO: Deleting statefulset ss
  Jul 29 13:18:57.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6503" for this suite. @ 07/29/23 13:18:57.607
• [71.301 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 07/29/23 13:18:57.615
  Jul 29 13:18:57.615: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename security-context @ 07/29/23 13:18:57.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:18:57.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:18:57.636
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/29/23 13:18:57.639
  E0729 13:18:58.312974      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:18:59.313063      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:00.313156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:01.313394      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:19:01.657
  Jul 29 13:19:01.661: INFO: Trying to get logs from node ip-172-31-33-37 pod security-context-6d650ca1-0c36-4d26-9b9a-7b2c3390097e container test-container: <nil>
  STEP: delete the pod @ 07/29/23 13:19:01.676
  Jul 29 13:19:01.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-1966" for this suite. @ 07/29/23 13:19:01.693
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 07/29/23 13:19:01.703
  Jul 29 13:19:01.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename dns @ 07/29/23 13:19:01.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:19:01.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:19:01.723
  STEP: Creating a test externalName service @ 07/29/23 13:19:01.726
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3455.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3455.svc.cluster.local; sleep 1; done
   @ 07/29/23 13:19:01.729
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3455.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3455.svc.cluster.local; sleep 1; done
   @ 07/29/23 13:19:01.729
  STEP: creating a pod to probe DNS @ 07/29/23 13:19:01.729
  STEP: submitting the pod to kubernetes @ 07/29/23 13:19:01.729
  E0729 13:19:02.314033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:03.314126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/29/23 13:19:03.747
  STEP: looking for the results for each expected name from probers @ 07/29/23 13:19:03.749
  Jul 29 13:19:03.759: INFO: DNS probes using dns-test-195ac25c-b424-497f-a7a6-4b535d0e9d63 succeeded

  STEP: changing the externalName to bar.example.com @ 07/29/23 13:19:03.759
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3455.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3455.svc.cluster.local; sleep 1; done
   @ 07/29/23 13:19:03.767
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3455.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3455.svc.cluster.local; sleep 1; done
   @ 07/29/23 13:19:03.767
  STEP: creating a second pod to probe DNS @ 07/29/23 13:19:03.767
  STEP: submitting the pod to kubernetes @ 07/29/23 13:19:03.767
  E0729 13:19:04.316416      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:05.316781      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:06.317383      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:07.318223      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:08.318332      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:09.318441      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:10.319162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:11.319596      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/29/23 13:19:11.795
  STEP: looking for the results for each expected name from probers @ 07/29/23 13:19:11.798
  Jul 29 13:19:11.807: INFO: DNS probes using dns-test-c03d6772-57e9-46ab-91b1-a7294198cc5c succeeded

  STEP: changing the service to type=ClusterIP @ 07/29/23 13:19:11.807
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3455.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3455.svc.cluster.local; sleep 1; done
   @ 07/29/23 13:19:11.822
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3455.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3455.svc.cluster.local; sleep 1; done
   @ 07/29/23 13:19:11.822
  STEP: creating a third pod to probe DNS @ 07/29/23 13:19:11.822
  STEP: submitting the pod to kubernetes @ 07/29/23 13:19:11.825
  E0729 13:19:12.319976      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:13.320244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/29/23 13:19:13.839
  STEP: looking for the results for each expected name from probers @ 07/29/23 13:19:13.843
  Jul 29 13:19:13.852: INFO: DNS probes using dns-test-47123bfb-42bd-4a2b-a509-5b9d85be6615 succeeded

  Jul 29 13:19:13.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 13:19:13.856
  STEP: deleting the pod @ 07/29/23 13:19:13.867
  STEP: deleting the pod @ 07/29/23 13:19:13.879
  STEP: deleting the test externalName service @ 07/29/23 13:19:13.891
  STEP: Destroying namespace "dns-3455" for this suite. @ 07/29/23 13:19:13.909
• [12.213 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 07/29/23 13:19:13.917
  Jul 29 13:19:13.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 13:19:13.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:19:13.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:19:13.941
  STEP: Creating simple DaemonSet "daemon-set" @ 07/29/23 13:19:13.961
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 13:19:13.966
  Jul 29 13:19:13.969: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:19:13.969: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:19:13.973: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 13:19:13.973: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  E0729 13:19:14.320567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:19:14.976: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:19:14.977: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:19:14.979: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 13:19:14.979: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  E0729 13:19:15.321263      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:19:15.978: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:19:15.978: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:19:15.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 13:19:15.981: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  E0729 13:19:16.322015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:19:16.977: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:19:16.977: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:19:16.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 13:19:16.981: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  E0729 13:19:17.322056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:19:17.979: INFO: DaemonSet pods can't tolerate node ip-172-31-18-12 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:19:17.979: INFO: DaemonSet pods can't tolerate node ip-172-31-85-196 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jul 29 13:19:17.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 13:19:17.982: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 07/29/23 13:19:17.985
  STEP: DeleteCollection of the DaemonSets @ 07/29/23 13:19:17.989
  STEP: Verify that ReplicaSets have been deleted @ 07/29/23 13:19:17.997
  Jul 29 13:19:18.006: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31275"},"items":null}

  Jul 29 13:19:18.013: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31275"},"items":[{"metadata":{"name":"daemon-set-bvjqz","generateName":"daemon-set-","namespace":"daemonsets-3977","uid":"d2de6457-ab4a-4c19-ada7-6160748b3474","resourceVersion":"31260","creationTimestamp":"2023-07-29T13:19:13Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a5cc2b7b-0388-4a25-8014-c3ba6346d8fd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T13:19:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5cc2b7b-0388-4a25-8014-c3ba6346d8fd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T13:19:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.8.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wfvtc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wfvtc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-5-66","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-5-66"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T13:19:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T13:19:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T13:19:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T13:19:13Z"}],"hostIP":"172.31.5.66","podIP":"192.168.8.174","podIPs":[{"ip":"192.168.8.174"}],"startTime":"2023-07-29T13:19:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T13:19:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://7d9e631a32cbddc2455d727f2e31a7524d2b04a6429791ab7a70fdc3110a17ff","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-g67vl","generateName":"daemon-set-","namespace":"daemonsets-3977","uid":"c86e6366-67f1-47ce-8815-a21ac446d1f3","resourceVersion":"31272","creationTimestamp":"2023-07-29T13:19:13Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a5cc2b7b-0388-4a25-8014-c3ba6346d8fd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T13:19:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5cc2b7b-0388-4a25-8014-c3ba6346d8fd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T13:19:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.10.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-99bxm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-99bxm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-19-67","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-19-67"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T13:19:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T13:19:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T13:19:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T13:19:13Z"}],"hostIP":"172.31.19.67","podIP":"192.168.10.16","podIPs":[{"ip":"192.168.10.16"}],"startTime":"2023-07-29T13:19:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T13:19:17Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://293eecbb5da7c25a89f9821b148223fd8ec31b85acd75aa9fbc71e8aada9dd54","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vh5gw","generateName":"daemon-set-","namespace":"daemonsets-3977","uid":"21aa95ca-a858-4fb3-be5d-672e52197899","resourceVersion":"31257","creationTimestamp":"2023-07-29T13:19:13Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a5cc2b7b-0388-4a25-8014-c3ba6346d8fd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T13:19:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5cc2b7b-0388-4a25-8014-c3ba6346d8fd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T13:19:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.129.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-n976z","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-n976z","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-33-37","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-33-37"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T13:19:14Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T13:19:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T13:19:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T13:19:13Z"}],"hostIP":"172.31.33.37","podIP":"192.168.129.121","podIPs":[{"ip":"192.168.129.121"}],"startTime":"2023-07-29T13:19:14Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T13:19:14Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://2c5d6b88fffafbe83ba055c4ee624b0c65bd3eb602d9b7bfa780fbb668b996f7","started":true}],"qosClass":"BestEffort"}}]}

  Jul 29 13:19:18.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3977" for this suite. @ 07/29/23 13:19:18.032
• [4.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 07/29/23 13:19:18.04
  Jul 29 13:19:18.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename secrets @ 07/29/23 13:19:18.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:19:18.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:19:18.062
  STEP: Creating secret with name secret-test-2fb30845-d571-4231-9452-cf7e226d3e70 @ 07/29/23 13:19:18.065
  STEP: Creating a pod to test consume secrets @ 07/29/23 13:19:18.07
  E0729 13:19:18.323001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:19.323428      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:20.324287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:21.324373      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:19:22.089
  Jul 29 13:19:22.092: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-secrets-3b014659-a874-4494-a3e1-7fdfacba790d container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 13:19:22.1
  Jul 29 13:19:22.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1760" for this suite. @ 07/29/23 13:19:22.115
• [4.082 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 07/29/23 13:19:22.123
  Jul 29 13:19:22.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 13:19:22.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:19:22.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:19:22.142
  STEP: apply creating a deployment @ 07/29/23 13:19:22.144
  Jul 29 13:19:22.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8719" for this suite. @ 07/29/23 13:19:22.161
• [0.044 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 07/29/23 13:19:22.167
  Jul 29 13:19:22.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 13:19:22.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:19:22.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:19:22.188
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 13:19:22.19
  E0729 13:19:22.324470      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:23.324710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:24.325507      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:25.326479      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:19:26.21
  Jul 29 13:19:26.213: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-b6fc35fe-094f-4ef8-a0fd-d66cf117011d container client-container: <nil>
  STEP: delete the pod @ 07/29/23 13:19:26.225
  Jul 29 13:19:26.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3401" for this suite. @ 07/29/23 13:19:26.243
• [4.082 seconds]
------------------------------
SSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 07/29/23 13:19:26.25
  Jul 29 13:19:26.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename endpointslicemirroring @ 07/29/23 13:19:26.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:19:26.27
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:19:26.272
  STEP: mirroring a new custom Endpoint @ 07/29/23 13:19:26.284
  Jul 29 13:19:26.295: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0729 13:19:26.327298      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:27.327321      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 07/29/23 13:19:28.3
  Jul 29 13:19:28.308: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0729 13:19:28.328161      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:29.328347      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 07/29/23 13:19:30.313
  Jul 29 13:19:30.322: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0729 13:19:30.328378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:31.328762      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:19:32.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 13:19:32.329316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "endpointslicemirroring-5538" for this suite. @ 07/29/23 13:19:32.329
• [6.085 seconds]
------------------------------
S
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 07/29/23 13:19:32.336
  Jul 29 13:19:32.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename events @ 07/29/23 13:19:32.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:19:32.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:19:32.355
  STEP: creating a test event @ 07/29/23 13:19:32.358
  STEP: listing all events in all namespaces @ 07/29/23 13:19:32.364
  STEP: patching the test event @ 07/29/23 13:19:32.369
  STEP: fetching the test event @ 07/29/23 13:19:32.375
  STEP: updating the test event @ 07/29/23 13:19:32.379
  STEP: getting the test event @ 07/29/23 13:19:32.388
  STEP: deleting the test event @ 07/29/23 13:19:32.391
  STEP: listing all events in all namespaces @ 07/29/23 13:19:32.399
  Jul 29 13:19:32.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9748" for this suite. @ 07/29/23 13:19:32.407
• [0.077 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 07/29/23 13:19:32.414
  Jul 29 13:19:32.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename endpointslice @ 07/29/23 13:19:32.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:19:32.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:19:32.434
  E0729 13:19:33.329444      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:34.329534      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:35.330406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:36.331386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:37.331504      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 07/29/23 13:19:37.582
  E0729 13:19:38.332203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:39.332389      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:40.332732      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:41.333700      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:42.333788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 07/29/23 13:19:42.591
  E0729 13:19:43.333876      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:44.334664      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:45.334741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:46.334887      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:47.334918      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 07/29/23 13:19:47.598
  E0729 13:19:48.335026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:49.336021      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:50.336646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:51.337343      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:52.338018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 07/29/23 13:19:52.607
  Jul 29 13:19:52.627: INFO: EndpointSlice for Service endpointslice-1494/example-named-port not found
  E0729 13:19:53.338922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:54.339994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:55.340909      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:56.341373      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:57.342378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:58.342898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:19:59.342994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:00.343974      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:01.344342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:02.344754      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:20:02.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1494" for this suite. @ 07/29/23 13:20:02.639
• [30.231 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 07/29/23 13:20:02.645
  Jul 29 13:20:02.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 13:20:02.646
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:20:02.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:20:02.666
  Jul 29 13:20:02.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  W0729 13:20:02.670215      18 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc0008dffc0 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0729 13:20:03.345592      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:04.345847      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0729 13:20:05.221281      18 warnings.go:70] unknown field "alpha"
  W0729 13:20:05.221302      18 warnings.go:70] unknown field "beta"
  W0729 13:20:05.221308      18 warnings.go:70] unknown field "delta"
  W0729 13:20:05.221314      18 warnings.go:70] unknown field "epsilon"
  W0729 13:20:05.221319      18 warnings.go:70] unknown field "gamma"
  E0729 13:20:05.346662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:20:05.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7706" for this suite. @ 07/29/23 13:20:05.765
• [3.127 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 07/29/23 13:20:05.773
  Jul 29 13:20:05.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename ingress @ 07/29/23 13:20:05.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:20:05.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:20:05.794
  STEP: getting /apis @ 07/29/23 13:20:05.797
  STEP: getting /apis/networking.k8s.io @ 07/29/23 13:20:05.804
  STEP: getting /apis/networking.k8s.iov1 @ 07/29/23 13:20:05.805
  STEP: creating @ 07/29/23 13:20:05.806
  STEP: getting @ 07/29/23 13:20:05.821
  STEP: listing @ 07/29/23 13:20:05.825
  STEP: watching @ 07/29/23 13:20:05.831
  Jul 29 13:20:05.831: INFO: starting watch
  STEP: cluster-wide listing @ 07/29/23 13:20:05.833
  STEP: cluster-wide watching @ 07/29/23 13:20:05.837
  Jul 29 13:20:05.837: INFO: starting watch
  STEP: patching @ 07/29/23 13:20:05.839
  STEP: updating @ 07/29/23 13:20:05.845
  Jul 29 13:20:05.854: INFO: waiting for watch events with expected annotations
  Jul 29 13:20:05.854: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/29/23 13:20:05.855
  STEP: updating /status @ 07/29/23 13:20:05.861
  STEP: get /status @ 07/29/23 13:20:05.873
  STEP: deleting @ 07/29/23 13:20:05.876
  STEP: deleting a collection @ 07/29/23 13:20:05.889
  Jul 29 13:20:05.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-8092" for this suite. @ 07/29/23 13:20:05.911
• [0.146 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 07/29/23 13:20:05.919
  Jul 29 13:20:05.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename gc @ 07/29/23 13:20:05.92
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:20:05.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:20:05.946
  STEP: create the rc @ 07/29/23 13:20:05.953
  W0729 13:20:05.963038      18 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0729 13:20:06.347641      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:07.348353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:08.348572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:09.348800      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:10.349225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 07/29/23 13:20:10.969
  STEP: wait for all pods to be garbage collected @ 07/29/23 13:20:10.976
  E0729 13:20:11.349888      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:12.350244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:13.350350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:14.350411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:15.350605      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/29/23 13:20:15.982
  W0729 13:20:15.985651      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 29 13:20:15.985: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 13:20:15.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1196" for this suite. @ 07/29/23 13:20:15.988
• [10.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 07/29/23 13:20:15.999
  Jul 29 13:20:15.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename deployment @ 07/29/23 13:20:16
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:20:16.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:20:16.018
  STEP: creating a Deployment @ 07/29/23 13:20:16.024
  STEP: waiting for Deployment to be created @ 07/29/23 13:20:16.03
  STEP: waiting for all Replicas to be Ready @ 07/29/23 13:20:16.032
  Jul 29 13:20:16.033: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 13:20:16.033: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 13:20:16.040: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 13:20:16.040: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 13:20:16.055: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 13:20:16.055: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 13:20:16.078: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 13:20:16.078: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0729 13:20:16.350832      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:17.350919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:20:17.696: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jul 29 13:20:17.696: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jul 29 13:20:17.764: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 07/29/23 13:20:17.765
  W0729 13:20:17.774837      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 29 13:20:17.776: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 07/29/23 13:20:17.776
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 0
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:17.778: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:17.786: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:17.786: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:17.804: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:17.804: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:17.824: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1
  Jul 29 13:20:17.824: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1
  Jul 29 13:20:17.844: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1
  Jul 29 13:20:17.844: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1
  E0729 13:20:18.351011      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:20:18.715: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:18.715: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:18.735: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1
  STEP: listing Deployments @ 07/29/23 13:20:18.735
  Jul 29 13:20:18.739: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 07/29/23 13:20:18.739
  Jul 29 13:20:18.750: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 07/29/23 13:20:18.75
  Jul 29 13:20:18.757: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 13:20:18.762: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 13:20:18.779: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 13:20:18.805: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 13:20:18.818: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0729 13:20:19.351416      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:20:19.751: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 13:20:19.868: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 13:20:19.883: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 13:20:19.898: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 13:20:19.905: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0729 13:20:20.351990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:20:20.785: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 07/29/23 13:20:20.808
  STEP: fetching the DeploymentStatus @ 07/29/23 13:20:20.814
  Jul 29 13:20:20.820: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1
  Jul 29 13:20:20.820: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1
  Jul 29 13:20:20.820: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1
  Jul 29 13:20:20.820: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1
  Jul 29 13:20:20.821: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 1
  Jul 29 13:20:20.821: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:20.821: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:20.821: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:20.821: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:20.821: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 2
  Jul 29 13:20:20.822: INFO: observed Deployment test-deployment in namespace deployment-8506 with ReadyReplicas 3
  STEP: deleting the Deployment @ 07/29/23 13:20:20.822
  Jul 29 13:20:20.831: INFO: observed event type MODIFIED
  Jul 29 13:20:20.831: INFO: observed event type MODIFIED
  Jul 29 13:20:20.831: INFO: observed event type MODIFIED
  Jul 29 13:20:20.831: INFO: observed event type MODIFIED
  Jul 29 13:20:20.831: INFO: observed event type MODIFIED
  Jul 29 13:20:20.831: INFO: observed event type MODIFIED
  Jul 29 13:20:20.831: INFO: observed event type MODIFIED
  Jul 29 13:20:20.831: INFO: observed event type MODIFIED
  Jul 29 13:20:20.831: INFO: observed event type MODIFIED
  Jul 29 13:20:20.831: INFO: observed event type MODIFIED
  Jul 29 13:20:20.831: INFO: observed event type MODIFIED
  Jul 29 13:20:20.831: INFO: observed event type MODIFIED
  Jul 29 13:20:20.834: INFO: Log out all the ReplicaSets if there is no deployment created
  Jul 29 13:20:20.839: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-8506  857c1524-c137-4a6f-86e9-c6e21483f70c 31906 3 2023-07-29 13:20:16 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 199b992a-2476-408e-bed1-2b06caacf8cc 0xc003cc54e7 0xc003cc54e8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 13:20:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"199b992a-2476-408e-bed1-2b06caacf8cc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 13:20:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cc5570 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jul 29 13:20:20.843: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-8506  0adc2001-550f-47fd-a99e-a1351ace8884 32000 4 2023-07-29 13:20:17 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 199b992a-2476-408e-bed1-2b06caacf8cc 0xc003cc55d7 0xc003cc55d8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 13:20:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"199b992a-2476-408e-bed1-2b06caacf8cc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 13:20:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cc5660 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jul 29 13:20:20.846: INFO: pod: "test-deployment-5b5dcbcd95-mwhsp":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-mwhsp test-deployment-5b5dcbcd95- deployment-8506  6c46fa61-3f28-4cb2-a065-e6ef71981ce8 31993 0 2023-07-29 13:20:17 +0000 UTC 2023-07-29 13:20:21 +0000 UTC 0xc004072918 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 0adc2001-550f-47fd-a99e-a1351ace8884 0xc004072947 0xc004072948}] [] [{kube-controller-manager Update v1 2023-07-29 13:20:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0adc2001-550f-47fd-a99e-a1351ace8884\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 13:20:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.129.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2csh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2csh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:192.168.129.114,StartTime:2023-07-29 13:20:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 13:20:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://5b2cdd6e708fa1c715daa281b7435fd8cf9f5c97aaf9a5ba439b1ccc5e09cc6f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.129.114,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul 29 13:20:20.847: INFO: pod: "test-deployment-5b5dcbcd95-qr6rr":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-qr6rr test-deployment-5b5dcbcd95- deployment-8506  8f18e64b-85ec-42ce-8716-ac5b83449582 31963 0 2023-07-29 13:20:18 +0000 UTC 2023-07-29 13:20:20 +0000 UTC 0xc004072b10 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 0adc2001-550f-47fd-a99e-a1351ace8884 0xc004072b47 0xc004072b48}] [] [{kube-controller-manager Update v1 2023-07-29 13:20:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0adc2001-550f-47fd-a99e-a1351ace8884\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 13:20:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.10.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6cs24,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6cs24,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:192.168.10.17,StartTime:2023-07-29 13:20:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 13:20:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://6c88f85878336062a84ff8bb9aacb12c873cbe39baba8621db772ab52280ded4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.10.17,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul 29 13:20:20.847: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-8506  2a82a255-e071-45f0-aed8-d15c44d1c9d9 31991 2 2023-07-29 13:20:18 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 199b992a-2476-408e-bed1-2b06caacf8cc 0xc003cc56c7 0xc003cc56c8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 13:20:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"199b992a-2476-408e-bed1-2b06caacf8cc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 13:20:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cc5750 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Jul 29 13:20:20.855: INFO: pod: "test-deployment-6fc78d85c6-6hhc9":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-6hhc9 test-deployment-6fc78d85c6- deployment-8506  8f92827d-12f4-428d-8d40-acb29de107cb 31990 0 2023-07-29 13:20:19 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 2a82a255-e071-45f0-aed8-d15c44d1c9d9 0xc003cc5c57 0xc003cc5c58}] [] [{kube-controller-manager Update v1 2023-07-29 13:20:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a82a255-e071-45f0-aed8-d15c44d1c9d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 13:20:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.10.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m52th,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m52th,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-19-67,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.19.67,PodIP:192.168.10.35,StartTime:2023-07-29 13:20:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 13:20:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ba01734c6425577e1a2d491169e2cc24e30df454543b0ae256f120939107df51,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.10.35,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul 29 13:20:20.855: INFO: pod: "test-deployment-6fc78d85c6-gxwct":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-gxwct test-deployment-6fc78d85c6- deployment-8506  4e83f946-b319-4d03-93e9-d4f1c4af210f 32009 0 2023-07-29 13:20:18 +0000 UTC 2023-07-29 13:20:21 +0000 UTC 0xc003cc5e20 map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 2a82a255-e071-45f0-aed8-d15c44d1c9d9 0xc003cc5e57 0xc003cc5e58}] [] [{kube-controller-manager Update v1 2023-07-29 13:20:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a82a255-e071-45f0-aed8-d15c44d1c9d9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 13:20:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.129.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6r6dx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6r6dx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-33-37,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 13:20:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.33.37,PodIP:192.168.129.94,StartTime:2023-07-29 13:20:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 13:20:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f2233da1d22182d8defd9d5b92d48c29be6e21b7716fe9c74d26cf1a5167b059,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.129.94,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul 29 13:20:20.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8506" for this suite. @ 07/29/23 13:20:20.862
• [4.869 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 07/29/23 13:20:20.869
  Jul 29 13:20:20.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 13:20:20.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:20:20.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:20:20.894
  STEP: Setting up server cert @ 07/29/23 13:20:20.92
  E0729 13:20:21.361152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 13:20:21.722
  STEP: Deploying the webhook pod @ 07/29/23 13:20:21.729
  STEP: Wait for the deployment to be ready @ 07/29/23 13:20:21.74
  Jul 29 13:20:21.748: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 13:20:22.355143      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:23.355235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 13:20:23.759
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 13:20:23.771
  E0729 13:20:24.356016      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:20:24.771: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 07/29/23 13:20:24.775
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 07/29/23 13:20:24.776
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 07/29/23 13:20:24.776
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 07/29/23 13:20:24.776
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 07/29/23 13:20:24.777
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/29/23 13:20:24.777
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/29/23 13:20:24.778
  Jul 29 13:20:24.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1945" for this suite. @ 07/29/23 13:20:24.819
  STEP: Destroying namespace "webhook-markers-8529" for this suite. @ 07/29/23 13:20:24.828
• [3.967 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 07/29/23 13:20:24.837
  Jul 29 13:20:24.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 13:20:24.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:20:24.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:20:24.859
  STEP: creating a replication controller @ 07/29/23 13:20:24.862
  Jul 29 13:20:24.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 create -f -'
  Jul 29 13:20:25.187: INFO: stderr: ""
  Jul 29 13:20:25.187: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/29/23 13:20:25.187
  Jul 29 13:20:25.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 13:20:25.255: INFO: stderr: ""
  Jul 29 13:20:25.255: INFO: stdout: "update-demo-nautilus-g7gps update-demo-nautilus-lcqbf "
  Jul 29 13:20:25.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 get pods update-demo-nautilus-g7gps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 13:20:25.316: INFO: stderr: ""
  Jul 29 13:20:25.316: INFO: stdout: ""
  Jul 29 13:20:25.316: INFO: update-demo-nautilus-g7gps is created but not running
  E0729 13:20:25.356681      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:26.357401      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:27.357488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:28.357711      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:29.357794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:20:30.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0729 13:20:30.358176      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:20:30.380: INFO: stderr: ""
  Jul 29 13:20:30.380: INFO: stdout: "update-demo-nautilus-g7gps update-demo-nautilus-lcqbf "
  Jul 29 13:20:30.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 get pods update-demo-nautilus-g7gps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 13:20:30.442: INFO: stderr: ""
  Jul 29 13:20:30.442: INFO: stdout: "true"
  Jul 29 13:20:30.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 get pods update-demo-nautilus-g7gps -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 13:20:30.502: INFO: stderr: ""
  Jul 29 13:20:30.502: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 13:20:30.502: INFO: validating pod update-demo-nautilus-g7gps
  Jul 29 13:20:30.506: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 13:20:30.507: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 13:20:30.507: INFO: update-demo-nautilus-g7gps is verified up and running
  Jul 29 13:20:30.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 get pods update-demo-nautilus-lcqbf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 13:20:30.569: INFO: stderr: ""
  Jul 29 13:20:30.569: INFO: stdout: ""
  Jul 29 13:20:30.569: INFO: update-demo-nautilus-lcqbf is created but not running
  E0729 13:20:31.359095      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:32.359214      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:33.359291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:34.360018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:35.360109      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:20:35.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 13:20:35.638: INFO: stderr: ""
  Jul 29 13:20:35.638: INFO: stdout: "update-demo-nautilus-g7gps update-demo-nautilus-lcqbf "
  Jul 29 13:20:35.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 get pods update-demo-nautilus-g7gps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 13:20:35.700: INFO: stderr: ""
  Jul 29 13:20:35.700: INFO: stdout: "true"
  Jul 29 13:20:35.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 get pods update-demo-nautilus-g7gps -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 13:20:35.760: INFO: stderr: ""
  Jul 29 13:20:35.760: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 13:20:35.760: INFO: validating pod update-demo-nautilus-g7gps
  Jul 29 13:20:35.764: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 13:20:35.764: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 13:20:35.764: INFO: update-demo-nautilus-g7gps is verified up and running
  Jul 29 13:20:35.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 get pods update-demo-nautilus-lcqbf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 13:20:35.827: INFO: stderr: ""
  Jul 29 13:20:35.827: INFO: stdout: "true"
  Jul 29 13:20:35.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 get pods update-demo-nautilus-lcqbf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 13:20:35.887: INFO: stderr: ""
  Jul 29 13:20:35.887: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 13:20:35.887: INFO: validating pod update-demo-nautilus-lcqbf
  Jul 29 13:20:35.894: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 13:20:35.894: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 13:20:35.894: INFO: update-demo-nautilus-lcqbf is verified up and running
  STEP: using delete to clean up resources @ 07/29/23 13:20:35.894
  Jul 29 13:20:35.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 delete --grace-period=0 --force -f -'
  Jul 29 13:20:35.959: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 13:20:35.959: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul 29 13:20:35.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 get rc,svc -l name=update-demo --no-headers'
  Jul 29 13:20:36.049: INFO: stderr: "No resources found in kubectl-7158 namespace.\n"
  Jul 29 13:20:36.049: INFO: stdout: ""
  Jul 29 13:20:36.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-7158 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 29 13:20:36.114: INFO: stderr: ""
  Jul 29 13:20:36.114: INFO: stdout: ""
  Jul 29 13:20:36.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7158" for this suite. @ 07/29/23 13:20:36.118
• [11.287 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 07/29/23 13:20:36.124
  Jul 29 13:20:36.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/29/23 13:20:36.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:20:36.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:20:36.144
  STEP: create the container to handle the HTTPGet hook request. @ 07/29/23 13:20:36.151
  E0729 13:20:36.360704      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:37.360821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:38.361336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:39.361405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 07/29/23 13:20:40.176
  E0729 13:20:40.362241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:41.363229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 07/29/23 13:20:42.19
  STEP: delete the pod with lifecycle hook @ 07/29/23 13:20:42.21
  E0729 13:20:42.363736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:43.363844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:20:44.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6903" for this suite. @ 07/29/23 13:20:44.233
• [8.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 07/29/23 13:20:44.242
  Jul 29 13:20:44.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 13:20:44.243
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:20:44.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:20:44.26
  STEP: Creating pod liveness-32f65f78-1ac8-4082-98c9-964d1db9ed40 in namespace container-probe-4121 @ 07/29/23 13:20:44.263
  E0729 13:20:44.364152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:45.364261      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:20:46.278: INFO: Started pod liveness-32f65f78-1ac8-4082-98c9-964d1db9ed40 in namespace container-probe-4121
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 13:20:46.278
  Jul 29 13:20:46.281: INFO: Initial restart count of pod liveness-32f65f78-1ac8-4082-98c9-964d1db9ed40 is 0
  E0729 13:20:46.364657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:47.365387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:48.365913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:49.366196      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:50.366775      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:51.366901      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:52.367863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:53.367957      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:54.367985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:55.368539      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:56.369497      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:57.369675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:58.369942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:20:59.370042      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:00.370316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:01.370404      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:02.371023      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:03.371970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:04.372510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:05.372677      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:06.329: INFO: Restart count of pod container-probe-4121/liveness-32f65f78-1ac8-4082-98c9-964d1db9ed40 is now 1 (20.048382139s elapsed)
  Jul 29 13:21:06.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 13:21:06.334
  STEP: Destroying namespace "container-probe-4121" for this suite. @ 07/29/23 13:21:06.353
• [22.118 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 07/29/23 13:21:06.361
  Jul 29 13:21:06.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename crd-webhook @ 07/29/23 13:21:06.361
  E0729 13:21:06.373457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:06.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:06.38
  STEP: Setting up server cert @ 07/29/23 13:21:06.382
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/29/23 13:21:06.903
  STEP: Deploying the custom resource conversion webhook pod @ 07/29/23 13:21:06.909
  STEP: Wait for the deployment to be ready @ 07/29/23 13:21:06.921
  Jul 29 13:21:06.928: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0729 13:21:07.374383      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:08.374536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 13:21:08.939
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 13:21:08.948
  E0729 13:21:09.374898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:09.948: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul 29 13:21:09.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  E0729 13:21:10.375608      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:11.375790      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:12.375881      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 07/29/23 13:21:12.522
  STEP: Create a v2 custom resource @ 07/29/23 13:21:12.538
  STEP: List CRs in v1 @ 07/29/23 13:21:12.582
  STEP: List CRs in v2 @ 07/29/23 13:21:12.587
  Jul 29 13:21:12.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-7455" for this suite. @ 07/29/23 13:21:13.144
• [6.790 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 07/29/23 13:21:13.151
  Jul 29 13:21:13.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 13:21:13.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:13.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:13.174
  STEP: fetching services @ 07/29/23 13:21:13.176
  Jul 29 13:21:13.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8446" for this suite. @ 07/29/23 13:21:13.184
• [0.039 seconds]
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 07/29/23 13:21:13.191
  Jul 29 13:21:13.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename proxy @ 07/29/23 13:21:13.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:13.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:13.21
  Jul 29 13:21:13.213: INFO: Creating pod...
  E0729 13:21:13.376433      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:14.376763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:15.226: INFO: Creating service...
  Jul 29 13:21:15.234: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/pods/agnhost/proxy?method=DELETE
  Jul 29 13:21:15.241: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 29 13:21:15.241: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/pods/agnhost/proxy?method=OPTIONS
  Jul 29 13:21:15.247: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 29 13:21:15.247: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/pods/agnhost/proxy?method=PATCH
  Jul 29 13:21:15.251: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 29 13:21:15.251: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/pods/agnhost/proxy?method=POST
  Jul 29 13:21:15.255: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 29 13:21:15.255: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/pods/agnhost/proxy?method=PUT
  Jul 29 13:21:15.260: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 29 13:21:15.260: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/services/e2e-proxy-test-service/proxy?method=DELETE
  Jul 29 13:21:15.267: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 29 13:21:15.267: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jul 29 13:21:15.272: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 29 13:21:15.272: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/services/e2e-proxy-test-service/proxy?method=PATCH
  Jul 29 13:21:15.278: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 29 13:21:15.278: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/services/e2e-proxy-test-service/proxy?method=POST
  Jul 29 13:21:15.284: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 29 13:21:15.284: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/services/e2e-proxy-test-service/proxy?method=PUT
  Jul 29 13:21:15.289: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 29 13:21:15.290: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/pods/agnhost/proxy?method=GET
  Jul 29 13:21:15.294: INFO: http.Client request:GET StatusCode:301
  Jul 29 13:21:15.294: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/services/e2e-proxy-test-service/proxy?method=GET
  Jul 29 13:21:15.297: INFO: http.Client request:GET StatusCode:301
  Jul 29 13:21:15.297: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/pods/agnhost/proxy?method=HEAD
  Jul 29 13:21:15.300: INFO: http.Client request:HEAD StatusCode:301
  Jul 29 13:21:15.300: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2599/services/e2e-proxy-test-service/proxy?method=HEAD
  Jul 29 13:21:15.305: INFO: http.Client request:HEAD StatusCode:301
  Jul 29 13:21:15.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-2599" for this suite. @ 07/29/23 13:21:15.308
• [2.124 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 07/29/23 13:21:15.316
  Jul 29 13:21:15.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename podtemplate @ 07/29/23 13:21:15.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:15.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:15.337
  STEP: Create a pod template @ 07/29/23 13:21:15.339
  STEP: Replace a pod template @ 07/29/23 13:21:15.348
  Jul 29 13:21:15.355: INFO: Found updated podtemplate annotation: "true"

  Jul 29 13:21:15.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-7439" for this suite. @ 07/29/23 13:21:15.358
• [0.050 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 07/29/23 13:21:15.367
  Jul 29 13:21:15.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename gc @ 07/29/23 13:21:15.368
  E0729 13:21:15.377172      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:15.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:15.386
  Jul 29 13:21:15.417: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"eadc0709-d004-46ba-a761-5a506e511f41", Controller:(*bool)(0xc00343af76), BlockOwnerDeletion:(*bool)(0xc00343af77)}}
  Jul 29 13:21:15.426: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4b3cbed1-1f36-415f-8431-05c9909607a5", Controller:(*bool)(0xc0035bd81e), BlockOwnerDeletion:(*bool)(0xc0035bd81f)}}
  Jul 29 13:21:15.433: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"05d63f27-b88d-46d9-8186-1c7ee6d20049", Controller:(*bool)(0xc0035bda2a), BlockOwnerDeletion:(*bool)(0xc0035bda2b)}}
  E0729 13:21:16.377914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:17.378009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:18.378091      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:19.378261      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:20.378394      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:20.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3118" for this suite. @ 07/29/23 13:21:20.453
• [5.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 07/29/23 13:21:20.461
  Jul 29 13:21:20.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 13:21:20.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:20.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:20.485
  STEP: Creating the pod @ 07/29/23 13:21:20.488
  E0729 13:21:21.379318      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:22.379974      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:23.034: INFO: Successfully updated pod "labelsupdate0f8d999b-28a9-475a-bf5c-9a299bd5582d"
  E0729 13:21:23.380628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:24.380829      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:25.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5702" for this suite. @ 07/29/23 13:21:25.065
• [4.611 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 07/29/23 13:21:25.076
  Jul 29 13:21:25.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename init-container @ 07/29/23 13:21:25.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:25.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:25.101
  STEP: creating the pod @ 07/29/23 13:21:25.107
  Jul 29 13:21:25.108: INFO: PodSpec: initContainers in spec.initContainers
  E0729 13:21:25.381807      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:26.382096      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:27.382177      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:28.382488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:28.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8478" for this suite. @ 07/29/23 13:21:28.859
• [3.790 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 07/29/23 13:21:28.867
  Jul 29 13:21:28.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 13:21:28.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:28.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:28.887
  STEP: Creating configMap with name configmap-test-upd-294d8bd0-31e2-4599-9144-d01bbc9940c1 @ 07/29/23 13:21:28.894
  STEP: Creating the pod @ 07/29/23 13:21:28.899
  E0729 13:21:29.382627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:30.382926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 07/29/23 13:21:30.915
  STEP: Waiting for pod with binary data @ 07/29/23 13:21:30.931
  Jul 29 13:21:30.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6592" for this suite. @ 07/29/23 13:21:30.941
• [2.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 07/29/23 13:21:30.949
  Jul 29 13:21:30.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename watch @ 07/29/23 13:21:30.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:30.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:30.97
  STEP: getting a starting resourceVersion @ 07/29/23 13:21:30.973
  STEP: starting a background goroutine to produce watch events @ 07/29/23 13:21:30.975
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 07/29/23 13:21:30.975
  E0729 13:21:31.383732      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:32.384174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:33.384845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:33.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2376" for this suite. @ 07/29/23 13:21:33.804
• [2.908 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 07/29/23 13:21:33.858
  Jul 29 13:21:33.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 13:21:33.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:33.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:33.88
  STEP: Setting up server cert @ 07/29/23 13:21:33.905
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 13:21:34.334
  STEP: Deploying the webhook pod @ 07/29/23 13:21:34.339
  STEP: Wait for the deployment to be ready @ 07/29/23 13:21:34.351
  Jul 29 13:21:34.357: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 13:21:34.385848      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:35.386115      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:36.367: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 13, 21, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 13, 21, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 13, 21, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 13, 21, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0729 13:21:36.386874      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:37.386921      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 13:21:38.371
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 13:21:38.38
  E0729 13:21:38.387679      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:39.380: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/29/23 13:21:39.383
  E0729 13:21:39.388059      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a pod @ 07/29/23 13:21:39.399
  E0729 13:21:40.388190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:41.388368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 07/29/23 13:21:41.414
  Jul 29 13:21:41.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=webhook-6855 attach --namespace=webhook-6855 to-be-attached-pod -i -c=container1'
  Jul 29 13:21:41.487: INFO: rc: 1
  Jul 29 13:21:41.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6855" for this suite. @ 07/29/23 13:21:41.532
  STEP: Destroying namespace "webhook-markers-9967" for this suite. @ 07/29/23 13:21:41.538
• [7.689 seconds]
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 07/29/23 13:21:41.547
  Jul 29 13:21:41.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 13:21:41.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:41.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:41.565
  STEP: creating service in namespace services-8840 @ 07/29/23 13:21:41.568
  STEP: creating service affinity-clusterip-transition in namespace services-8840 @ 07/29/23 13:21:41.568
  STEP: creating replication controller affinity-clusterip-transition in namespace services-8840 @ 07/29/23 13:21:41.582
  I0729 13:21:41.589903      18 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-8840, replica count: 3
  E0729 13:21:42.389406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:43.390091      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:44.390196      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0729 13:21:44.641632      18 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 13:21:44.649: INFO: Creating new exec pod
  E0729 13:21:45.391049      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:46.391457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:47.391536      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:47.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-8840 exec execpod-affinity2h5dg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jul 29 13:21:47.797: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jul 29 13:21:47.797: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 13:21:47.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-8840 exec execpod-affinity2h5dg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.165 80'
  Jul 29 13:21:47.920: INFO: stderr: "+ nc -v -t -w 2 10.152.183.165 80\n+ echo hostName\nConnection to 10.152.183.165 80 port [tcp/http] succeeded!\n"
  Jul 29 13:21:47.920: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 13:21:47.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-8840 exec execpod-affinity2h5dg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.165:80/ ; done'
  Jul 29 13:21:48.127: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n"
  Jul 29 13:21:48.127: INFO: stdout: "\naffinity-clusterip-transition-frfxv\naffinity-clusterip-transition-s56jh\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-frfxv\naffinity-clusterip-transition-frfxv\naffinity-clusterip-transition-frfxv\naffinity-clusterip-transition-frfxv\naffinity-clusterip-transition-frfxv\naffinity-clusterip-transition-s56jh\naffinity-clusterip-transition-frfxv\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-s56jh\naffinity-clusterip-transition-frfxv\naffinity-clusterip-transition-s56jh"
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-frfxv
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-s56jh
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-frfxv
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-frfxv
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-frfxv
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-frfxv
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-frfxv
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-s56jh
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-frfxv
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-s56jh
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-frfxv
  Jul 29 13:21:48.127: INFO: Received response from host: affinity-clusterip-transition-s56jh
  Jul 29 13:21:48.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-8840 exec execpod-affinity2h5dg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.165:80/ ; done'
  Jul 29 13:21:48.340: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.165:80/\n"
  Jul 29 13:21:48.340: INFO: stdout: "\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl\naffinity-clusterip-transition-jg2zl"
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Received response from host: affinity-clusterip-transition-jg2zl
  Jul 29 13:21:48.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 13:21:48.344: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8840, will wait for the garbage collector to delete the pods @ 07/29/23 13:21:48.355
  E0729 13:21:48.392387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:48.415: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.685794ms
  Jul 29 13:21:48.516: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.990025ms
  E0729 13:21:49.393211      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-8840" for this suite. @ 07/29/23 13:21:50.336
• [8.796 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 07/29/23 13:21:50.343
  Jul 29 13:21:50.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename job @ 07/29/23 13:21:50.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:50.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:50.363
  STEP: Creating a job @ 07/29/23 13:21:50.366
  STEP: Ensuring active pods == parallelism @ 07/29/23 13:21:50.371
  E0729 13:21:50.393530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:51.394538      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 07/29/23 13:21:52.376
  E0729 13:21:52.394961      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:52.891: INFO: Successfully updated pod "adopt-release-8g5sk"
  STEP: Checking that the Job readopts the Pod @ 07/29/23 13:21:52.891
  E0729 13:21:53.395563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:54.396080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 07/29/23 13:21:54.898
  E0729 13:21:55.397093      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:55.409: INFO: Successfully updated pod "adopt-release-8g5sk"
  STEP: Checking that the Job releases the Pod @ 07/29/23 13:21:55.409
  E0729 13:21:56.397295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:57.398241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:21:57.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5754" for this suite. @ 07/29/23 13:21:57.42
• [7.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 07/29/23 13:21:57.427
  Jul 29 13:21:57.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename tables @ 07/29/23 13:21:57.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:57.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:57.453
  Jul 29 13:21:57.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-5652" for this suite. @ 07/29/23 13:21:57.462
• [0.045 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 07/29/23 13:21:57.473
  Jul 29 13:21:57.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:21:57.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:21:57.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:21:57.493
  STEP: Creating projection with secret that has name projected-secret-test-0326c517-3f68-4a05-9b96-434a592ca6ba @ 07/29/23 13:21:57.496
  STEP: Creating a pod to test consume secrets @ 07/29/23 13:21:57.502
  E0729 13:21:58.398933      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:21:59.399024      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:00.399993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:01.400358      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:22:01.523
  Jul 29 13:22:01.525: INFO: Trying to get logs from node ip-172-31-19-67 pod pod-projected-secrets-06b18499-72e2-4ec3-b63f-a17052ba6c9a container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 13:22:01.532
  Jul 29 13:22:01.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-439" for this suite. @ 07/29/23 13:22:01.555
• [4.087 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 07/29/23 13:22:01.561
  Jul 29 13:22:01.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 07/29/23 13:22:01.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:22:01.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:22:01.582
  STEP: Setting up the test @ 07/29/23 13:22:01.585
  STEP: Creating hostNetwork=false pod @ 07/29/23 13:22:01.585
  E0729 13:22:02.401327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:03.401589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 07/29/23 13:22:03.608
  E0729 13:22:04.401710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:05.401918      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 07/29/23 13:22:05.624
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 07/29/23 13:22:05.624
  Jul 29 13:22:05.624: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-330 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:22:05.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:22:05.625: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:22:05.625: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-330/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 29 13:22:05.683: INFO: Exec stderr: ""
  Jul 29 13:22:05.683: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-330 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:22:05.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:22:05.683: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:22:05.683: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-330/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 29 13:22:05.742: INFO: Exec stderr: ""
  Jul 29 13:22:05.742: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-330 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:22:05.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:22:05.743: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:22:05.743: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-330/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 29 13:22:05.798: INFO: Exec stderr: ""
  Jul 29 13:22:05.799: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-330 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:22:05.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:22:05.799: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:22:05.799: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-330/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 29 13:22:05.854: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 07/29/23 13:22:05.854
  Jul 29 13:22:05.854: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-330 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:22:05.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:22:05.855: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:22:05.855: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-330/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul 29 13:22:05.905: INFO: Exec stderr: ""
  Jul 29 13:22:05.906: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-330 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:22:05.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:22:05.906: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:22:05.906: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-330/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul 29 13:22:05.991: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 07/29/23 13:22:05.991
  Jul 29 13:22:05.991: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-330 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:22:05.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:22:05.992: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:22:05.992: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-330/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 29 13:22:06.048: INFO: Exec stderr: ""
  Jul 29 13:22:06.048: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-330 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:22:06.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:22:06.049: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:22:06.049: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-330/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 29 13:22:06.107: INFO: Exec stderr: ""
  Jul 29 13:22:06.108: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-330 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:22:06.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:22:06.108: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:22:06.108: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-330/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 29 13:22:06.164: INFO: Exec stderr: ""
  Jul 29 13:22:06.164: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-330 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 13:22:06.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:22:06.165: INFO: ExecWithOptions: Clientset creation
  Jul 29 13:22:06.165: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-330/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 29 13:22:06.225: INFO: Exec stderr: ""
  Jul 29 13:22:06.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-330" for this suite. @ 07/29/23 13:22:06.229
• [4.674 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 07/29/23 13:22:06.237
  Jul 29 13:22:06.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pods @ 07/29/23 13:22:06.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:22:06.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:22:06.256
  STEP: Create set of pods @ 07/29/23 13:22:06.26
  Jul 29 13:22:06.266: INFO: created test-pod-1
  Jul 29 13:22:06.271: INFO: created test-pod-2
  Jul 29 13:22:06.278: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 07/29/23 13:22:06.278
  E0729 13:22:06.402700      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:07.402926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 07/29/23 13:22:08.32
  Jul 29 13:22:08.326: INFO: Pod quantity 3 is different from expected quantity 0
  E0729 13:22:08.403028      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:22:09.331: INFO: Pod quantity 2 is different from expected quantity 0
  E0729 13:22:09.403266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:22:10.332: INFO: Pod quantity 1 is different from expected quantity 0
  E0729 13:22:10.403805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:22:11.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5767" for this suite. @ 07/29/23 13:22:11.335
• [5.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 07/29/23 13:22:11.343
  Jul 29 13:22:11.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 13:22:11.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:22:11.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:22:11.365
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 13:22:11.368
  E0729 13:22:11.404672      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:12.404692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:13.405312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:14.405404      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:22:15.393
  Jul 29 13:22:15.396: INFO: Trying to get logs from node ip-172-31-19-67 pod downwardapi-volume-b1468536-c7ca-4a5b-bb3e-cd771b559229 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 13:22:15.403
  E0729 13:22:15.405645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:22:15.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1227" for this suite. @ 07/29/23 13:22:15.421
• [4.085 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 07/29/23 13:22:15.429
  Jul 29 13:22:15.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 13:22:15.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:22:15.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:22:15.447
  STEP: Creating pod test-grpc-e3879598-5f69-4697-9e54-2a6862d2fb39 in namespace container-probe-3077 @ 07/29/23 13:22:15.45
  E0729 13:22:16.406543      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:17.407072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:22:17.466: INFO: Started pod test-grpc-e3879598-5f69-4697-9e54-2a6862d2fb39 in namespace container-probe-3077
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 13:22:17.466
  Jul 29 13:22:17.470: INFO: Initial restart count of pod test-grpc-e3879598-5f69-4697-9e54-2a6862d2fb39 is 0
  E0729 13:22:18.407201      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:19.407290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:20.407371      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:21.407594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:22.408404      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:23.408583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:24.408677      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:25.408770      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:26.409736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:27.409833      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:28.409906      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:29.410103      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:30.410214      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:31.410422      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:32.410506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:33.410599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:34.410928      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:35.411003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:36.411801      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:37.412119      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:38.412224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:39.412322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:40.413147      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:41.414137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:42.414163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:43.414245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:44.414765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:45.414907      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:46.415603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:47.415684      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:48.415774      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:49.415965      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:50.416062      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:51.416149      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:52.416275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:53.416366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:54.416481      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:55.417275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:56.417382      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:57.417569      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:58.417661      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:22:59.418183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:00.418930      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:01.419086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:02.419189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:03.419969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:04.420570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:05.421095      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:06.422024      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:07.422227      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:08.422317      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:09.423185      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:10.423269      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:11.423944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:12.424053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:13.424577      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:14.424667      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:15.424850      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:16.425442      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:17.425530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:18.425579      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:19.426228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:20.426933      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:21.427030      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:22.427127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:23.427230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:24.427305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:25.427386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:26.428324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:27.428638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:28.428738      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:29.428808      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:30.428916      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:31.429215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:32.429314      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:33.429494      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:23:33.653: INFO: Restart count of pod container-probe-3077/test-grpc-e3879598-5f69-4697-9e54-2a6862d2fb39 is now 1 (1m16.183004099s elapsed)
  Jul 29 13:23:33.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 13:23:33.657
  STEP: Destroying namespace "container-probe-3077" for this suite. @ 07/29/23 13:23:33.671
• [78.247 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 07/29/23 13:23:33.679
  Jul 29 13:23:33.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/29/23 13:23:33.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:23:33.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:23:33.698
  STEP: creating @ 07/29/23 13:23:33.701
  STEP: getting @ 07/29/23 13:23:33.72
  STEP: listing in namespace @ 07/29/23 13:23:33.725
  STEP: patching @ 07/29/23 13:23:33.728
  STEP: deleting @ 07/29/23 13:23:33.736
  Jul 29 13:23:33.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-9416" for this suite. @ 07/29/23 13:23:33.755
• [0.081 seconds]
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 07/29/23 13:23:33.76
  Jul 29 13:23:33.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 13:23:33.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:23:33.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:23:33.782
  STEP: Creating a pod to test emptydir volume type on node default medium @ 07/29/23 13:23:33.79
  E0729 13:23:34.429821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:35.430105      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:36.430424      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:37.430514      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:23:37.809
  Jul 29 13:23:37.816: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-5701b784-67f8-446a-be68-d8c9635f91d2 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 13:23:37.83
  Jul 29 13:23:37.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7792" for this suite. @ 07/29/23 13:23:37.848
• [4.094 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 07/29/23 13:23:37.855
  Jul 29 13:23:37.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 13:23:37.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:23:37.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:23:37.878
  STEP: create the container @ 07/29/23 13:23:37.882
  W0729 13:23:37.891354      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/29/23 13:23:37.891
  E0729 13:23:38.431589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:39.431673      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:40.431756      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/29/23 13:23:40.934
  STEP: the container should be terminated @ 07/29/23 13:23:40.937
  STEP: the termination message should be set @ 07/29/23 13:23:40.937
  Jul 29 13:23:40.937: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 07/29/23 13:23:40.937
  Jul 29 13:23:40.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4448" for this suite. @ 07/29/23 13:23:40.955
• [3.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 07/29/23 13:23:40.962
  Jul 29 13:23:40.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 13:23:40.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:23:40.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:23:40.983
  STEP: Creating service test in namespace statefulset-3346 @ 07/29/23 13:23:40.986
  STEP: Creating a new StatefulSet @ 07/29/23 13:23:40.99
  Jul 29 13:23:41.000: INFO: Found 0 stateful pods, waiting for 3
  E0729 13:23:41.432032      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:42.432931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:43.432946      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:44.433130      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:45.433348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:46.433404      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:47.433563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:48.433763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:49.433932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:50.434090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:23:51.004: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 13:23:51.004: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 13:23:51.004: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 13:23:51.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-3346 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 13:23:51.138: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 13:23:51.138: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 13:23:51.138: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0729 13:23:51.434199      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:52.434360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:53.434534      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:54.434642      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:55.434773      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:56.435226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:57.435324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:58.435965      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:23:59.436056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:00.436625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/29/23 13:24:01.15
  Jul 29 13:24:01.171: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/29/23 13:24:01.171
  E0729 13:24:01.436994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:02.437221      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:03.437284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:04.437379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:05.437569      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:06.437761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:07.438216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:08.438418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:09.438607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:10.438715      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 07/29/23 13:24:11.186
  Jul 29 13:24:11.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-3346 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 13:24:11.316: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 13:24:11.316: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 13:24:11.316: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0729 13:24:11.438845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:12.438925      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:13.439026      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:14.439103      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:15.439972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:16.440391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:17.440568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:18.441414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:19.441499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:20.441683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 07/29/23 13:24:21.339
  Jul 29 13:24:21.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-3346 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0729 13:24:21.442076      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:24:21.464: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 13:24:21.464: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 13:24:21.464: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0729 13:24:22.442631      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:23.442912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:24.442997      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:25.443105      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:26.443488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:27.443587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:28.443687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:29.443806      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:30.443878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:31.444834      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:24:31.498: INFO: Updating stateful set ss2
  E0729 13:24:32.445159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:33.445710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:34.446707      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:35.446811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:36.447319      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:37.447420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:38.447979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:39.449006      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:40.449198      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:41.449287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 07/29/23 13:24:41.514
  Jul 29 13:24:41.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=statefulset-3346 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 13:24:41.642: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 13:24:41.642: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 13:24:41.642: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0729 13:24:42.449702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:43.449799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:44.449878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:45.450056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:46.450466      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:47.450655      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:48.450735      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:49.450946      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:50.451969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:51.452062      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:24:51.663: INFO: Deleting all statefulset in ns statefulset-3346
  Jul 29 13:24:51.666: INFO: Scaling statefulset ss2 to 0
  E0729 13:24:52.452155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:53.452238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:54.452329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:55.452485      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:56.452537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:57.453583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:58.453675      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:24:59.453857      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:00.454321      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:01.454505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:25:01.683: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 13:25:01.686: INFO: Deleting statefulset ss2
  Jul 29 13:25:01.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3346" for this suite. @ 07/29/23 13:25:01.704
• [80.748 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 07/29/23 13:25:01.714
  Jul 29 13:25:01.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename podtemplate @ 07/29/23 13:25:01.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:25:01.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:25:01.743
  STEP: Create set of pod templates @ 07/29/23 13:25:01.746
  Jul 29 13:25:01.751: INFO: created test-podtemplate-1
  Jul 29 13:25:01.757: INFO: created test-podtemplate-2
  Jul 29 13:25:01.760: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 07/29/23 13:25:01.76
  STEP: delete collection of pod templates @ 07/29/23 13:25:01.765
  Jul 29 13:25:01.765: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 07/29/23 13:25:01.783
  Jul 29 13:25:01.783: INFO: requesting list of pod templates to confirm quantity
  Jul 29 13:25:01.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4644" for this suite. @ 07/29/23 13:25:01.788
• [0.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 07/29/23 13:25:01.795
  Jul 29 13:25:01.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 13:25:01.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:25:01.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:25:01.814
  Jul 29 13:25:01.834: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 07/29/23 13:25:01.84
  Jul 29 13:25:01.844: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 13:25:01.844: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 07/29/23 13:25:01.844
  Jul 29 13:25:01.862: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 13:25:01.862: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  E0729 13:25:02.454631      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:25:02.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 13:25:02.866: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  E0729 13:25:03.455457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:25:03.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 29 13:25:03.866: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 07/29/23 13:25:03.868
  Jul 29 13:25:03.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 29 13:25:03.884: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0729 13:25:04.456239      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:25:04.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 13:25:04.889: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 07/29/23 13:25:04.889
  Jul 29 13:25:04.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 13:25:04.901: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  E0729 13:25:05.456411      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:25:05.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 13:25:05.904: INFO: Node ip-172-31-19-67 is running 0 daemon pod, expected 1
  E0729 13:25:06.456806      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:25:06.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 29 13:25:06.906: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 13:25:06.913
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4029, will wait for the garbage collector to delete the pods @ 07/29/23 13:25:06.913
  Jul 29 13:25:06.973: INFO: Deleting DaemonSet.extensions daemon-set took: 6.251323ms
  Jul 29 13:25:07.074: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.023481ms
  E0729 13:25:07.457205      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:08.458033      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:25:08.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 13:25:08.478: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 13:25:08.481: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34636"},"items":null}

  Jul 29 13:25:08.485: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34636"},"items":null}

  Jul 29 13:25:08.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4029" for this suite. @ 07/29/23 13:25:08.513
• [6.725 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 07/29/23 13:25:08.523
  Jul 29 13:25:08.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:25:08.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:25:08.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:25:08.552
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 13:25:08.555
  E0729 13:25:09.458134      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:10.458226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:11.458503      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:12.458597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:25:12.572
  Jul 29 13:25:12.575: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-ac028419-3e24-451d-96a9-b94a44945f3f container client-container: <nil>
  STEP: delete the pod @ 07/29/23 13:25:12.597
  Jul 29 13:25:12.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7189" for this suite. @ 07/29/23 13:25:12.617
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 07/29/23 13:25:12.626
  Jul 29 13:25:12.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 13:25:12.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:25:12.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:25:12.647
  STEP: Creating a cronjob @ 07/29/23 13:25:12.65
  STEP: Ensuring more than one job is running at a time @ 07/29/23 13:25:12.655
  E0729 13:25:13.458882      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:14.458970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:15.459627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:16.459994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:17.460084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:18.460283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:19.460357      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:20.460743      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:21.461531      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:22.462541      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:23.462926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:24.462998      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:25.463110      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:26.463438      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:27.463528      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:28.463987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:29.464083      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:30.464176      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:31.464699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:32.464813      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:33.465305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:34.466060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:35.466166      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:36.466594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:37.466687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:38.466884      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:39.467401      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:40.467707      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:41.467989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:42.468088      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:43.468177      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:44.468457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:45.468564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:46.469488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:47.469586      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:48.469816      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:49.469908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:50.470071      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:51.470578      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:52.470670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:53.471456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:54.471551      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:55.471638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:56.472485      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:57.472569      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:58.472778      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:25:59.473392      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:00.473483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:01.474092      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:02.474185      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:03.474955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:04.475043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:05.475131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:06.475460      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:07.475261      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:08.475969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:09.476060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:10.476160      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:11.477126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:12.477220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:13.478101      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:14.478303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:15.478402      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:16.478459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:17.479336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:18.479429      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:19.479526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:20.479966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:21.480039      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:22.480779      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:23.480871      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:24.481583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:25.481674      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:26.482524      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:27.482901      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:28.483963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:29.485019      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:30.485098      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:31.485659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:32.486341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:33.486901      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:34.487001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:35.487963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:36.488193      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:37.488201      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:38.488487      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:39.489066      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:40.489165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:41.490156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:42.491186      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:43.491275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:44.491366      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:45.491962      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:46.492363      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:47.492650      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:48.492747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:49.492849      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:50.493012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:51.493560      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:52.493755      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:53.493785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:54.494432      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:55.495013      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:56.495103      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:57.495191      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:58.496277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:26:59.496384      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:00.497157      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 07/29/23 13:27:00.659
  STEP: Removing cronjob @ 07/29/23 13:27:00.662
  Jul 29 13:27:00.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1243" for this suite. @ 07/29/23 13:27:00.672
• [108.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 07/29/23 13:27:00.68
  Jul 29 13:27:00.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 13:27:00.681
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:27:00.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:27:00.702
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 07/29/23 13:27:00.717
  E0729 13:27:01.497262      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:02.498226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:03.498360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:04.498908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:05.499016      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:06.499412      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:07.499947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:08.500036      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:09.500128      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:10.500324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:11.500686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:12.501555      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:13.501646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:14.501736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:15.502579      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:16.502919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 07/29/23 13:27:16.795
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 07/29/23 13:27:16.798
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 07/29/23 13:27:16.805
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 07/29/23 13:27:16.805
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 07/29/23 13:27:16.828
  E0729 13:27:17.503012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:18.503103      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:19.503207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 07/29/23 13:27:19.846
  E0729 13:27:20.503990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 07/29/23 13:27:20.852
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 07/29/23 13:27:20.859
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 07/29/23 13:27:20.859
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 07/29/23 13:27:20.88
  E0729 13:27:21.504120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 07/29/23 13:27:21.886
  E0729 13:27:22.504761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:23.504855      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 07/29/23 13:27:23.896
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 07/29/23 13:27:23.906
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 07/29/23 13:27:23.906
  Jul 29 13:27:23.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9367" for this suite. @ 07/29/23 13:27:23.939
• [23.268 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 07/29/23 13:27:23.951
  Jul 29 13:27:23.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename watch @ 07/29/23 13:27:23.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:27:23.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:27:23.975
  STEP: creating a watch on configmaps with label A @ 07/29/23 13:27:23.985
  STEP: creating a watch on configmaps with label B @ 07/29/23 13:27:23.987
  STEP: creating a watch on configmaps with label A or B @ 07/29/23 13:27:23.989
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 07/29/23 13:27:23.99
  Jul 29 13:27:23.996: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5673  8b06b481-a621-4826-ad1b-2a1746f23d74 35092 0 2023-07-29 13:27:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 13:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:27:23.996: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5673  8b06b481-a621-4826-ad1b-2a1746f23d74 35092 0 2023-07-29 13:27:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 13:27:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 07/29/23 13:27:23.996
  Jul 29 13:27:24.005: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5673  8b06b481-a621-4826-ad1b-2a1746f23d74 35093 0 2023-07-29 13:27:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 13:27:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:27:24.006: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5673  8b06b481-a621-4826-ad1b-2a1746f23d74 35093 0 2023-07-29 13:27:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 13:27:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 07/29/23 13:27:24.006
  Jul 29 13:27:24.021: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5673  8b06b481-a621-4826-ad1b-2a1746f23d74 35094 0 2023-07-29 13:27:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 13:27:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:27:24.022: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5673  8b06b481-a621-4826-ad1b-2a1746f23d74 35094 0 2023-07-29 13:27:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 13:27:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 07/29/23 13:27:24.022
  Jul 29 13:27:24.029: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5673  8b06b481-a621-4826-ad1b-2a1746f23d74 35095 0 2023-07-29 13:27:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 13:27:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:27:24.029: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5673  8b06b481-a621-4826-ad1b-2a1746f23d74 35095 0 2023-07-29 13:27:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 13:27:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 07/29/23 13:27:24.03
  Jul 29 13:27:24.035: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5673  78b85885-02f8-4c84-ba26-03656af19784 35096 0 2023-07-29 13:27:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 13:27:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:27:24.035: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5673  78b85885-02f8-4c84-ba26-03656af19784 35096 0 2023-07-29 13:27:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 13:27:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0729 13:27:24.504945      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:25.505175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:26.505501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:27.505572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:28.506456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:29.506617      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:30.506734      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:31.507619      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:32.507973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:33.508525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 07/29/23 13:27:34.035
  Jul 29 13:27:34.043: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5673  78b85885-02f8-4c84-ba26-03656af19784 35150 0 2023-07-29 13:27:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 13:27:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:27:34.043: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5673  78b85885-02f8-4c84-ba26-03656af19784 35150 0 2023-07-29 13:27:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 13:27:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0729 13:27:34.509479      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:35.509554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:36.510534      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:37.510743      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:38.510906      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:39.511002      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:40.511978      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:41.512016      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:42.512717      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:43.512815      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:27:44.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5673" for this suite. @ 07/29/23 13:27:44.049
• [20.105 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 07/29/23 13:27:44.057
  Jul 29 13:27:44.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 13:27:44.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:27:44.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:27:44.076
  STEP: creating a ServiceAccount @ 07/29/23 13:27:44.079
  STEP: watching for the ServiceAccount to be added @ 07/29/23 13:27:44.087
  STEP: patching the ServiceAccount @ 07/29/23 13:27:44.09
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 07/29/23 13:27:44.096
  STEP: deleting the ServiceAccount @ 07/29/23 13:27:44.098
  Jul 29 13:27:44.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6495" for this suite. @ 07/29/23 13:27:44.114
• [0.063 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 07/29/23 13:27:44.121
  Jul 29 13:27:44.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 13:27:44.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:27:44.137
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:27:44.14
  STEP: Creating pod liveness-1e7b3ddf-e3f7-402c-9ec0-c5230e1715c2 in namespace container-probe-5392 @ 07/29/23 13:27:44.143
  E0729 13:27:44.512923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:45.513004      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:27:46.159: INFO: Started pod liveness-1e7b3ddf-e3f7-402c-9ec0-c5230e1715c2 in namespace container-probe-5392
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 13:27:46.159
  Jul 29 13:27:46.162: INFO: Initial restart count of pod liveness-1e7b3ddf-e3f7-402c-9ec0-c5230e1715c2 is 0
  E0729 13:27:46.513702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:47.513906      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:48.513990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:49.514197      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:50.514739      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:51.514825      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:52.514929      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:53.516031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:54.517062      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:55.517266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:56.518031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:57.518938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:58.519570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:27:59.519989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:00.520902      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:01.521003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:02.521682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:03.521758      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:04.522680      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:05.523032      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:06.523426      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:07.523699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:08.524537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:09.524808      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:10.525017      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:11.525257      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:12.525564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:13.526611      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:14.527098      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:15.527190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:16.527290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:17.527379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:18.527965      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:19.528146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:20.528826      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:21.528837      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:22.529484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:23.529652      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:24.530444      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:25.530629      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:26.531628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:27.531977      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:28.532708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:29.532809      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:30.532931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:31.533022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:32.533106      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:33.533301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:34.533387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:35.533550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:36.533635      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:37.533740      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:38.534163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:39.534386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:40.535408      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:41.536386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:42.537098      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:43.537196      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:44.538040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:45.538153      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:46.538895      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:47.539971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:48.540066      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:49.540708      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:50.540910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:51.540994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:52.541063      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:53.542090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:54.542976      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:55.543971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:56.544018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:57.544878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:58.545423      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:28:59.545588      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:00.546305      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:01.546378      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:02.546529      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:03.547162      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:04.547276      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:05.547974      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:06.548722      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:07.548903      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:08.549023      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:09.549649      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:10.550522      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:11.550608      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:12.551261      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:13.551353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:14.551967      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:15.552154      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:16.552875      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:17.552990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:18.553065      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:19.553252      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:20.553849      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:21.554662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:22.555628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:23.556450      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:24.557234      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:25.557321      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:26.557940      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:27.558041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:28.558714      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:29.559381      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:30.559793      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:31.559856      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:32.560558      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:33.560739      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:34.561569      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:35.561669      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:36.562606      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:37.562782      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:38.563475      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:39.563969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:40.564805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:41.564867      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:42.565090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:43.565184      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:44.565287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:45.566125      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:46.567065      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:47.567161      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:48.568031      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:49.568127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:50.568616      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:51.568730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:52.568826      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:53.568939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:54.569865      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:55.569956      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:56.570844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:57.570916      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:58.571293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:29:59.571972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:00.572921      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:01.573219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:02.573707      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:03.573805      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:04.574475      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:05.574582      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:06.574844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:07.575084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:08.575858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:09.575991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:10.576459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:11.576670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:12.576893      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:13.577053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:14.577178      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:15.577376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:16.578013      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:17.578109      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:18.579132      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:19.579972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:20.580604      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:21.580845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:22.581424      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:23.581584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:24.582067      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:25.582209      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:26.583027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:27.584024      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:28.584085      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:29.584219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:30.584711      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:31.584799      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:32.585310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:33.585397      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:34.586375      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:35.586471      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:36.587126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:37.587970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:38.588852      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:39.589029      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:40.589817      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:41.590882      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:42.591596      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:43.591704      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:44.592090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:45.592192      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:46.593143      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:47.593244      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:48.594093      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:49.594138      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:50.594747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:51.594913      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:52.595128      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:53.595984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:54.596319      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:55.597277      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:56.598057      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:57.598900      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:58.599693      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:30:59.599976      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:00.600392      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:01.600471      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:02.601528      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:03.601623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:04.601954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:05.602065      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:06.602364      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:07.602482      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:08.602948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:09.603067      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:10.603998      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:11.604284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:12.604609      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:13.604778      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:14.605097      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:15.605499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:16.605563      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:17.606165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:18.606306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:19.606421      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:20.607116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:21.607222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:22.607984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:23.608074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:24.608281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:25.608376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:26.608598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:27.608796      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:28.608796      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:29.609005      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:30.609783      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:31.610627      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:32.610844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:33.610928      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:34.611988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:35.612091      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:36.612506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:37.612596      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:38.612794      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:39.612895      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:40.613891      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:41.614662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:42.615365      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:43.615553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:44.615623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:45.616121      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:46.616335      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:31:46.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 13:31:46.677
  STEP: Destroying namespace "container-probe-5392" for this suite. @ 07/29/23 13:31:46.691
• [242.578 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 07/29/23 13:31:46.699
  Jul 29 13:31:46.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/29/23 13:31:46.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:31:46.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:31:46.72
  STEP: create the container to handle the HTTPGet hook request. @ 07/29/23 13:31:46.726
  E0729 13:31:47.616450      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:48.616542      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 07/29/23 13:31:48.744
  E0729 13:31:49.616628      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:50.617159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 07/29/23 13:31:50.763
  STEP: delete the pod with lifecycle hook @ 07/29/23 13:31:50.782
  E0729 13:31:51.617670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:52.617757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:31:52.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3510" for this suite. @ 07/29/23 13:31:52.801
• [6.109 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 07/29/23 13:31:52.809
  Jul 29 13:31:52.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename dns @ 07/29/23 13:31:52.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:31:52.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:31:52.831
  STEP: Creating a test headless service @ 07/29/23 13:31:52.834
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-368.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-368.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-368.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-368.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-368.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-368.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-368.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-368.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-368.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-368.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-368.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-368.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 79.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.79_udp@PTR;check="$$(dig +tcp +noall +answer +search 79.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.79_tcp@PTR;sleep 1; done
   @ 07/29/23 13:31:52.85
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-368.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-368.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-368.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-368.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-368.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-368.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-368.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-368.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-368.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-368.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-368.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-368.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 79.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.79_udp@PTR;check="$$(dig +tcp +noall +answer +search 79.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.79_tcp@PTR;sleep 1; done
   @ 07/29/23 13:31:52.85
  STEP: creating a pod to probe DNS @ 07/29/23 13:31:52.85
  STEP: submitting the pod to kubernetes @ 07/29/23 13:31:52.85
  E0729 13:31:53.617851      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:54.617951      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/29/23 13:31:54.874
  STEP: looking for the results for each expected name from probers @ 07/29/23 13:31:54.878
  Jul 29 13:31:54.882: INFO: Unable to read wheezy_udp@dns-test-service.dns-368.svc.cluster.local from pod dns-368/dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211: the server could not find the requested resource (get pods dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211)
  Jul 29 13:31:54.886: INFO: Unable to read wheezy_tcp@dns-test-service.dns-368.svc.cluster.local from pod dns-368/dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211: the server could not find the requested resource (get pods dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211)
  Jul 29 13:31:54.890: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-368.svc.cluster.local from pod dns-368/dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211: the server could not find the requested resource (get pods dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211)
  Jul 29 13:31:54.894: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-368.svc.cluster.local from pod dns-368/dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211: the server could not find the requested resource (get pods dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211)
  Jul 29 13:31:54.912: INFO: Unable to read jessie_udp@dns-test-service.dns-368.svc.cluster.local from pod dns-368/dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211: the server could not find the requested resource (get pods dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211)
  Jul 29 13:31:54.915: INFO: Unable to read jessie_tcp@dns-test-service.dns-368.svc.cluster.local from pod dns-368/dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211: the server could not find the requested resource (get pods dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211)
  Jul 29 13:31:54.919: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-368.svc.cluster.local from pod dns-368/dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211: the server could not find the requested resource (get pods dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211)
  Jul 29 13:31:54.923: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-368.svc.cluster.local from pod dns-368/dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211: the server could not find the requested resource (get pods dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211)
  Jul 29 13:31:54.937: INFO: Lookups using dns-368/dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211 failed for: [wheezy_udp@dns-test-service.dns-368.svc.cluster.local wheezy_tcp@dns-test-service.dns-368.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-368.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-368.svc.cluster.local jessie_udp@dns-test-service.dns-368.svc.cluster.local jessie_tcp@dns-test-service.dns-368.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-368.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-368.svc.cluster.local]

  E0729 13:31:55.618509      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:56.618591      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:57.618817      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:58.618971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:31:59.619969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:31:59.997: INFO: DNS probes using dns-368/dns-test-1562cb2f-cea6-4e64-8dc6-a9fc29404211 succeeded

  Jul 29 13:31:59.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 13:32:00
  STEP: deleting the test service @ 07/29/23 13:32:00.016
  STEP: deleting the test headless service @ 07/29/23 13:32:00.043
  STEP: Destroying namespace "dns-368" for this suite. @ 07/29/23 13:32:00.054
• [7.250 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 07/29/23 13:32:00.06
  Jul 29 13:32:00.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 13:32:00.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:32:00.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:32:00.083
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/29/23 13:32:00.086
  E0729 13:32:00.620056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:01.620469      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:02.620571      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:03.620665      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:32:04.109
  Jul 29 13:32:04.112: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-0ef69712-4881-4acb-9f06-72835e79ca1d container test-container: <nil>
  STEP: delete the pod @ 07/29/23 13:32:04.131
  Jul 29 13:32:04.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7706" for this suite. @ 07/29/23 13:32:04.152
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 07/29/23 13:32:04.162
  Jul 29 13:32:04.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 13:32:04.163
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:32:04.182
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:32:04.184
  STEP: creating an Endpoint @ 07/29/23 13:32:04.195
  STEP: waiting for available Endpoint @ 07/29/23 13:32:04.198
  STEP: listing all Endpoints @ 07/29/23 13:32:04.2
  STEP: updating the Endpoint @ 07/29/23 13:32:04.203
  STEP: fetching the Endpoint @ 07/29/23 13:32:04.211
  STEP: patching the Endpoint @ 07/29/23 13:32:04.215
  STEP: fetching the Endpoint @ 07/29/23 13:32:04.222
  STEP: deleting the Endpoint by Collection @ 07/29/23 13:32:04.225
  STEP: waiting for Endpoint deletion @ 07/29/23 13:32:04.233
  STEP: fetching the Endpoint @ 07/29/23 13:32:04.234
  Jul 29 13:32:04.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5570" for this suite. @ 07/29/23 13:32:04.24
• [0.086 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 07/29/23 13:32:04.249
  Jul 29 13:32:04.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename containers @ 07/29/23 13:32:04.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:32:04.266
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:32:04.269
  STEP: Creating a pod to test override all @ 07/29/23 13:32:04.271
  E0729 13:32:04.621480      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:05.621963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:06.622219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:07.622312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:32:08.292
  Jul 29 13:32:08.295: INFO: Trying to get logs from node ip-172-31-33-37 pod client-containers-7a6d4e9b-c828-4fc7-ab8b-9343b56a2e20 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 13:32:08.302
  Jul 29 13:32:08.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3920" for this suite. @ 07/29/23 13:32:08.322
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 07/29/23 13:32:08.331
  Jul 29 13:32:08.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pods @ 07/29/23 13:32:08.332
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:32:08.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:32:08.353
  E0729 13:32:08.623143      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:09.623501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:10.623905      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:11.624215      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:12.624998      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:13.625149      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:32:14.428
  Jul 29 13:32:14.431: INFO: Trying to get logs from node ip-172-31-33-37 pod client-envvars-4fd831f4-d559-4e17-ada6-9151170b6e22 container env3cont: <nil>
  STEP: delete the pod @ 07/29/23 13:32:14.439
  Jul 29 13:32:14.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4580" for this suite. @ 07/29/23 13:32:14.457
• [6.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 07/29/23 13:32:14.464
  Jul 29 13:32:14.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename services @ 07/29/23 13:32:14.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:32:14.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:32:14.488
  STEP: creating service multi-endpoint-test in namespace services-1785 @ 07/29/23 13:32:14.491
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1785 to expose endpoints map[] @ 07/29/23 13:32:14.498
  Jul 29 13:32:14.503: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  E0729 13:32:14.625868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:32:15.511: INFO: successfully validated that service multi-endpoint-test in namespace services-1785 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-1785 @ 07/29/23 13:32:15.511
  E0729 13:32:15.626572      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:16.627447      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1785 to expose endpoints map[pod1:[100]] @ 07/29/23 13:32:17.529
  Jul 29 13:32:17.539: INFO: successfully validated that service multi-endpoint-test in namespace services-1785 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-1785 @ 07/29/23 13:32:17.539
  E0729 13:32:17.628264      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:18.628496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1785 to expose endpoints map[pod1:[100] pod2:[101]] @ 07/29/23 13:32:19.552
  Jul 29 13:32:19.565: INFO: successfully validated that service multi-endpoint-test in namespace services-1785 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 07/29/23 13:32:19.565
  Jul 29 13:32:19.565: INFO: Creating new exec pod
  E0729 13:32:19.629148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:20.629385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:21.630039      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:32:22.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-1785 exec execpod25w2r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  E0729 13:32:22.630571      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:32:22.712: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jul 29 13:32:22.712: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 13:32:22.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-1785 exec execpod25w2r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.155 80'
  Jul 29 13:32:22.835: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.152.183.155 80\nConnection to 10.152.183.155 80 port [tcp/http] succeeded!\n"
  Jul 29 13:32:22.835: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 13:32:22.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-1785 exec execpod25w2r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jul 29 13:32:22.957: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jul 29 13:32:22.957: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 13:32:22.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=services-1785 exec execpod25w2r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.152.183.155 81'
  Jul 29 13:32:23.076: INFO: stderr: "+ nc -v -t -w 2 10.152.183.155 81\n+ echo hostName\nConnection to 10.152.183.155 81 port [tcp/*] succeeded!\n"
  Jul 29 13:32:23.076: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-1785 @ 07/29/23 13:32:23.076
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1785 to expose endpoints map[pod2:[101]] @ 07/29/23 13:32:23.095
  Jul 29 13:32:23.108: INFO: successfully validated that service multi-endpoint-test in namespace services-1785 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-1785 @ 07/29/23 13:32:23.108
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1785 to expose endpoints map[] @ 07/29/23 13:32:23.123
  Jul 29 13:32:23.134: INFO: successfully validated that service multi-endpoint-test in namespace services-1785 exposes endpoints map[]
  Jul 29 13:32:23.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1785" for this suite. @ 07/29/23 13:32:23.152
• [8.695 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 07/29/23 13:32:23.16
  Jul 29 13:32:23.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 13:32:23.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:32:23.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:32:23.181
  STEP: Counting existing ResourceQuota @ 07/29/23 13:32:23.183
  E0729 13:32:23.631506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:24.631940      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:25.632044      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:26.632854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:27.633618      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/29/23 13:32:28.187
  STEP: Ensuring resource quota status is calculated @ 07/29/23 13:32:28.192
  E0729 13:32:28.634280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:29.634367      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 07/29/23 13:32:30.197
  STEP: Creating a NodePort Service @ 07/29/23 13:32:30.215
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 07/29/23 13:32:30.25
  STEP: Ensuring resource quota status captures service creation @ 07/29/23 13:32:30.283
  E0729 13:32:30.635010      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:31.635260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 07/29/23 13:32:32.288
  STEP: Ensuring resource quota status released usage @ 07/29/23 13:32:32.324
  E0729 13:32:32.635815      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:33.635922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:32:34.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7705" for this suite. @ 07/29/23 13:32:34.331
• [11.176 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 07/29/23 13:32:34.337
  Jul 29 13:32:34.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 13:32:34.338
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:32:34.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:32:34.357
  STEP: Creating a ResourceQuota @ 07/29/23 13:32:34.36
  STEP: Getting a ResourceQuota @ 07/29/23 13:32:34.369
  STEP: Updating a ResourceQuota @ 07/29/23 13:32:34.371
  STEP: Verifying a ResourceQuota was modified @ 07/29/23 13:32:34.377
  STEP: Deleting a ResourceQuota @ 07/29/23 13:32:34.381
  STEP: Verifying the deleted ResourceQuota @ 07/29/23 13:32:34.386
  Jul 29 13:32:34.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8378" for this suite. @ 07/29/23 13:32:34.392
• [0.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 07/29/23 13:32:34.399
  Jul 29 13:32:34.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 13:32:34.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:32:34.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:32:34.419
  STEP: Given a ReplicationController is created @ 07/29/23 13:32:34.422
  STEP: When the matched label of one of its pods change @ 07/29/23 13:32:34.426
  Jul 29 13:32:34.429: INFO: Pod name pod-release: Found 0 pods out of 1
  E0729 13:32:34.636557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:35.637352      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:36.638169      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:37.638241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:38.638336      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:32:39.435: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/29/23 13:32:39.448
  E0729 13:32:39.638938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:32:40.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2760" for this suite. @ 07/29/23 13:32:40.463
• [6.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 07/29/23 13:32:40.475
  Jul 29 13:32:40.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename secrets @ 07/29/23 13:32:40.476
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:32:40.494
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:32:40.497
  STEP: Creating secret with name secret-test-2e78453d-9ff7-4f1b-8203-f4720d00dacf @ 07/29/23 13:32:40.502
  STEP: Creating a pod to test consume secrets @ 07/29/23 13:32:40.509
  E0729 13:32:40.639161      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:41.639265      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:42.639961      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:43.640501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:32:44.537
  Jul 29 13:32:44.541: INFO: Trying to get logs from node ip-172-31-19-67 pod pod-secrets-82934d8d-aeae-42e3-a729-7772d4606d6e container secret-env-test: <nil>
  STEP: delete the pod @ 07/29/23 13:32:44.55
  Jul 29 13:32:44.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5183" for this suite. @ 07/29/23 13:32:44.576
• [4.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 07/29/23 13:32:44.586
  Jul 29 13:32:44.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename taint-multiple-pods @ 07/29/23 13:32:44.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:32:44.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:32:44.61
  Jul 29 13:32:44.615: INFO: Waiting up to 1m0s for all nodes to be ready
  E0729 13:32:44.641073      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:45.641219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:46.642016      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:47.642139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:48.642175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:49.642334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:50.643278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:51.643562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:52.643701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:53.643861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:54.644758      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:55.644937      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:56.645405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:57.645594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:58.646635      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:32:59.646751      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:00.647271      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:01.647396      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:02.647406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:03.647971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:04.648709      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:05.648842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:06.649337      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:07.649462      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:08.649474      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:09.649682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:10.649791      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:11.650842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:12.651218      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:13.652012      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:14.652943      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:15.653496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:16.653685      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:17.653883      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:18.654151      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:19.654250      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:20.654764      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:21.654976      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:22.655592      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:23.655697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:24.656062      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:25.656281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:26.656481      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:27.656583      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:28.657001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:29.657131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:30.657165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:31.657412      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:32.657933      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:33.658177      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:34.659109      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:35.659225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:36.659549      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:37.659648      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:38.660710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:39.660813      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:40.661017      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:41.661127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:42.662030      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:43.662137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:33:44.631: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 13:33:44.634: INFO: Starting informer...
  STEP: Starting pods... @ 07/29/23 13:33:44.634
  E0729 13:33:44.663003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:33:44.849: INFO: Pod1 is running on ip-172-31-33-37. Tainting Node
  E0729 13:33:45.663137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:46.663488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:33:47.069: INFO: Pod2 is running on ip-172-31-33-37. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/29/23 13:33:47.069
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/29/23 13:33:47.078
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 07/29/23 13:33:47.082
  E0729 13:33:47.663952      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:48.664203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:49.664320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:50.664761      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:51.665170      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:52.665941      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:33:53.299: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0729 13:33:53.666772      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:54.666911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:55.667016      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:56.667419      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:57.667510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:58.667976      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:33:59.668156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:00.668346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:01.669164      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:02.669260      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:03.669459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:04.669550      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:05.669630      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:06.669896      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:07.669964      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:08.670043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:09.670207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:10.670395      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:11.670850      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:12.671656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:34:13.343: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jul 29 13:34:13.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/29/23 13:34:13.357
  STEP: Destroying namespace "taint-multiple-pods-6001" for this suite. @ 07/29/23 13:34:13.361
• [88.783 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 07/29/23 13:34:13.37
  Jul 29 13:34:13.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/29/23 13:34:13.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:34:13.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:34:13.425
  E0729 13:34:13.672459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:14.672544      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:34:15.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 07/29/23 13:34:15.467
  STEP: Cleaning up the configmap @ 07/29/23 13:34:15.472
  STEP: Cleaning up the pod @ 07/29/23 13:34:15.478
  STEP: Destroying namespace "emptydir-wrapper-151" for this suite. @ 07/29/23 13:34:15.49
• [2.131 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 07/29/23 13:34:15.502
  Jul 29 13:34:15.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename job @ 07/29/23 13:34:15.502
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:34:15.52
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:34:15.523
  STEP: Creating a job @ 07/29/23 13:34:15.525
  STEP: Ensuring active pods == parallelism @ 07/29/23 13:34:15.534
  E0729 13:34:15.673800      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:16.674472      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 07/29/23 13:34:17.539
  STEP: deleting Job.batch foo in namespace job-8419, will wait for the garbage collector to delete the pods @ 07/29/23 13:34:17.539
  Jul 29 13:34:17.600: INFO: Deleting Job.batch foo took: 6.887827ms
  E0729 13:34:17.675238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:34:17.701: INFO: Terminating Job.batch foo pods took: 101.088966ms
  E0729 13:34:18.676023      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:19.676347      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:20.676613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:21.676878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:22.676947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:23.677149      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:24.677344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:25.677611      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:26.677953      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:27.678165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:28.679219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:29.679275      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:30.679617      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:31.679958      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:32.680155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:33.680360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:34.680409      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:35.680457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:36.680821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:37.680992      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:38.681072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:39.681119      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:40.681326      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:41.681552      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:42.682607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:43.683665      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:44.683750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:45.683963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:46.684342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:47.684967      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:48.685180      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 07/29/23 13:34:49.502
  Jul 29 13:34:49.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8419" for this suite. @ 07/29/23 13:34:49.512
• [34.016 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 07/29/23 13:34:49.518
  Jul 29 13:34:49.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 13:34:49.519
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:34:49.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:34:49.54
  STEP: Counting existing ResourceQuota @ 07/29/23 13:34:49.543
  E0729 13:34:49.685903      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:50.686540      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:51.686919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:52.687540      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:53.687575      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/29/23 13:34:54.546
  STEP: Ensuring resource quota status is calculated @ 07/29/23 13:34:54.551
  E0729 13:34:54.687982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:55.688072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 07/29/23 13:34:56.556
  STEP: Ensuring ResourceQuota status captures the pod usage @ 07/29/23 13:34:56.572
  E0729 13:34:56.688420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:57.689393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 07/29/23 13:34:58.575
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 07/29/23 13:34:58.578
  STEP: Ensuring a pod cannot update its resource requirements @ 07/29/23 13:34:58.58
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 07/29/23 13:34:58.583
  E0729 13:34:58.690267      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:34:59.690359      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/29/23 13:35:00.589
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 13:35:00.6
  E0729 13:35:00.691148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:01.691973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:35:02.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-630" for this suite. @ 07/29/23 13:35:02.607
• [13.096 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 07/29/23 13:35:02.614
  Jul 29 13:35:02.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:35:02.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:35:02.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:35:02.637
  STEP: Creating secret with name projected-secret-test-597c39d0-9b27-4af5-9b54-b556096e6658 @ 07/29/23 13:35:02.64
  STEP: Creating a pod to test consume secrets @ 07/29/23 13:35:02.644
  E0729 13:35:02.692025      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:03.692309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:04.693279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:05.693362      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:35:06.67
  Jul 29 13:35:06.672: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-projected-secrets-2e44bccc-b09b-43a5-be51-e7de172072ee container secret-volume-test: <nil>
  E0729 13:35:06.693875      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 07/29/23 13:35:06.699
  Jul 29 13:35:06.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3019" for this suite. @ 07/29/23 13:35:06.718
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 07/29/23 13:35:06.726
  Jul 29 13:35:06.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 13:35:06.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:35:06.741
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:35:06.745
  STEP: Setting up server cert @ 07/29/23 13:35:06.768
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 13:35:07.053
  STEP: Deploying the webhook pod @ 07/29/23 13:35:07.06
  STEP: Wait for the deployment to be ready @ 07/29/23 13:35:07.07
  Jul 29 13:35:07.077: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 13:35:07.694878      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:08.694937      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 13:35:09.086
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 13:35:09.096
  E0729 13:35:09.696015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:35:10.096: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 07/29/23 13:35:10.101
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 07/29/23 13:35:10.12
  STEP: Creating a configMap that should not be mutated @ 07/29/23 13:35:10.126
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 07/29/23 13:35:10.136
  STEP: Creating a configMap that should be mutated @ 07/29/23 13:35:10.143
  Jul 29 13:35:10.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5223" for this suite. @ 07/29/23 13:35:10.205
  STEP: Destroying namespace "webhook-markers-3351" for this suite. @ 07/29/23 13:35:10.214
• [3.494 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 07/29/23 13:35:10.22
  Jul 29 13:35:10.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 13:35:10.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:35:10.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:35:10.247
  STEP: Setting up server cert @ 07/29/23 13:35:10.273
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 13:35:10.577
  STEP: Deploying the webhook pod @ 07/29/23 13:35:10.581
  STEP: Wait for the deployment to be ready @ 07/29/23 13:35:10.592
  Jul 29 13:35:10.602: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 13:35:10.696907      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:11.697266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 13:35:12.612
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 13:35:12.624
  E0729 13:35:12.697496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:35:13.624: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 07/29/23 13:35:13.629
  STEP: create a namespace for the webhook @ 07/29/23 13:35:13.643
  STEP: create a configmap should be unconditionally rejected by the webhook @ 07/29/23 13:35:13.662
  Jul 29 13:35:13.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 13:35:13.697911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-7677" for this suite. @ 07/29/23 13:35:13.729
  STEP: Destroying namespace "webhook-markers-1802" for this suite. @ 07/29/23 13:35:13.735
  STEP: Destroying namespace "fail-closed-namespace-3774" for this suite. @ 07/29/23 13:35:13.741
• [3.527 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 07/29/23 13:35:13.75
  Jul 29 13:35:13.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pods @ 07/29/23 13:35:13.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:35:13.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:35:13.772
  Jul 29 13:35:13.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: creating the pod @ 07/29/23 13:35:13.775
  STEP: submitting the pod to kubernetes @ 07/29/23 13:35:13.775
  E0729 13:35:14.698348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:15.698728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:35:15.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4254" for this suite. @ 07/29/23 13:35:15.819
• [2.075 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 07/29/23 13:35:15.826
  Jul 29 13:35:15.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename gc @ 07/29/23 13:35:15.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:35:15.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:35:15.844
  STEP: create the rc1 @ 07/29/23 13:35:15.851
  STEP: create the rc2 @ 07/29/23 13:35:15.857
  E0729 13:35:16.699010      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:17.699124      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:18.701427      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:19.701615      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:20.701730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:21.701776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 07/29/23 13:35:21.866
  STEP: delete the rc simpletest-rc-to-be-deleted @ 07/29/23 13:35:22.604
  STEP: wait for the rc to be deleted @ 07/29/23 13:35:22.613
  E0729 13:35:22.711121      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:23.711175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:24.711289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:25.713554      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:26.714681      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:35:27.637: INFO: 73 pods remaining
  Jul 29 13:35:27.638: INFO: 73 pods has nil DeletionTimestamp
  Jul 29 13:35:27.638: INFO: 
  E0729 13:35:27.714763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:28.715072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:29.716019      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:30.717028      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:31.717060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/29/23 13:35:32.625
  W0729 13:35:32.629164      18 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  Jul 29 13:35:32.629: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 13:35:32.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-29w9j" in namespace "gc-3116"
  Jul 29 13:35:32.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gc7q" in namespace "gc-3116"
  Jul 29 13:35:32.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lbw9" in namespace "gc-3116"
  Jul 29 13:35:32.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zkrj" in namespace "gc-3116"
  Jul 29 13:35:32.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-46tcv" in namespace "gc-3116"
  Jul 29 13:35:32.695: INFO: Deleting pod "simpletest-rc-to-be-deleted-49pf6" in namespace "gc-3116"
  Jul 29 13:35:32.710: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b48v" in namespace "gc-3116"
  E0729 13:35:32.717744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:35:32.721: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hprh" in namespace "gc-3116"
  Jul 29 13:35:32.734: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lgrs" in namespace "gc-3116"
  Jul 29 13:35:32.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mf7v" in namespace "gc-3116"
  Jul 29 13:35:32.763: INFO: Deleting pod "simpletest-rc-to-be-deleted-59gr4" in namespace "gc-3116"
  Jul 29 13:35:32.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vrtz" in namespace "gc-3116"
  Jul 29 13:35:32.811: INFO: Deleting pod "simpletest-rc-to-be-deleted-6874s" in namespace "gc-3116"
  Jul 29 13:35:32.823: INFO: Deleting pod "simpletest-rc-to-be-deleted-6b9z7" in namespace "gc-3116"
  Jul 29 13:35:32.837: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fwxn" in namespace "gc-3116"
  Jul 29 13:35:32.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-6g9vf" in namespace "gc-3116"
  Jul 29 13:35:32.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nrlz" in namespace "gc-3116"
  Jul 29 13:35:32.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-72g95" in namespace "gc-3116"
  Jul 29 13:35:32.889: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gp5r" in namespace "gc-3116"
  Jul 29 13:35:32.904: INFO: Deleting pod "simpletest-rc-to-be-deleted-7krpq" in namespace "gc-3116"
  Jul 29 13:35:32.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-89cqn" in namespace "gc-3116"
  Jul 29 13:35:32.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-8hcv2" in namespace "gc-3116"
  Jul 29 13:35:32.952: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vn8x" in namespace "gc-3116"
  Jul 29 13:35:32.965: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qlzd" in namespace "gc-3116"
  Jul 29 13:35:32.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5z4g" in namespace "gc-3116"
  Jul 29 13:35:32.990: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcmrr" in namespace "gc-3116"
  Jul 29 13:35:33.002: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfngw" in namespace "gc-3116"
  Jul 29 13:35:33.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-bj92r" in namespace "gc-3116"
  Jul 29 13:35:33.025: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm2fl" in namespace "gc-3116"
  Jul 29 13:35:33.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6d7s" in namespace "gc-3116"
  Jul 29 13:35:33.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-cbz28" in namespace "gc-3116"
  Jul 29 13:35:33.063: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxzmq" in namespace "gc-3116"
  Jul 29 13:35:33.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-d45t9" in namespace "gc-3116"
  Jul 29 13:35:33.098: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9lk7" in namespace "gc-3116"
  Jul 29 13:35:33.111: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkgcd" in namespace "gc-3116"
  Jul 29 13:35:33.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpf7c" in namespace "gc-3116"
  Jul 29 13:35:33.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-f72nz" in namespace "gc-3116"
  Jul 29 13:35:33.158: INFO: Deleting pod "simpletest-rc-to-be-deleted-fpjj5" in namespace "gc-3116"
  Jul 29 13:35:33.178: INFO: Deleting pod "simpletest-rc-to-be-deleted-gp4mv" in namespace "gc-3116"
  Jul 29 13:35:33.204: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsmcw" in namespace "gc-3116"
  Jul 29 13:35:33.223: INFO: Deleting pod "simpletest-rc-to-be-deleted-gz5dq" in namespace "gc-3116"
  Jul 29 13:35:33.238: INFO: Deleting pod "simpletest-rc-to-be-deleted-h5wsw" in namespace "gc-3116"
  Jul 29 13:35:33.254: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6jhg" in namespace "gc-3116"
  Jul 29 13:35:33.271: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvqp2" in namespace "gc-3116"
  Jul 29 13:35:33.291: INFO: Deleting pod "simpletest-rc-to-be-deleted-hw6wf" in namespace "gc-3116"
  Jul 29 13:35:33.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwjn8" in namespace "gc-3116"
  Jul 29 13:35:33.322: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnmf7" in namespace "gc-3116"
  Jul 29 13:35:33.335: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnrhj" in namespace "gc-3116"
  Jul 29 13:35:33.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-jvwsg" in namespace "gc-3116"
  Jul 29 13:35:33.380: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2sd4" in namespace "gc-3116"
  Jul 29 13:35:33.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3116" for this suite. @ 07/29/23 13:35:33.405
• [17.586 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 07/29/23 13:35:33.417
  Jul 29 13:35:33.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 13:35:33.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:35:33.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:35:33.536
  E0729 13:35:33.718445      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:34.719973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:35.720590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:36.721547      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:37.721614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:38.721970      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:39.722774      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:40.723149      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:41.724311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:42.725139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:43.726108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:44.726461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:45.727317      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:46.728387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:47.729114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:48.730152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:49.731111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 07/29/23 13:35:50.55
  E0729 13:35:50.731701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:51.732084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:52.732450      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:53.732692      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:54.733662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/29/23 13:35:55.553
  STEP: Ensuring resource quota status is calculated @ 07/29/23 13:35:55.559
  E0729 13:35:55.733741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:56.734171      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 07/29/23 13:35:57.562
  STEP: Ensuring resource quota status captures configMap creation @ 07/29/23 13:35:57.573
  E0729 13:35:57.734431      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:35:58.734533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 07/29/23 13:35:59.577
  STEP: Ensuring resource quota status released usage @ 07/29/23 13:35:59.584
  E0729 13:35:59.734907      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:00.735976      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:36:01.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5663" for this suite. @ 07/29/23 13:36:01.591
• [28.181 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 07/29/23 13:36:01.599
  Jul 29 13:36:01.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 13:36:01.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:36:01.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:36:01.618
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 13:36:01.621
  E0729 13:36:01.736343      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:02.736420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:03.736965      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:04.737059      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:36:05.643
  Jul 29 13:36:05.647: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-c639275f-0f71-486a-8365-5d6f7f063131 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 13:36:05.654
  Jul 29 13:36:05.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9376" for this suite. @ 07/29/23 13:36:05.669
• [4.077 seconds]
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 07/29/23 13:36:05.676
  Jul 29 13:36:05.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename dns @ 07/29/23 13:36:05.677
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:36:05.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:36:05.696
  STEP: Creating a test headless service @ 07/29/23 13:36:05.699
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5984.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5984.svc.cluster.local;sleep 1; done
   @ 07/29/23 13:36:05.703
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5984.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5984.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5984.svc.cluster.local;sleep 1; done
   @ 07/29/23 13:36:05.703
  STEP: creating a pod to probe DNS @ 07/29/23 13:36:05.703
  STEP: submitting the pod to kubernetes @ 07/29/23 13:36:05.703
  E0729 13:36:05.737460      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:06.737871      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/29/23 13:36:07.722
  STEP: looking for the results for each expected name from probers @ 07/29/23 13:36:07.725
  Jul 29 13:36:07.731: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local from pod dns-5984/dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f: the server could not find the requested resource (get pods dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f)
  Jul 29 13:36:07.735: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local from pod dns-5984/dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f: the server could not find the requested resource (get pods dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f)
  E0729 13:36:07.738295      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:36:07.738: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5984.svc.cluster.local from pod dns-5984/dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f: the server could not find the requested resource (get pods dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f)
  Jul 29 13:36:07.743: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5984.svc.cluster.local from pod dns-5984/dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f: the server could not find the requested resource (get pods dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f)
  Jul 29 13:36:07.746: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local from pod dns-5984/dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f: the server could not find the requested resource (get pods dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f)
  Jul 29 13:36:07.750: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local from pod dns-5984/dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f: the server could not find the requested resource (get pods dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f)
  Jul 29 13:36:07.754: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5984.svc.cluster.local from pod dns-5984/dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f: the server could not find the requested resource (get pods dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f)
  Jul 29 13:36:07.757: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5984.svc.cluster.local from pod dns-5984/dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f: the server could not find the requested resource (get pods dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f)
  Jul 29 13:36:07.757: INFO: Lookups using dns-5984/dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5984.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5984.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5984.svc.cluster.local jessie_udp@dns-test-service-2.dns-5984.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5984.svc.cluster.local]

  E0729 13:36:08.738412      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:09.739081      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:10.739163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:11.739983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:12.740201      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:36:12.789: INFO: DNS probes using dns-5984/dns-test-3640736a-4d8d-48ce-95f9-a604c8abfa3f succeeded

  Jul 29 13:36:12.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 13:36:12.792
  STEP: deleting the test headless service @ 07/29/23 13:36:12.804
  STEP: Destroying namespace "dns-5984" for this suite. @ 07/29/23 13:36:12.819
• [7.149 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 07/29/23 13:36:12.826
  Jul 29 13:36:12.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename dns @ 07/29/23 13:36:12.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:36:12.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:36:12.845
  STEP: Creating a test headless service @ 07/29/23 13:36:12.848
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7653.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7653.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 07/29/23 13:36:12.852
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7653.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7653.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 07/29/23 13:36:12.852
  STEP: creating a pod to probe DNS @ 07/29/23 13:36:12.852
  STEP: submitting the pod to kubernetes @ 07/29/23 13:36:12.852
  E0729 13:36:13.740291      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:14.740507      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/29/23 13:36:14.872
  STEP: looking for the results for each expected name from probers @ 07/29/23 13:36:14.876
  Jul 29 13:36:14.892: INFO: DNS probes using dns-7653/dns-test-e087bf3c-0175-4a20-bff8-12467c3cea11 succeeded

  Jul 29 13:36:14.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 13:36:14.898
  STEP: deleting the test headless service @ 07/29/23 13:36:14.909
  STEP: Destroying namespace "dns-7653" for this suite. @ 07/29/23 13:36:14.921
• [2.104 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 07/29/23 13:36:14.929
  Jul 29 13:36:14.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 13:36:14.93
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:36:14.945
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:36:14.948
  STEP: Creating configMap with name cm-test-opt-del-ca81299d-dace-471d-962f-57353aefef02 @ 07/29/23 13:36:14.954
  STEP: Creating configMap with name cm-test-opt-upd-bc064dae-1c3d-4aa7-ae13-7c3e63b1dc47 @ 07/29/23 13:36:14.958
  STEP: Creating the pod @ 07/29/23 13:36:14.963
  E0729 13:36:15.741395      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:16.741767      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-ca81299d-dace-471d-962f-57353aefef02 @ 07/29/23 13:36:17.003
  STEP: Updating configmap cm-test-opt-upd-bc064dae-1c3d-4aa7-ae13-7c3e63b1dc47 @ 07/29/23 13:36:17.008
  STEP: Creating configMap with name cm-test-opt-create-2b1c98f0-9e48-493a-91b3-3594db3420c0 @ 07/29/23 13:36:17.013
  STEP: waiting to observe update in volume @ 07/29/23 13:36:17.016
  E0729 13:36:17.741837      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:18.742094      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:19.743080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:20.743977      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:21.744190      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:22.744384      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:23.745383      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:24.745687      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:25.745776      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:26.746158      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:27.746265      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:28.747293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:29.747991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:30.748076      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:31.749154      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:32.749351      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:33.750121      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:34.750303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:35.750394      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:36.750750      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:37.751474      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:38.751985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:39.752757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:40.752939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:41.753678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:42.753881      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:43.754645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:44.754919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:45.754966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:46.755333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:47.755956      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:48.755990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:49.756080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:50.756947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:51.757410      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:52.757506      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:53.758584      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:54.758753      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:55.759455      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:56.759732      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:57.760240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:58.760320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:36:59.760783      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:00.760924      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:01.760992      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:02.761173      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:03.761656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:04.761859      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:05.762785      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:06.762921      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:07.763987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:08.764129      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:09.764895      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:10.765110      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:11.766034      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:12.766117      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:13.766296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:14.767236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:15.767860      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:16.768096      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:17.768657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:18.768748      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:19.768917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:20.769854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:21.770154      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:22.770621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:23.771245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:24.771360      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:37:25.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6116" for this suite. @ 07/29/23 13:37:25.339
• [70.416 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 07/29/23 13:37:25.347
  Jul 29 13:37:25.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:37:25.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:37:25.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:37:25.369
  STEP: Creating secret with name s-test-opt-del-a6715eb8-be03-4d27-ac41-884cf5286662 @ 07/29/23 13:37:25.376
  STEP: Creating secret with name s-test-opt-upd-1b246809-09f9-46ee-9ea9-45b741163414 @ 07/29/23 13:37:25.388
  STEP: Creating the pod @ 07/29/23 13:37:25.392
  E0729 13:37:25.771476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:26.771556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-a6715eb8-be03-4d27-ac41-884cf5286662 @ 07/29/23 13:37:27.448
  STEP: Updating secret s-test-opt-upd-1b246809-09f9-46ee-9ea9-45b741163414 @ 07/29/23 13:37:27.455
  STEP: Creating secret with name s-test-opt-create-cea27fa8-d499-44b2-af4e-747849fb4a38 @ 07/29/23 13:37:27.459
  STEP: waiting to observe update in volume @ 07/29/23 13:37:27.464
  E0729 13:37:27.771631      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:28.771738      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:29.772369      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:30.772489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:31.772917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:32.773111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:33.773938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:34.774122      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:35.775117      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:36.775207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:37.775253      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:38.776037      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:39.776108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:40.776384      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:41.776478      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:42.776575      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:43.776646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:44.776754      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:45.776844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:46.777744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:47.778527      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:48.778718      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:49.779087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:50.779177      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:51.779989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:52.780193      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:53.780813      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:54.780910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:55.781512      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:56.781755      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:57.782656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:58.783046      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:37:59.783368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:00.783985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:01.785060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:02.785120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:03.785657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:04.785863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:05.786530      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:06.786923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:07.787431      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:08.787546      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:09.788255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:10.788359      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:11.788508      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:12.788669      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:13.789458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:14.789544      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:15.790387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:16.790811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:17.791002      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:18.791979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:19.792435      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:20.793309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:21.794235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:22.794383      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:23.794629      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:24.794774      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:25.795665      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:26.796145      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:27.796582      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:28.796713      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:29.797461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:30.797642      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:31.797844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:32.797932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:33.798471      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:34.798597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:35.799662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:36.799765      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:37.800134      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:38.800236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:39.800285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:40.800476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:41.801489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:42.801689      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:43.802155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:44.802270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:45.802332      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:46.802591      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:47.803402      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:48.803833      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:49.803918      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:50.804939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:51.805095      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:52.805534      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:53.805773      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:54.805951      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:55.806062      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:38:55.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1377" for this suite. @ 07/29/23 13:38:55.85
• [90.509 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 07/29/23 13:38:55.857
  Jul 29 13:38:55.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 13:38:55.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:38:55.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:38:55.881
  STEP: Creating a pod to test env composition @ 07/29/23 13:38:55.883
  E0729 13:38:56.806725      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:57.806926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:58.807505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:38:59.807589      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:38:59.912
  Jul 29 13:38:59.914: INFO: Trying to get logs from node ip-172-31-33-37 pod var-expansion-6313fc96-eabd-4980-adfe-a5e0096a459f container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 13:38:59.929
  Jul 29 13:38:59.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5251" for this suite. @ 07/29/23 13:38:59.948
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 07/29/23 13:38:59.956
  Jul 29 13:38:59.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename secrets @ 07/29/23 13:38:59.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:38:59.973
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:38:59.977
  STEP: Creating secret with name secret-test-map-b735d4f6-3206-4bbb-a87e-8edb512b3f82 @ 07/29/23 13:38:59.98
  STEP: Creating a pod to test consume secrets @ 07/29/23 13:38:59.986
  E0729 13:39:00.808154      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:01.808426      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:02.809092      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:03.809297      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:39:04.007
  Jul 29 13:39:04.011: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-secrets-2c899093-b772-485a-bcda-a63c49c4e644 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 13:39:04.018
  Jul 29 13:39:04.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2313" for this suite. @ 07/29/23 13:39:04.034
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 07/29/23 13:39:04.042
  Jul 29 13:39:04.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 13:39:04.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:04.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:04.06
  STEP: Setting up server cert @ 07/29/23 13:39:04.083
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 13:39:04.407
  STEP: Deploying the webhook pod @ 07/29/23 13:39:04.416
  STEP: Wait for the deployment to be ready @ 07/29/23 13:39:04.429
  Jul 29 13:39:04.436: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 13:39:04.809779      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:05.810859      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 13:39:06.445
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 13:39:06.454
  E0729 13:39:06.811846      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:39:07.454: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 29 13:39:07.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  E0729 13:39:07.811988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7502-crds.webhook.example.com via the AdmissionRegistration API @ 07/29/23 13:39:07.968
  STEP: Creating a custom resource while v1 is storage version @ 07/29/23 13:39:07.984
  E0729 13:39:08.812927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:09.813016      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 07/29/23 13:39:10.031
  STEP: Patching the custom resource while v2 is storage version @ 07/29/23 13:39:10.049
  Jul 29 13:39:10.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-37" for this suite. @ 07/29/23 13:39:10.625
  STEP: Destroying namespace "webhook-markers-5545" for this suite. @ 07/29/23 13:39:10.632
• [6.598 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 07/29/23 13:39:10.641
  Jul 29 13:39:10.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 13:39:10.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:10.654
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:10.657
  E0729 13:39:10.813741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:11.813863      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:39:12.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 13:39:12.680: INFO: Deleting pod "var-expansion-cab49a7a-ca53-4c02-908d-3fe55eb40a00" in namespace "var-expansion-1769"
  Jul 29 13:39:12.688: INFO: Wait up to 5m0s for pod "var-expansion-cab49a7a-ca53-4c02-908d-3fe55eb40a00" to be fully deleted
  E0729 13:39:12.814116      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:13.815155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-1769" for this suite. @ 07/29/23 13:39:14.695
• [4.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 07/29/23 13:39:14.71
  Jul 29 13:39:14.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename secrets @ 07/29/23 13:39:14.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:14.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:14.729
  STEP: Creating projection with secret that has name secret-emptykey-test-0342c210-b8dc-43b4-9659-89bf95aee061 @ 07/29/23 13:39:14.732
  Jul 29 13:39:14.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-347" for this suite. @ 07/29/23 13:39:14.738
• [0.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 07/29/23 13:39:14.747
  Jul 29 13:39:14.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 13:39:14.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:14.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:14.764
  STEP: Read namespace status @ 07/29/23 13:39:14.766
  Jul 29 13:39:14.769: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 07/29/23 13:39:14.769
  Jul 29 13:39:14.775: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 07/29/23 13:39:14.775
  Jul 29 13:39:14.783: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jul 29 13:39:14.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-869" for this suite. @ 07/29/23 13:39:14.787
• [0.047 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 07/29/23 13:39:14.794
  Jul 29 13:39:14.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename sched-pred @ 07/29/23 13:39:14.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:14.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:14.81
  Jul 29 13:39:14.812: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  E0729 13:39:14.815792      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:39:14.820: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 13:39:14.824: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-19-67 before test
  Jul 29 13:39:14.830: INFO: nginx-ingress-controller-kubernetes-worker-gllr9 from ingress-nginx-kubernetes-worker started at 2023-07-29 12:05:42 +0000 UTC (1 container statuses recorded)
  Jul 29 13:39:14.830: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 29 13:39:14.830: INFO: calico-kube-controllers-66cf5c7c9b-lp29z from kube-system started at 2023-07-29 12:05:47 +0000 UTC (1 container statuses recorded)
  Jul 29 13:39:14.830: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul 29 13:39:14.830: INFO: sonobuoy-e2e-job-fe057c2caff44e9c from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:39:14.830: INFO: 	Container e2e ready: true, restart count 0
  Jul 29 13:39:14.830: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:39:14.830: INFO: sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-7dv7m from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:39:14.830: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:39:14.830: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 13:39:14.830: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-33-37 before test
  Jul 29 13:39:14.833: INFO: nginx-ingress-controller-kubernetes-worker-zhwpz from ingress-nginx-kubernetes-worker started at 2023-07-29 13:34:13 +0000 UTC (1 container statuses recorded)
  Jul 29 13:39:14.834: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 29 13:39:14.834: INFO: sonobuoy from sonobuoy started at 2023-07-29 12:12:09 +0000 UTC (1 container statuses recorded)
  Jul 29 13:39:14.834: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 29 13:39:14.834: INFO: sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-fxbms from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:39:14.834: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:39:14.834: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 13:39:14.834: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-5-66 before test
  Jul 29 13:39:14.838: INFO: default-http-backend-kubernetes-worker-65fc475d49-fwzg8 from ingress-nginx-kubernetes-worker started at 2023-07-29 11:57:02 +0000 UTC (1 container statuses recorded)
  Jul 29 13:39:14.838: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul 29 13:39:14.838: INFO: nginx-ingress-controller-kubernetes-worker-hbv24 from ingress-nginx-kubernetes-worker started at 2023-07-29 11:57:02 +0000 UTC (1 container statuses recorded)
  Jul 29 13:39:14.838: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 29 13:39:14.838: INFO: coredns-5c7f76ccb8-7mf4p from kube-system started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:39:14.838: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 13:39:14.838: INFO: kube-state-metrics-5b95b4459c-849m5 from kube-system started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:39:14.838: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul 29 13:39:14.838: INFO: metrics-server-v0.5.2-6cf8c8b69c-slzlr from kube-system started at 2023-07-29 11:55:52 +0000 UTC (2 container statuses recorded)
  Jul 29 13:39:14.838: INFO: 	Container metrics-server ready: true, restart count 0
  Jul 29 13:39:14.838: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul 29 13:39:14.838: INFO: dashboard-metrics-scraper-6b8586b5c9-6s95z from kubernetes-dashboard started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:39:14.838: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul 29 13:39:14.838: INFO: kubernetes-dashboard-6869f4cd5f-fwg5d from kubernetes-dashboard started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:39:14.838: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul 29 13:39:14.838: INFO: sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-n84sm from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:39:14.838: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:39:14.838: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ip-172-31-19-67 @ 07/29/23 13:39:14.851
  STEP: verifying the node has the label node ip-172-31-33-37 @ 07/29/23 13:39:14.87
  STEP: verifying the node has the label node ip-172-31-5-66 @ 07/29/23 13:39:14.884
  Jul 29 13:39:14.904: INFO: Pod default-http-backend-kubernetes-worker-65fc475d49-fwzg8 requesting resource cpu=10m on Node ip-172-31-5-66
  Jul 29 13:39:14.906: INFO: Pod nginx-ingress-controller-kubernetes-worker-gllr9 requesting resource cpu=0m on Node ip-172-31-19-67
  Jul 29 13:39:14.906: INFO: Pod nginx-ingress-controller-kubernetes-worker-hbv24 requesting resource cpu=0m on Node ip-172-31-5-66
  Jul 29 13:39:14.906: INFO: Pod nginx-ingress-controller-kubernetes-worker-zhwpz requesting resource cpu=0m on Node ip-172-31-33-37
  Jul 29 13:39:14.907: INFO: Pod calico-kube-controllers-66cf5c7c9b-lp29z requesting resource cpu=0m on Node ip-172-31-19-67
  Jul 29 13:39:14.907: INFO: Pod coredns-5c7f76ccb8-7mf4p requesting resource cpu=100m on Node ip-172-31-5-66
  Jul 29 13:39:14.908: INFO: Pod kube-state-metrics-5b95b4459c-849m5 requesting resource cpu=0m on Node ip-172-31-5-66
  Jul 29 13:39:14.908: INFO: Pod metrics-server-v0.5.2-6cf8c8b69c-slzlr requesting resource cpu=5m on Node ip-172-31-5-66
  Jul 29 13:39:14.909: INFO: Pod dashboard-metrics-scraper-6b8586b5c9-6s95z requesting resource cpu=0m on Node ip-172-31-5-66
  Jul 29 13:39:14.909: INFO: Pod kubernetes-dashboard-6869f4cd5f-fwg5d requesting resource cpu=0m on Node ip-172-31-5-66
  Jul 29 13:39:14.911: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-33-37
  Jul 29 13:39:14.911: INFO: Pod sonobuoy-e2e-job-fe057c2caff44e9c requesting resource cpu=0m on Node ip-172-31-19-67
  Jul 29 13:39:14.912: INFO: Pod sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-7dv7m requesting resource cpu=0m on Node ip-172-31-19-67
  Jul 29 13:39:14.912: INFO: Pod sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-fxbms requesting resource cpu=0m on Node ip-172-31-33-37
  Jul 29 13:39:14.912: INFO: Pod sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-n84sm requesting resource cpu=0m on Node ip-172-31-5-66
  STEP: Starting Pods to consume most of the cluster CPU. @ 07/29/23 13:39:14.912
  Jul 29 13:39:14.913: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-19-67
  Jul 29 13:39:14.920: INFO: Creating a pod which consumes cpu=1400m on Node ip-172-31-33-37
  Jul 29 13:39:14.927: INFO: Creating a pod which consumes cpu=1319m on Node ip-172-31-5-66
  E0729 13:39:15.816080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:16.816284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 07/29/23 13:39:16.947
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-511b3f18-30b9-431a-85a9-0ce10a05642f.17765a433bfd93f4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-979/filler-pod-511b3f18-30b9-431a-85a9-0ce10a05642f to ip-172-31-5-66] @ 07/29/23 13:39:16.951
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-511b3f18-30b9-431a-85a9-0ce10a05642f.17765a43645e47cf], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/29/23 13:39:16.951
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-511b3f18-30b9-431a-85a9-0ce10a05642f.17765a4365848a9a], Reason = [Created], Message = [Created container filler-pod-511b3f18-30b9-431a-85a9-0ce10a05642f] @ 07/29/23 13:39:16.951
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-511b3f18-30b9-431a-85a9-0ce10a05642f.17765a43699e3530], Reason = [Started], Message = [Started container filler-pod-511b3f18-30b9-431a-85a9-0ce10a05642f] @ 07/29/23 13:39:16.951
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c3c7b6e6-abd8-44fe-9542-48171222bec7.17765a433bc42df9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-979/filler-pod-c3c7b6e6-abd8-44fe-9542-48171222bec7 to ip-172-31-33-37] @ 07/29/23 13:39:16.952
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c3c7b6e6-abd8-44fe-9542-48171222bec7.17765a4361b199f7], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/29/23 13:39:16.952
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c3c7b6e6-abd8-44fe-9542-48171222bec7.17765a4362cdc25a], Reason = [Created], Message = [Created container filler-pod-c3c7b6e6-abd8-44fe-9542-48171222bec7] @ 07/29/23 13:39:16.952
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c3c7b6e6-abd8-44fe-9542-48171222bec7.17765a436730ffde], Reason = [Started], Message = [Started container filler-pod-c3c7b6e6-abd8-44fe-9542-48171222bec7] @ 07/29/23 13:39:16.952
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cb5b0800-67e7-43c8-8d3f-75052a6eee53.17765a433b235501], Reason = [Scheduled], Message = [Successfully assigned sched-pred-979/filler-pod-cb5b0800-67e7-43c8-8d3f-75052a6eee53 to ip-172-31-19-67] @ 07/29/23 13:39:16.952
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cb5b0800-67e7-43c8-8d3f-75052a6eee53.17765a4362a41b31], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/29/23 13:39:16.952
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cb5b0800-67e7-43c8-8d3f-75052a6eee53.17765a436416f10e], Reason = [Created], Message = [Created container filler-pod-cb5b0800-67e7-43c8-8d3f-75052a6eee53] @ 07/29/23 13:39:16.952
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cb5b0800-67e7-43c8-8d3f-75052a6eee53.17765a4367b1ee2f], Reason = [Started], Message = [Started container filler-pod-cb5b0800-67e7-43c8-8d3f-75052a6eee53] @ 07/29/23 13:39:16.952
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.17765a43b4548cbe], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 3 Insufficient cpu. preemption: 0/5 nodes are available: 2 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] @ 07/29/23 13:39:16.963
  E0729 13:39:17.817061      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node ip-172-31-33-37 @ 07/29/23 13:39:17.963
  STEP: verifying the node doesn't have the label node @ 07/29/23 13:39:17.974
  STEP: removing the label node off the node ip-172-31-5-66 @ 07/29/23 13:39:17.978
  STEP: verifying the node doesn't have the label node @ 07/29/23 13:39:17.992
  STEP: removing the label node off the node ip-172-31-19-67 @ 07/29/23 13:39:17.996
  STEP: verifying the node doesn't have the label node @ 07/29/23 13:39:18.011
  Jul 29 13:39:18.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-979" for this suite. @ 07/29/23 13:39:18.02
• [3.233 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 07/29/23 13:39:18.027
  Jul 29 13:39:18.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename lease-test @ 07/29/23 13:39:18.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:18.044
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:18.047
  Jul 29 13:39:18.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-1198" for this suite. @ 07/29/23 13:39:18.105
• [0.084 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 07/29/23 13:39:18.111
  Jul 29 13:39:18.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:39:18.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:18.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:18.13
  STEP: Creating projection with secret that has name projected-secret-test-4f40b211-2599-4f2a-a92b-411b0c47bee7 @ 07/29/23 13:39:18.133
  STEP: Creating a pod to test consume secrets @ 07/29/23 13:39:18.138
  E0729 13:39:18.818672      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:19.818939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:20.819019      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:21.819968      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:39:22.158
  Jul 29 13:39:22.160: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-projected-secrets-dbb7515b-8d0e-4cd8-b142-55cbe5e040a5 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 13:39:22.166
  Jul 29 13:39:22.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-266" for this suite. @ 07/29/23 13:39:22.187
• [4.082 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 07/29/23 13:39:22.193
  Jul 29 13:39:22.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 13:39:22.194
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:22.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:22.215
  STEP: create deployment with httpd image @ 07/29/23 13:39:22.218
  Jul 29 13:39:22.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-2921 create -f -'
  Jul 29 13:39:22.512: INFO: stderr: ""
  Jul 29 13:39:22.512: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 07/29/23 13:39:22.513
  Jul 29 13:39:22.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-2921 diff -f -'
  E0729 13:39:22.820809      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:39:23.194: INFO: rc: 1
  Jul 29 13:39:23.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-2921 delete -f -'
  Jul 29 13:39:23.262: INFO: stderr: ""
  Jul 29 13:39:23.262: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jul 29 13:39:23.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2921" for this suite. @ 07/29/23 13:39:23.267
• [1.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 07/29/23 13:39:23.277
  Jul 29 13:39:23.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl-logs @ 07/29/23 13:39:23.277
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:23.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:23.308
  STEP: creating an pod @ 07/29/23 13:39:23.313
  Jul 29 13:39:23.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-logs-5723 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jul 29 13:39:23.402: INFO: stderr: ""
  Jul 29 13:39:23.402: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 07/29/23 13:39:23.402
  Jul 29 13:39:23.402: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0729 13:39:23.821084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:24.821174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:39:25.410: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 07/29/23 13:39:25.41
  Jul 29 13:39:25.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-logs-5723 logs logs-generator logs-generator'
  Jul 29 13:39:25.477: INFO: stderr: ""
  Jul 29 13:39:25.477: INFO: stdout: "I0729 13:39:24.121421       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/729 392\nI0729 13:39:24.321536       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/wtt 444\nI0729 13:39:24.521857       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/tk7 442\nI0729 13:39:24.722146       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/2zq 443\nI0729 13:39:24.922250       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/qjv 300\nI0729 13:39:25.121488       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/6b4 281\nI0729 13:39:25.321787       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/n92b 584\n"
  STEP: limiting log lines @ 07/29/23 13:39:25.477
  Jul 29 13:39:25.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-logs-5723 logs logs-generator logs-generator --tail=1'
  Jul 29 13:39:25.555: INFO: stderr: ""
  Jul 29 13:39:25.555: INFO: stdout: "I0729 13:39:25.522082       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vf44 314\n"
  Jul 29 13:39:25.555: INFO: got output "I0729 13:39:25.522082       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vf44 314\n"
  STEP: limiting log bytes @ 07/29/23 13:39:25.555
  Jul 29 13:39:25.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-logs-5723 logs logs-generator logs-generator --limit-bytes=1'
  Jul 29 13:39:25.624: INFO: stderr: ""
  Jul 29 13:39:25.624: INFO: stdout: "I"
  Jul 29 13:39:25.624: INFO: got output "I"
  STEP: exposing timestamps @ 07/29/23 13:39:25.624
  Jul 29 13:39:25.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-logs-5723 logs logs-generator logs-generator --tail=1 --timestamps'
  Jul 29 13:39:25.691: INFO: stderr: ""
  Jul 29 13:39:25.691: INFO: stdout: "2023-07-29T13:39:25.522190155Z I0729 13:39:25.522082       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vf44 314\n"
  Jul 29 13:39:25.691: INFO: got output "2023-07-29T13:39:25.522190155Z I0729 13:39:25.522082       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vf44 314\n"
  STEP: restricting to a time range @ 07/29/23 13:39:25.691
  E0729 13:39:25.821777      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:26.822108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:27.822211      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:39:28.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-logs-5723 logs logs-generator logs-generator --since=1s'
  Jul 29 13:39:28.258: INFO: stderr: ""
  Jul 29 13:39:28.258: INFO: stdout: "I0729 13:39:27.322247       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/rc45 320\nI0729 13:39:27.521496       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/s9jf 207\nI0729 13:39:27.721807       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/5t7 385\nI0729 13:39:27.922132       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/4gj5 240\nI0729 13:39:28.122250       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/q74p 201\n"
  Jul 29 13:39:28.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-logs-5723 logs logs-generator logs-generator --since=24h'
  Jul 29 13:39:28.325: INFO: stderr: ""
  Jul 29 13:39:28.325: INFO: stdout: "I0729 13:39:24.121421       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/729 392\nI0729 13:39:24.321536       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/wtt 444\nI0729 13:39:24.521857       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/tk7 442\nI0729 13:39:24.722146       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/2zq 443\nI0729 13:39:24.922250       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/qjv 300\nI0729 13:39:25.121488       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/6b4 281\nI0729 13:39:25.321787       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/n92b 584\nI0729 13:39:25.522082       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/vf44 314\nI0729 13:39:25.722254       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/l2g6 332\nI0729 13:39:25.921510       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/lkgt 323\nI0729 13:39:26.121800       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/hlgf 283\nI0729 13:39:26.322092       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/27s 551\nI0729 13:39:26.522250       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/c44 532\nI0729 13:39:26.721481       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/l77 328\nI0729 13:39:26.921782       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/mg2g 200\nI0729 13:39:27.122073       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/c64 408\nI0729 13:39:27.322247       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/rc45 320\nI0729 13:39:27.521496       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/s9jf 207\nI0729 13:39:27.721807       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/5t7 385\nI0729 13:39:27.922132       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/4gj5 240\nI0729 13:39:28.122250       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/q74p 201\nI0729 13:39:28.321493       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/xwm4 553\n"
  Jul 29 13:39:28.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-logs-5723 delete pod logs-generator'
  E0729 13:39:28.822357      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:39:29.023: INFO: stderr: ""
  Jul 29 13:39:29.023: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jul 29 13:39:29.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-5723" for this suite. @ 07/29/23 13:39:29.027
• [5.757 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 07/29/23 13:39:29.034
  Jul 29 13:39:29.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 13:39:29.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:29.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:29.052
  Jul 29 13:39:29.058: INFO: Got root ca configmap in namespace "svcaccounts-2685"
  Jul 29 13:39:29.065: INFO: Deleted root ca configmap in namespace "svcaccounts-2685"
  STEP: waiting for a new root ca configmap created @ 07/29/23 13:39:29.565
  Jul 29 13:39:29.571: INFO: Recreated root ca configmap in namespace "svcaccounts-2685"
  Jul 29 13:39:29.576: INFO: Updated root ca configmap in namespace "svcaccounts-2685"
  E0729 13:39:29.822559      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for the root ca configmap reconciled @ 07/29/23 13:39:30.077
  Jul 29 13:39:30.080: INFO: Reconciled root ca configmap in namespace "svcaccounts-2685"
  Jul 29 13:39:30.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2685" for this suite. @ 07/29/23 13:39:30.086
• [1.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 07/29/23 13:39:30.094
  Jul 29 13:39:30.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 13:39:30.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:30.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:30.115
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/29/23 13:39:30.118
  E0729 13:39:30.822925      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:31.823982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:32.824842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:33.824939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:39:34.144
  Jul 29 13:39:34.150: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-06ae1ed2-c4a5-49af-afad-b08a9f0508bf container test-container: <nil>
  STEP: delete the pod @ 07/29/23 13:39:34.158
  Jul 29 13:39:34.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4099" for this suite. @ 07/29/23 13:39:34.178
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 07/29/23 13:39:34.185
  Jul 29 13:39:34.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename endpointslice @ 07/29/23 13:39:34.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:34.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:34.211
  Jul 29 13:39:34.223: INFO: Endpoints addresses: [172.31.18.12 172.31.85.196] , ports: [6443]
  Jul 29 13:39:34.223: INFO: EndpointSlices addresses: [172.31.18.12 172.31.85.196] , ports: [6443]
  Jul 29 13:39:34.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6555" for this suite. @ 07/29/23 13:39:34.227
• [0.048 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 07/29/23 13:39:34.234
  Jul 29 13:39:34.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:39:34.235
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:34.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:34.256
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 13:39:34.259
  E0729 13:39:34.825226      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:35.825323      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:36.826122      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:37.826201      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:39:38.278
  Jul 29 13:39:38.281: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-2b3d5879-9c89-446b-8a68-b7625dcc6760 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 13:39:38.289
  Jul 29 13:39:38.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1684" for this suite. @ 07/29/23 13:39:38.309
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 07/29/23 13:39:38.316
  Jul 29 13:39:38.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 13:39:38.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:38.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:38.335
  Jul 29 13:39:38.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  E0729 13:39:38.826271      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:39.827128      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:40.828001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:39:41.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5238" for this suite. @ 07/29/23 13:39:41.436
• [3.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 07/29/23 13:39:41.45
  Jul 29 13:39:41.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:39:41.451
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:41.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:41.471
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 13:39:41.474
  E0729 13:39:41.828057      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:42.828126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:43.828560      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:44.828851      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:39:45.495
  Jul 29 13:39:45.498: INFO: Trying to get logs from node ip-172-31-33-37 pod downwardapi-volume-6955c5c7-2eef-4023-aae2-28d1f44aeed3 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 13:39:45.505
  Jul 29 13:39:45.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6945" for this suite. @ 07/29/23 13:39:45.524
• [4.081 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 07/29/23 13:39:45.532
  Jul 29 13:39:45.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 13:39:45.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:45.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:45.553
  STEP: getting /apis @ 07/29/23 13:39:45.556
  STEP: getting /apis/node.k8s.io @ 07/29/23 13:39:45.56
  STEP: getting /apis/node.k8s.io/v1 @ 07/29/23 13:39:45.561
  STEP: creating @ 07/29/23 13:39:45.562
  STEP: watching @ 07/29/23 13:39:45.577
  Jul 29 13:39:45.578: INFO: starting watch
  STEP: getting @ 07/29/23 13:39:45.582
  STEP: listing @ 07/29/23 13:39:45.584
  STEP: patching @ 07/29/23 13:39:45.588
  STEP: updating @ 07/29/23 13:39:45.592
  Jul 29 13:39:45.597: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 07/29/23 13:39:45.597
  STEP: deleting a collection @ 07/29/23 13:39:45.609
  Jul 29 13:39:45.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7839" for this suite. @ 07/29/23 13:39:45.628
• [0.103 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 07/29/23 13:39:45.635
  Jul 29 13:39:45.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename subpath @ 07/29/23 13:39:45.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:39:45.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:39:45.654
  STEP: Setting up data @ 07/29/23 13:39:45.656
  STEP: Creating pod pod-subpath-test-downwardapi-l648 @ 07/29/23 13:39:45.666
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 13:39:45.666
  E0729 13:39:45.829096      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:46.829621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:47.829728      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:48.830238      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:49.830272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:50.830348      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:51.831299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:52.832003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:53.832235      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:54.832443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:55.832965      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:56.833414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:57.833861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:58.834027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:39:59.834819      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:00.834927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:01.835771      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:02.836588      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:03.837206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:04.837265      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:05.837815      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:06.838094      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:07.838180      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:08.838346      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:40:09.733
  Jul 29 13:40:09.736: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-subpath-test-downwardapi-l648 container test-container-subpath-downwardapi-l648: <nil>
  STEP: delete the pod @ 07/29/23 13:40:09.742
  STEP: Deleting pod pod-subpath-test-downwardapi-l648 @ 07/29/23 13:40:09.758
  Jul 29 13:40:09.758: INFO: Deleting pod "pod-subpath-test-downwardapi-l648" in namespace "subpath-9366"
  Jul 29 13:40:09.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9366" for this suite. @ 07/29/23 13:40:09.764
• [24.136 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 07/29/23 13:40:09.772
  Jul 29 13:40:09.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 13:40:09.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:40:09.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:40:09.794
  Jul 29 13:40:09.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3594" for this suite. @ 07/29/23 13:40:09.808
• [0.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 07/29/23 13:40:09.815
  Jul 29 13:40:09.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pods @ 07/29/23 13:40:09.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:40:09.836
  E0729 13:40:09.838448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:40:09.839
  STEP: creating the pod @ 07/29/23 13:40:09.842
  STEP: submitting the pod to kubernetes @ 07/29/23 13:40:09.842
  W0729 13:40:09.851936      18 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0729 13:40:10.838944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:11.839040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 07/29/23 13:40:11.862
  STEP: updating the pod @ 07/29/23 13:40:11.865
  Jul 29 13:40:12.378: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3ebbeb24-e6e0-4fa3-8cb1-aa2c9631dca9"
  E0729 13:40:12.839947      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:13.840844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:14.841613      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:15.841757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:40:16.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9127" for this suite. @ 07/29/23 13:40:16.395
• [6.586 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 07/29/23 13:40:16.402
  Jul 29 13:40:16.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/29/23 13:40:16.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:40:16.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:40:16.423
  STEP: creating @ 07/29/23 13:40:16.426
  STEP: getting @ 07/29/23 13:40:16.441
  STEP: listing @ 07/29/23 13:40:16.448
  STEP: deleting @ 07/29/23 13:40:16.45
  Jul 29 13:40:16.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-1196" for this suite. @ 07/29/23 13:40:16.471
• [0.074 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 07/29/23 13:40:16.477
  Jul 29 13:40:16.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename events @ 07/29/23 13:40:16.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:40:16.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:40:16.5
  STEP: Create set of events @ 07/29/23 13:40:16.505
  STEP: get a list of Events with a label in the current namespace @ 07/29/23 13:40:16.521
  STEP: delete a list of events @ 07/29/23 13:40:16.525
  Jul 29 13:40:16.525: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/29/23 13:40:16.545
  Jul 29 13:40:16.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7284" for this suite. @ 07/29/23 13:40:16.551
• [0.079 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 07/29/23 13:40:16.557
  Jul 29 13:40:16.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 13:40:16.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:40:16.573
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:40:16.576
  STEP: Counting existing ResourceQuota @ 07/29/23 13:40:16.578
  E0729 13:40:16.841791      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:17.841850      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:18.842534      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:19.843598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:20.844607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/29/23 13:40:21.586
  STEP: Ensuring resource quota status is calculated @ 07/29/23 13:40:21.59
  E0729 13:40:21.845645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:22.846129      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:40:23.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7531" for this suite. @ 07/29/23 13:40:23.598
• [7.048 seconds]
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 07/29/23 13:40:23.605
  Jul 29 13:40:23.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename sched-pred @ 07/29/23 13:40:23.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:40:23.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:40:23.627
  Jul 29 13:40:23.633: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 29 13:40:23.640: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 13:40:23.642: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-19-67 before test
  Jul 29 13:40:23.648: INFO: nginx-ingress-controller-kubernetes-worker-gllr9 from ingress-nginx-kubernetes-worker started at 2023-07-29 12:05:42 +0000 UTC (1 container statuses recorded)
  Jul 29 13:40:23.648: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 29 13:40:23.648: INFO: calico-kube-controllers-66cf5c7c9b-lp29z from kube-system started at 2023-07-29 12:05:47 +0000 UTC (1 container statuses recorded)
  Jul 29 13:40:23.648: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul 29 13:40:23.648: INFO: sonobuoy-e2e-job-fe057c2caff44e9c from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:40:23.648: INFO: 	Container e2e ready: true, restart count 0
  Jul 29 13:40:23.648: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:40:23.648: INFO: sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-7dv7m from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:40:23.648: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:40:23.648: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 13:40:23.648: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-33-37 before test
  Jul 29 13:40:23.652: INFO: nginx-ingress-controller-kubernetes-worker-zhwpz from ingress-nginx-kubernetes-worker started at 2023-07-29 13:34:13 +0000 UTC (1 container statuses recorded)
  Jul 29 13:40:23.652: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 29 13:40:23.652: INFO: sonobuoy from sonobuoy started at 2023-07-29 12:12:09 +0000 UTC (1 container statuses recorded)
  Jul 29 13:40:23.652: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 29 13:40:23.652: INFO: sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-fxbms from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:40:23.652: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:40:23.652: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 13:40:23.652: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-5-66 before test
  Jul 29 13:40:23.657: INFO: default-http-backend-kubernetes-worker-65fc475d49-fwzg8 from ingress-nginx-kubernetes-worker started at 2023-07-29 11:57:02 +0000 UTC (1 container statuses recorded)
  Jul 29 13:40:23.657: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul 29 13:40:23.657: INFO: nginx-ingress-controller-kubernetes-worker-hbv24 from ingress-nginx-kubernetes-worker started at 2023-07-29 11:57:02 +0000 UTC (1 container statuses recorded)
  Jul 29 13:40:23.658: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 29 13:40:23.658: INFO: coredns-5c7f76ccb8-7mf4p from kube-system started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:40:23.658: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 13:40:23.658: INFO: kube-state-metrics-5b95b4459c-849m5 from kube-system started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:40:23.658: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul 29 13:40:23.658: INFO: metrics-server-v0.5.2-6cf8c8b69c-slzlr from kube-system started at 2023-07-29 11:55:52 +0000 UTC (2 container statuses recorded)
  Jul 29 13:40:23.658: INFO: 	Container metrics-server ready: true, restart count 0
  Jul 29 13:40:23.658: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul 29 13:40:23.658: INFO: dashboard-metrics-scraper-6b8586b5c9-6s95z from kubernetes-dashboard started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:40:23.658: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul 29 13:40:23.658: INFO: kubernetes-dashboard-6869f4cd5f-fwg5d from kubernetes-dashboard started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:40:23.658: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul 29 13:40:23.658: INFO: sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-n84sm from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:40:23.658: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:40:23.658: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/29/23 13:40:23.658
  E0729 13:40:23.846693      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:24.847379      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/29/23 13:40:25.678
  STEP: Trying to apply a random label on the found node. @ 07/29/23 13:40:25.694
  STEP: verifying the node has the label kubernetes.io/e2e-7a0ea38c-64e2-4342-a6e7-c4683356a190 95 @ 07/29/23 13:40:25.704
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 07/29/23 13:40:25.706
  E0729 13:40:25.847734      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:26.847809      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.33.37 on the node which pod4 resides and expect not scheduled @ 07/29/23 13:40:27.718
  E0729 13:40:27.848352      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:28.848440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:29.848534      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:30.848697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:31.849541      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:32.849724      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:33.850640      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:34.851299      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:35.852179      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:36.852349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:37.852446      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:38.852548      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:39.853375      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:40.853557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:41.853762      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:42.854098      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:43.854993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:44.855087      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:45.855480      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:46.855842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:47.856888      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:48.857134      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:49.857444      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:50.858077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:51.858120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:52.858321      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:53.859239      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:54.859994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:55.860070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:56.860570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:57.861009      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:58.861100      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:40:59.861266      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:00.861466      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:01.861698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:02.861922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:03.862514      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:04.862730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:05.863752      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:06.864158      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:07.864386      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:08.864484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:09.864855      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:10.865059      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:11.865895      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:12.865997      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:13.866742      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:14.866929      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:15.867224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:16.867972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:17.868303      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:18.868496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:19.868786      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:20.869212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:21.869955      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:22.870160      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:23.871066      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:24.871981      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:25.872419      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:26.872547      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:27.873078      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:28.873159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:29.873597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:30.874611      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:31.875484      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:32.876525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:33.877263      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:34.877798      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:35.878304      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:36.878684      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:37.878833      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:38.878993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:39.879243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:40.879952      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:41.880682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:42.881353      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:43.881567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:44.881664      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:45.881741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:46.882232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:47.882427      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:48.882980      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:49.883991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:50.884085      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:51.885055      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:52.885143      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:53.886206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:54.886311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:55.886406      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:56.886923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:57.887988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:58.888352      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:41:59.888428      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:00.888696      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:01.889657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:02.889768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:03.889944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:04.890040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:05.890217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:06.890488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:07.890650      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:08.890898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:09.891003      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:10.891962      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:11.892248      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:12.892562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:13.892757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:14.892982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:15.893152      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:16.894113      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:17.894254      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:18.894880      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:19.894908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:20.895370      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:21.895522      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:22.895606      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:23.895926      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:24.896090      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:25.897084      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:26.898150      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:27.898239      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:28.898319      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:29.899147      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:30.899812      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:31.900040      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:32.900356      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:33.900439      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:34.901077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:35.901394      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:36.901467      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:37.901657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:38.901699      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:39.902213      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:40.902854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:41.902904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:42.902999      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:43.903965      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:44.905002      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:45.905250      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:46.906219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:47.906311      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:48.906403      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:49.906548      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:50.907148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:51.907242      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:52.907966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:53.908138      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:54.908289      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:55.909236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:56.909327      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:57.909526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:58.910541      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:42:59.910725      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:00.910899      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:01.911975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:02.912227      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:03.912647      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:04.912739      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:05.912931      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:06.913975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:07.914097      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:08.914183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:09.914830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:10.914919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:11.915967      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:12.916121      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:13.917100      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:14.917421      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:15.917608      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:16.918188      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:17.918372      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:18.919062      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:19.919166      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:20.920114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:21.920296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:22.921191      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:23.921283      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:24.921376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:25.921483      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:26.922210      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:27.922376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:28.922670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:29.922827      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:30.922908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:31.923959      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:32.924680      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:33.924875      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:34.925039      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:35.925182      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:36.925992      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:37.926080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:38.926155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:39.927220      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:40.927963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:41.928222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:42.928315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:43.928485      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:44.928573      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:45.929099      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:46.930060      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:47.930224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:48.930587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:49.930741      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:50.930904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:51.930991      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:52.931964      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:53.932127      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:54.932256      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:55.932713      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:56.932803      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:57.932963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:58.933648      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:43:59.934391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:00.934900      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:01.934988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:02.935976      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:03.936137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:04.936258      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:05.937109      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:06.937364      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:07.937905      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:08.937854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:09.938899      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:10.939965      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:11.940232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:12.940476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:13.940640      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:14.941120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:15.941313      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:16.941912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:17.942647      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:18.943557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:19.943645      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:20.943757      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:21.944518      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:22.944606      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:23.944697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:24.944787      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:25.944973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:26.945255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:27.945462      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:28.945594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:29.945881      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:30.946873      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:31.946910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:32.947963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:33.948404      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:34.949326      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:35.950294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:36.950898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:37.950992      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:38.951971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:39.952157      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:40.952824      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:41.953763      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:42.954650      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:43.954747      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:44.955245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:45.955333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:46.955963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:47.956070      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:48.956448      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:49.956643      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:50.957263      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:51.957556      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:52.958310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:53.958420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:54.958515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:55.958603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:56.958907      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:57.959967      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:58.960987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:44:59.961085      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:00.961470      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:01.961499      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:02.962511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:03.962595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:04.962607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:05.963016      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:06.963989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:07.964155      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:08.964243      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:09.964515      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:10.965279      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:11.965479      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:12.965911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:13.966593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:14.966683      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:15.967577      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:16.967974      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:17.968156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:18.969137      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:19.969232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:20.970176      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:21.970459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:22.971133      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:23.972028      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:24.972830      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:25.972874      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:26.973465      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-7a0ea38c-64e2-4342-a6e7-c4683356a190 off the node ip-172-31-33-37 @ 07/29/23 13:45:27.728
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-7a0ea38c-64e2-4342-a6e7-c4683356a190 @ 07/29/23 13:45:27.74
  Jul 29 13:45:27.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8441" for this suite. @ 07/29/23 13:45:27.749
• [304.149 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 07/29/23 13:45:27.755
  Jul 29 13:45:27.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 13:45:27.756
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:45:27.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:45:27.776
  STEP: Setting up server cert @ 07/29/23 13:45:27.8
  E0729 13:45:27.973778      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 13:45:28.281
  STEP: Deploying the webhook pod @ 07/29/23 13:45:28.289
  STEP: Wait for the deployment to be ready @ 07/29/23 13:45:28.301
  Jul 29 13:45:28.307: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0729 13:45:28.974560      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:29.975043      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 13:45:30.318
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 13:45:30.327
  E0729 13:45:30.975976      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:45:31.327: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 07/29/23 13:45:31.33
  STEP: Creating a custom resource definition that should be denied by the webhook @ 07/29/23 13:45:31.345
  Jul 29 13:45:31.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  Jul 29 13:45:31.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-363" for this suite. @ 07/29/23 13:45:31.402
  STEP: Destroying namespace "webhook-markers-749" for this suite. @ 07/29/23 13:45:31.411
• [3.662 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 07/29/23 13:45:31.418
  Jul 29 13:45:31.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 13:45:31.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:45:31.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:45:31.437
  STEP: creating a replication controller @ 07/29/23 13:45:31.44
  Jul 29 13:45:31.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 create -f -'
  Jul 29 13:45:31.782: INFO: stderr: ""
  Jul 29 13:45:31.782: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/29/23 13:45:31.782
  Jul 29 13:45:31.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 13:45:31.850: INFO: stderr: ""
  Jul 29 13:45:31.850: INFO: stdout: "update-demo-nautilus-q6xd2 update-demo-nautilus-v7kxh "
  Jul 29 13:45:31.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods update-demo-nautilus-q6xd2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 13:45:31.912: INFO: stderr: ""
  Jul 29 13:45:31.912: INFO: stdout: ""
  Jul 29 13:45:31.912: INFO: update-demo-nautilus-q6xd2 is created but not running
  E0729 13:45:31.976106      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:32.976330      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:33.976440      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:34.976709      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:35.976736      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:45:36.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0729 13:45:36.976985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:45:36.984: INFO: stderr: ""
  Jul 29 13:45:36.984: INFO: stdout: "update-demo-nautilus-q6xd2 update-demo-nautilus-v7kxh "
  Jul 29 13:45:36.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods update-demo-nautilus-q6xd2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 13:45:37.047: INFO: stderr: ""
  Jul 29 13:45:37.047: INFO: stdout: "true"
  Jul 29 13:45:37.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods update-demo-nautilus-q6xd2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 13:45:37.108: INFO: stderr: ""
  Jul 29 13:45:37.108: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 13:45:37.108: INFO: validating pod update-demo-nautilus-q6xd2
  Jul 29 13:45:37.114: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 13:45:37.114: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 13:45:37.114: INFO: update-demo-nautilus-q6xd2 is verified up and running
  Jul 29 13:45:37.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods update-demo-nautilus-v7kxh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 13:45:37.174: INFO: stderr: ""
  Jul 29 13:45:37.174: INFO: stdout: "true"
  Jul 29 13:45:37.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods update-demo-nautilus-v7kxh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 13:45:37.236: INFO: stderr: ""
  Jul 29 13:45:37.236: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 13:45:37.236: INFO: validating pod update-demo-nautilus-v7kxh
  Jul 29 13:45:37.241: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 13:45:37.241: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 13:45:37.241: INFO: update-demo-nautilus-v7kxh is verified up and running
  STEP: scaling down the replication controller @ 07/29/23 13:45:37.241
  Jul 29 13:45:37.243: INFO: scanned /root for discovery docs: <nil>
  Jul 29 13:45:37.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0729 13:45:37.977056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:45:38.321: INFO: stderr: ""
  Jul 29 13:45:38.321: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/29/23 13:45:38.321
  Jul 29 13:45:38.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 13:45:38.385: INFO: stderr: ""
  Jul 29 13:45:38.385: INFO: stdout: "update-demo-nautilus-q6xd2 update-demo-nautilus-v7kxh "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 07/29/23 13:45:38.385
  E0729 13:45:38.977462      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:39.977544      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:40.977723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:41.977923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:42.978011      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:45:43.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 13:45:43.446: INFO: stderr: ""
  Jul 29 13:45:43.446: INFO: stdout: "update-demo-nautilus-q6xd2 "
  Jul 29 13:45:43.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods update-demo-nautilus-q6xd2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 13:45:43.508: INFO: stderr: ""
  Jul 29 13:45:43.508: INFO: stdout: "true"
  Jul 29 13:45:43.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods update-demo-nautilus-q6xd2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 13:45:43.567: INFO: stderr: ""
  Jul 29 13:45:43.567: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 13:45:43.567: INFO: validating pod update-demo-nautilus-q6xd2
  Jul 29 13:45:43.573: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 13:45:43.573: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 13:45:43.573: INFO: update-demo-nautilus-q6xd2 is verified up and running
  STEP: scaling up the replication controller @ 07/29/23 13:45:43.573
  Jul 29 13:45:43.575: INFO: scanned /root for discovery docs: <nil>
  Jul 29 13:45:43.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0729 13:45:43.978781      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:45:44.658: INFO: stderr: ""
  Jul 29 13:45:44.658: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/29/23 13:45:44.658
  Jul 29 13:45:44.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 13:45:44.723: INFO: stderr: ""
  Jul 29 13:45:44.723: INFO: stdout: "update-demo-nautilus-q6xd2 update-demo-nautilus-rw8kt "
  Jul 29 13:45:44.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods update-demo-nautilus-q6xd2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 13:45:44.781: INFO: stderr: ""
  Jul 29 13:45:44.781: INFO: stdout: "true"
  Jul 29 13:45:44.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods update-demo-nautilus-q6xd2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 13:45:44.840: INFO: stderr: ""
  Jul 29 13:45:44.840: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 13:45:44.840: INFO: validating pod update-demo-nautilus-q6xd2
  Jul 29 13:45:44.843: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 13:45:44.843: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 13:45:44.843: INFO: update-demo-nautilus-q6xd2 is verified up and running
  Jul 29 13:45:44.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods update-demo-nautilus-rw8kt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 13:45:44.903: INFO: stderr: ""
  Jul 29 13:45:44.903: INFO: stdout: "true"
  Jul 29 13:45:44.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods update-demo-nautilus-rw8kt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 13:45:44.966: INFO: stderr: ""
  Jul 29 13:45:44.966: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 13:45:44.966: INFO: validating pod update-demo-nautilus-rw8kt
  Jul 29 13:45:44.971: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 13:45:44.971: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 13:45:44.971: INFO: update-demo-nautilus-rw8kt is verified up and running
  STEP: using delete to clean up resources @ 07/29/23 13:45:44.971
  Jul 29 13:45:44.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 delete --grace-period=0 --force -f -'
  E0729 13:45:44.979441      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:45:45.043: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 13:45:45.043: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul 29 13:45:45.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get rc,svc -l name=update-demo --no-headers'
  Jul 29 13:45:45.140: INFO: stderr: "No resources found in kubectl-6751 namespace.\n"
  Jul 29 13:45:45.140: INFO: stdout: ""
  Jul 29 13:45:45.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6751 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 29 13:45:45.239: INFO: stderr: ""
  Jul 29 13:45:45.239: INFO: stdout: ""
  Jul 29 13:45:45.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6751" for this suite. @ 07/29/23 13:45:45.245
• [13.832 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 07/29/23 13:45:45.251
  Jul 29 13:45:45.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 13:45:45.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:45:45.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:45:45.271
  STEP: Creating service test in namespace statefulset-2749 @ 07/29/23 13:45:45.274
  Jul 29 13:45:45.288: INFO: Found 0 stateful pods, waiting for 1
  E0729 13:45:45.979710      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:46.980105      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:47.980312      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:48.980413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:49.980564      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:50.980760      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:51.981011      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:52.982050      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:53.982548      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:54.982664      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:45:55.292: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 07/29/23 13:45:55.298
  W0729 13:45:55.308911      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 29 13:45:55.315: INFO: Found 1 stateful pods, waiting for 2
  E0729 13:45:55.983104      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:56.983533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:57.983601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:58.983994      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:45:59.984176      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:00.984385      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:01.984657      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:02.984760      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:03.984847      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:04.985035      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:46:05.321: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 13:46:05.321: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 07/29/23 13:46:05.329
  STEP: Delete all of the StatefulSets @ 07/29/23 13:46:05.333
  STEP: Verify that StatefulSets have been deleted @ 07/29/23 13:46:05.341
  Jul 29 13:46:05.344: INFO: Deleting all statefulset in ns statefulset-2749
  Jul 29 13:46:05.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2749" for this suite. @ 07/29/23 13:46:05.36
• [20.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 07/29/23 13:46:05.371
  Jul 29 13:46:05.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 13:46:05.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:46:05.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:46:05.396
  E0729 13:46:05.985735      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:06.986052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:46:07.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-642" for this suite. @ 07/29/23 13:46:07.428
• [2.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 07/29/23 13:46:07.438
  Jul 29 13:46:07.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 13:46:07.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:46:07.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:46:07.458
  STEP: validating cluster-info @ 07/29/23 13:46:07.46
  Jul 29 13:46:07.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-6646 cluster-info'
  Jul 29 13:46:07.522: INFO: stderr: ""
  Jul 29 13:46:07.523: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jul 29 13:46:07.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6646" for this suite. @ 07/29/23 13:46:07.527
• [0.096 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 07/29/23 13:46:07.534
  Jul 29 13:46:07.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pods @ 07/29/23 13:46:07.535
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:46:07.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:46:07.555
  STEP: creating pod @ 07/29/23 13:46:07.558
  E0729 13:46:07.986086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:08.986418      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:46:09.583: INFO: Pod pod-hostip-c4117551-2c38-4fcf-974e-2df036516d6d has hostIP: 172.31.33.37
  Jul 29 13:46:09.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3671" for this suite. @ 07/29/23 13:46:09.587
• [2.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 07/29/23 13:46:09.595
  Jul 29 13:46:09.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:46:09.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:46:09.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:46:09.612
  STEP: Creating configMap with name projected-configmap-test-volume-b8eb196b-c4d2-4a5c-b65e-f52f3c424bcd @ 07/29/23 13:46:09.619
  STEP: Creating a pod to test consume configMaps @ 07/29/23 13:46:09.622
  E0729 13:46:09.986528      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:10.986821      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:11.987245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:12.988035      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:46:13.643
  Jul 29 13:46:13.646: INFO: Trying to get logs from node ip-172-31-19-67 pod pod-projected-configmaps-b0c95c77-acaa-4a06-9757-9a40fa341d46 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 13:46:13.665
  Jul 29 13:46:13.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6739" for this suite. @ 07/29/23 13:46:13.683
• [4.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 07/29/23 13:46:13.691
  Jul 29 13:46:13.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:46:13.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:46:13.709
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:46:13.712
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-f384d100-cbc8-4b2c-8ffc-7bb9b9debd7e @ 07/29/23 13:46:13.719
  STEP: Creating the pod @ 07/29/23 13:46:13.723
  E0729 13:46:13.989142      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:14.989101      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-f384d100-cbc8-4b2c-8ffc-7bb9b9debd7e @ 07/29/23 13:46:15.763
  STEP: waiting to observe update in volume @ 07/29/23 13:46:15.768
  E0729 13:46:15.989754      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:16.990316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:17.991290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:18.991993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:19.992979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:20.993184      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:21.994149      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:22.994255      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:23.994865      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:24.994933      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:25.995602      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:26.995760      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:27.996227      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:28.996320      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:29.996744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:30.996854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:31.997854      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:32.998038      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:33.998822      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:34.998978      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:35.999206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:36.999306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:38.000001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:39.000079      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:40.000310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:41.000413      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:42.001391      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:43.001597      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:44.002205      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:45.002290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:46.003124      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:47.003971      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:48.004492      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:49.004677      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:50.005294      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:51.005525      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:52.006567      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:53.007511      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:54.008364      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:55.008461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:56.009443      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:57.009837      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:58.010219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:46:59.010309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:00.010767      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:01.010911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:02.011149      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:03.011977      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:04.012290      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:05.013160      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:06.013946      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:07.014301      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:08.014424      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:09.014855      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:10.015100      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:11.015989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:12.016253      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:13.016822      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:14.017010      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:15.017132      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:16.017595      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:17.017664      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:18.017860      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:19.017950      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:20.018041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:21.018922      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:22.018983      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:23.019069      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:24.019985      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:25.020682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:26.020768      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:27.021659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:28.022447      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:29.023469      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:30.023562      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:31.023942      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:32.024214      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:33.025015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:34.025223      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:35.025310      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:36.025520      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:37.026242      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:38.026349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:39.026988      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:40.027986      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:41.028080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:42.028690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:43.028806      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:44.028894      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:45.029883      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:46.030496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:47:46.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8220" for this suite. @ 07/29/23 13:47:46.165
• [92.481 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 07/29/23 13:47:46.172
  Jul 29 13:47:46.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename disruption @ 07/29/23 13:47:46.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:47:46.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:47:46.192
  STEP: Creating a kubernetes client @ 07/29/23 13:47:46.195
  Jul 29 13:47:46.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename disruption-2 @ 07/29/23 13:47:46.196
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:47:46.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:47:46.213
  STEP: Waiting for the pdb to be processed @ 07/29/23 13:47:46.224
  E0729 13:47:47.030592      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:48.030682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 07/29/23 13:47:48.236
  E0729 13:47:49.030920      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:50.031002      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 07/29/23 13:47:50.249
  E0729 13:47:51.031973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:52.032344      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 07/29/23 13:47:52.257
  STEP: listing a collection of PDBs in namespace disruption-6864 @ 07/29/23 13:47:52.26
  STEP: deleting a collection of PDBs @ 07/29/23 13:47:52.263
  STEP: Waiting for the PDB collection to be deleted @ 07/29/23 13:47:52.275
  Jul 29 13:47:52.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 13:47:52.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-2596" for this suite. @ 07/29/23 13:47:52.284
  STEP: Destroying namespace "disruption-6864" for this suite. @ 07/29/23 13:47:52.29
• [6.124 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 07/29/23 13:47:52.297
  Jul 29 13:47:52.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pods @ 07/29/23 13:47:52.298
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:47:52.315
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:47:52.318
  STEP: creating a Pod with a static label @ 07/29/23 13:47:52.325
  STEP: watching for Pod to be ready @ 07/29/23 13:47:52.333
  Jul 29 13:47:52.334: INFO: observed Pod pod-test in namespace pods-760 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jul 29 13:47:52.337: INFO: observed Pod pod-test in namespace pods-760 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 13:47:52 +0000 UTC  }]
  Jul 29 13:47:52.359: INFO: observed Pod pod-test in namespace pods-760 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 13:47:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 13:47:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 13:47:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 13:47:52 +0000 UTC  }]
  E0729 13:47:53.032919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:47:53.912: INFO: Found Pod pod-test in namespace pods-760 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 13:47:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 13:47:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 13:47:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 13:47:52 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 07/29/23 13:47:53.915
  STEP: getting the Pod and ensuring that it's patched @ 07/29/23 13:47:53.926
  STEP: replacing the Pod's status Ready condition to False @ 07/29/23 13:47:53.93
  STEP: check the Pod again to ensure its Ready conditions are False @ 07/29/23 13:47:53.941
  STEP: deleting the Pod via a Collection with a LabelSelector @ 07/29/23 13:47:53.941
  STEP: watching for the Pod to be deleted @ 07/29/23 13:47:53.949
  Jul 29 13:47:53.950: INFO: observed event type MODIFIED
  E0729 13:47:54.033889      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:55.034158      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:47:55.772: INFO: observed event type MODIFIED
  Jul 29 13:47:55.915: INFO: observed event type MODIFIED
  E0729 13:47:56.034925      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:47:56.211: INFO: observed event type MODIFIED
  Jul 29 13:47:56.924: INFO: observed event type MODIFIED
  Jul 29 13:47:56.940: INFO: observed event type MODIFIED
  Jul 29 13:47:56.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-760" for this suite. @ 07/29/23 13:47:56.952
• [4.660 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 07/29/23 13:47:56.958
  Jul 29 13:47:56.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 13:47:56.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:47:56.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:47:56.979
  STEP: Creating configMap with name configmap-test-volume-2a76e5a2-a6b2-46ac-b356-d0a9bdb3f3b0 @ 07/29/23 13:47:56.982
  STEP: Creating a pod to test consume configMaps @ 07/29/23 13:47:56.987
  E0729 13:47:57.035376      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:58.035490      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:47:59.035914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:00.036227      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:48:01.007
  Jul 29 13:48:01.011: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-configmaps-b764db55-7dbd-4fcd-ad96-d7527a7238b6 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 13:48:01.018
  Jul 29 13:48:01.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3851" for this suite. @ 07/29/23 13:48:01.034
  E0729 13:48:01.036217      18 retrywatcher.go:130] "Watch failed" err="context canceled"
• [4.082 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 07/29/23 13:48:01.041
  Jul 29 13:48:01.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename pods @ 07/29/23 13:48:01.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:48:01.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:48:01.062
  STEP: creating the pod @ 07/29/23 13:48:01.065
  STEP: submitting the pod to kubernetes @ 07/29/23 13:48:01.065
  E0729 13:48:02.036527      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:03.036632      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 07/29/23 13:48:03.087
  STEP: updating the pod @ 07/29/23 13:48:03.091
  Jul 29 13:48:03.602: INFO: Successfully updated pod "pod-update-796a4823-1798-49b9-a325-6df92ea61aac"
  STEP: verifying the updated pod is in kubernetes @ 07/29/23 13:48:03.606
  Jul 29 13:48:03.610: INFO: Pod update OK
  Jul 29 13:48:03.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3402" for this suite. @ 07/29/23 13:48:03.613
• [2.578 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 07/29/23 13:48:03.62
  Jul 29 13:48:03.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename server-version @ 07/29/23 13:48:03.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:48:03.638
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:48:03.641
  STEP: Request ServerVersion @ 07/29/23 13:48:03.644
  STEP: Confirm major version @ 07/29/23 13:48:03.645
  Jul 29 13:48:03.645: INFO: Major version: 1
  STEP: Confirm minor version @ 07/29/23 13:48:03.645
  Jul 29 13:48:03.645: INFO: cleanMinorVersion: 27
  Jul 29 13:48:03.645: INFO: Minor version: 27
  Jul 29 13:48:03.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-3879" for this suite. @ 07/29/23 13:48:03.648
• [0.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 07/29/23 13:48:03.656
  Jul 29 13:48:03.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 13:48:03.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:48:03.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:48:03.676
  STEP: Waiting for pod completion @ 07/29/23 13:48:03.688
  E0729 13:48:04.036723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:05.037467      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:06.037578      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:07.037976      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:48:07.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5718" for this suite. @ 07/29/23 13:48:07.713
• [4.063 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 07/29/23 13:48:07.719
  Jul 29 13:48:07.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename webhook @ 07/29/23 13:48:07.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:48:07.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:48:07.742
  STEP: Setting up server cert @ 07/29/23 13:48:07.828
  E0729 13:48:08.038144      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 13:48:08.293
  STEP: Deploying the webhook pod @ 07/29/23 13:48:08.301
  STEP: Wait for the deployment to be ready @ 07/29/23 13:48:08.312
  Jul 29 13:48:08.318: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0729 13:48:09.038730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:10.038826      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 13:48:10.329
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 13:48:10.336
  E0729 13:48:11.039414      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:48:11.337: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/29/23 13:48:11.342
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/29/23 13:48:11.359
  STEP: Creating a dummy validating-webhook-configuration object @ 07/29/23 13:48:11.373
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 07/29/23 13:48:11.381
  STEP: Creating a dummy mutating-webhook-configuration object @ 07/29/23 13:48:11.388
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 07/29/23 13:48:11.396
  Jul 29 13:48:11.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3686" for this suite. @ 07/29/23 13:48:11.455
  STEP: Destroying namespace "webhook-markers-5247" for this suite. @ 07/29/23 13:48:11.462
• [3.748 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 07/29/23 13:48:11.469
  Jul 29 13:48:11.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 13:48:11.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:48:11.504
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:48:11.507
  STEP: Creating a pod to test downward api env vars @ 07/29/23 13:48:11.511
  E0729 13:48:12.039635      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:13.040552      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:14.040574      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:15.040669      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:48:15.535
  Jul 29 13:48:15.537: INFO: Trying to get logs from node ip-172-31-33-37 pod downward-api-52db7042-0d23-4fe4-81e7-f8d349e1fea9 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 13:48:15.546
  Jul 29 13:48:15.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7068" for this suite. @ 07/29/23 13:48:15.565
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 07/29/23 13:48:15.571
  Jul 29 13:48:15.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 13:48:15.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:48:15.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:48:15.605
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 07/29/23 13:48:15.609
  E0729 13:48:16.041659      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:17.042128      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:18.042769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:19.042910      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:48:19.634
  Jul 29 13:48:19.637: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-cc9ddbd7-7b2b-417b-9baf-5507def41482 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 13:48:19.645
  Jul 29 13:48:19.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2616" for this suite. @ 07/29/23 13:48:19.664
• [4.100 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 07/29/23 13:48:19.671
  Jul 29 13:48:19.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename subpath @ 07/29/23 13:48:19.672
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:48:19.689
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:48:19.691
  STEP: Setting up data @ 07/29/23 13:48:19.694
  STEP: Creating pod pod-subpath-test-configmap-gpqf @ 07/29/23 13:48:19.702
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 13:48:19.702
  E0729 13:48:20.043388      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:21.043504      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:22.043981      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:23.044077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:24.044169      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:25.044463      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:26.044914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:27.045018      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:28.046041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:29.046997      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:30.047984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:31.048077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:32.048453      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:33.048539      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:34.049077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:35.049248      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:36.049716      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:37.050691      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:38.050744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:39.050979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:40.051796      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:41.051990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:42.052284      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:43.053285      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:48:43.759
  Jul 29 13:48:43.762: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-subpath-test-configmap-gpqf container test-container-subpath-configmap-gpqf: <nil>
  STEP: delete the pod @ 07/29/23 13:48:43.771
  STEP: Deleting pod pod-subpath-test-configmap-gpqf @ 07/29/23 13:48:43.786
  Jul 29 13:48:43.786: INFO: Deleting pod "pod-subpath-test-configmap-gpqf" in namespace "subpath-7488"
  Jul 29 13:48:43.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7488" for this suite. @ 07/29/23 13:48:43.793
• [24.129 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 07/29/23 13:48:43.801
  Jul 29 13:48:43.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename sched-pred @ 07/29/23 13:48:43.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:48:43.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:48:43.82
  Jul 29 13:48:43.822: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 29 13:48:43.830: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 13:48:43.834: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-19-67 before test
  Jul 29 13:48:43.838: INFO: nginx-ingress-controller-kubernetes-worker-gllr9 from ingress-nginx-kubernetes-worker started at 2023-07-29 12:05:42 +0000 UTC (1 container statuses recorded)
  Jul 29 13:48:43.838: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 29 13:48:43.838: INFO: calico-kube-controllers-66cf5c7c9b-lp29z from kube-system started at 2023-07-29 12:05:47 +0000 UTC (1 container statuses recorded)
  Jul 29 13:48:43.838: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  Jul 29 13:48:43.838: INFO: sonobuoy-e2e-job-fe057c2caff44e9c from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:48:43.838: INFO: 	Container e2e ready: true, restart count 0
  Jul 29 13:48:43.838: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:48:43.838: INFO: sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-7dv7m from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:48:43.838: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:48:43.838: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 13:48:43.838: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-33-37 before test
  Jul 29 13:48:43.842: INFO: nginx-ingress-controller-kubernetes-worker-zhwpz from ingress-nginx-kubernetes-worker started at 2023-07-29 13:34:13 +0000 UTC (1 container statuses recorded)
  Jul 29 13:48:43.842: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 29 13:48:43.842: INFO: sonobuoy from sonobuoy started at 2023-07-29 12:12:09 +0000 UTC (1 container statuses recorded)
  Jul 29 13:48:43.842: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 29 13:48:43.842: INFO: sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-fxbms from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:48:43.842: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:48:43.842: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 13:48:43.842: INFO: 
  Logging pods the apiserver thinks is on node ip-172-31-5-66 before test
  Jul 29 13:48:43.848: INFO: default-http-backend-kubernetes-worker-65fc475d49-fwzg8 from ingress-nginx-kubernetes-worker started at 2023-07-29 11:57:02 +0000 UTC (1 container statuses recorded)
  Jul 29 13:48:43.848: INFO: 	Container default-http-backend-kubernetes-worker ready: true, restart count 0
  Jul 29 13:48:43.848: INFO: nginx-ingress-controller-kubernetes-worker-hbv24 from ingress-nginx-kubernetes-worker started at 2023-07-29 11:57:02 +0000 UTC (1 container statuses recorded)
  Jul 29 13:48:43.848: INFO: 	Container nginx-ingress-controllerkubernetes-worker ready: true, restart count 0
  Jul 29 13:48:43.848: INFO: coredns-5c7f76ccb8-7mf4p from kube-system started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:48:43.848: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 13:48:43.848: INFO: kube-state-metrics-5b95b4459c-849m5 from kube-system started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:48:43.848: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jul 29 13:48:43.848: INFO: metrics-server-v0.5.2-6cf8c8b69c-slzlr from kube-system started at 2023-07-29 11:55:52 +0000 UTC (2 container statuses recorded)
  Jul 29 13:48:43.848: INFO: 	Container metrics-server ready: true, restart count 0
  Jul 29 13:48:43.848: INFO: 	Container metrics-server-nanny ready: true, restart count 0
  Jul 29 13:48:43.848: INFO: dashboard-metrics-scraper-6b8586b5c9-6s95z from kubernetes-dashboard started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:48:43.848: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  Jul 29 13:48:43.848: INFO: kubernetes-dashboard-6869f4cd5f-fwg5d from kubernetes-dashboard started at 2023-07-29 11:55:52 +0000 UTC (1 container statuses recorded)
  Jul 29 13:48:43.848: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  Jul 29 13:48:43.848: INFO: sonobuoy-systemd-logs-daemon-set-0fc3b568add345cc-n84sm from sonobuoy started at 2023-07-29 12:12:11 +0000 UTC (2 container statuses recorded)
  Jul 29 13:48:43.848: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 13:48:43.848: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/29/23 13:48:43.848
  E0729 13:48:44.054048      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:45.054239      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/29/23 13:48:45.865
  STEP: Trying to apply a random label on the found node. @ 07/29/23 13:48:45.878
  STEP: verifying the node has the label kubernetes.io/e2e-cb4108a8-080d-4372-9143-b95c38c01294 42 @ 07/29/23 13:48:45.887
  STEP: Trying to relaunch the pod, now with labels. @ 07/29/23 13:48:45.89
  E0729 13:48:46.054811      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:47.054908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-cb4108a8-080d-4372-9143-b95c38c01294 off the node ip-172-31-33-37 @ 07/29/23 13:48:47.909
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-cb4108a8-080d-4372-9143-b95c38c01294 @ 07/29/23 13:48:47.924
  Jul 29 13:48:47.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-328" for this suite. @ 07/29/23 13:48:47.936
• [4.143 seconds]
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 07/29/23 13:48:47.944
  Jul 29 13:48:47.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename init-container @ 07/29/23 13:48:47.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:48:47.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:48:47.968
  STEP: creating the pod @ 07/29/23 13:48:47.973
  Jul 29 13:48:47.973: INFO: PodSpec: initContainers in spec.initContainers
  E0729 13:48:48.055759      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:49.056744      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:50.057058      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:51.057141      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:52.057603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:53.057865      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:54.058651      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:55.058732      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:56.058914      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:57.059962      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:58.060052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:48:59.060210      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:00.061189      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:01.061281      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:02.061568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:03.061987      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:04.062196      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:05.063108      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:06.063967      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:07.064396      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:08.064477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:09.064652      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:10.065598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:11.065796      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:12.065973      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:13.066068      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:14.066166      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:15.066300      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:16.066590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:17.066678      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:18.067280      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:19.067979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:20.068066      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:21.068174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:22.068449      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:23.068521      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:24.068593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:25.068796      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:26.068934      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:27.069022      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:28.069203      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:49:28.143: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c55f942a-1dab-4909-9430-48ea14501bf3", GenerateName:"", Namespace:"init-container-8701", SelfLink:"", UID:"755d4f50-eeae-4a7e-9bcd-9143b1d098bf", ResourceVersion:"43291", Generation:0, CreationTimestamp:time.Date(2023, time.July, 29, 13, 48, 47, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"973644427"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 29, 13, 48, 47, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005014c30), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 29, 13, 49, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005014ca8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-4crjk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003a14ce0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4crjk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4crjk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-4crjk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003dde600), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-33-37", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000383730), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003dde690)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003dde6b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003dde6b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003dde6bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0040bbb50), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 13, 48, 47, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 13, 48, 47, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 13, 48, 47, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 13, 48, 47, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.33.37", PodIP:"192.168.129.91", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.129.91"}}, StartTime:time.Date(2023, time.July, 29, 13, 48, 47, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000383810)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000383880)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://3a2a49382a8e5b6d36557ff816fa2ba9bf28232682c895b20b806797faadc1db", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003a14d80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003a14d60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003dde73f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jul 29 13:49:28.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8701" for this suite. @ 07/29/23 13:49:28.147
• [40.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 07/29/23 13:49:28.156
  Jul 29 13:49:28.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:49:28.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:49:28.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:49:28.177
  STEP: Creating projection with secret that has name projected-secret-test-da5ee621-3b77-4e8a-a6fe-7445700fc7ec @ 07/29/23 13:49:28.18
  STEP: Creating a pod to test consume secrets @ 07/29/23 13:49:28.184
  E0729 13:49:29.069315      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:30.069535      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:31.069646      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:32.070682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:49:32.202
  Jul 29 13:49:32.206: INFO: Trying to get logs from node ip-172-31-19-67 pod pod-projected-secrets-ba3766a6-3589-4010-9e48-150f0ddca63f container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 13:49:32.221
  Jul 29 13:49:32.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7461" for this suite. @ 07/29/23 13:49:32.24
• [4.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 07/29/23 13:49:32.248
  Jul 29 13:49:32.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 13:49:32.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:49:32.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:49:32.266
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 13:49:32.269
  E0729 13:49:33.070780      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:34.070925      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:35.071975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:36.072085      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:49:36.289
  Jul 29 13:49:36.292: INFO: Trying to get logs from node ip-172-31-19-67 pod downwardapi-volume-9f060f97-d107-4cf2-90c0-6ebc245e5ae8 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 13:49:36.297
  Jul 29 13:49:36.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4429" for this suite. @ 07/29/23 13:49:36.318
• [4.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 07/29/23 13:49:36.33
  Jul 29 13:49:36.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 13:49:36.331
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:49:36.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:49:36.353
  STEP: Creating pod test-grpc-a8c3f043-d920-4eee-b5b1-adc947f62a76 in namespace container-probe-6803 @ 07/29/23 13:49:36.356
  E0729 13:49:37.072975      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:38.073053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:49:38.376: INFO: Started pod test-grpc-a8c3f043-d920-4eee-b5b1-adc947f62a76 in namespace container-probe-6803
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 13:49:38.376
  Jul 29 13:49:38.379: INFO: Initial restart count of pod test-grpc-a8c3f043-d920-4eee-b5b1-adc947f62a76 is 0
  E0729 13:49:39.073905      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:40.074063      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:41.074369      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:42.074466      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:43.074537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:44.074625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:45.075452      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:46.076334      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:47.076912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:48.077011      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:49.077086      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:50.077993      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:51.078099      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:52.078350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:53.078923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:54.079029      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:55.079250      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:56.079580      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:57.079960      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:58.079999      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:49:59.080923      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:00.081111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:01.081209      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:02.081493      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:03.081730      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:04.081828      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:05.082598      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:06.083357      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:07.083602      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:08.083707      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:09.083966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:10.084056      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:11.084773      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:12.084866      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:13.084954      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:14.085042      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:15.085232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:16.085670      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:17.085979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:18.086074      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:19.086240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:20.086329      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:21.086898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:22.087150      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:23.087960      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:24.088149      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:25.088233      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:26.088339      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:27.088392      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:28.088561      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:29.088822      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:30.089047      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:31.089135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:32.089420      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:33.089682      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:34.089929      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:35.090430      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:36.090601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:37.091594      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:38.092225      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:39.092795      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:40.092986      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:41.093788      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:42.094049      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:43.094219      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:44.095293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:45.095377      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:46.095489      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:47.095963      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:48.097055      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:49.097135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:50.097430      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:51.097668      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:52.097877      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:53.098293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:54.098387      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:55.098477      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:56.098680      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:57.098909      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:58.099978      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:50:59.100720      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:00.100915      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:01.101861      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:02.101946      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:03.102641      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:04.102868      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:05.102915      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:06.103055      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:07.103324      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:08.103521      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:09.103618      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:10.104459      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:11.104625      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:12.104901      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:13.104982      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:14.105057      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:15.105906      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:16.106041      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:17.106333      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:18.106423      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:19.107375      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:20.108240      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:21.108941      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:22.109270      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:23.109979      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:24.110145      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:25.111212      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:26.112051      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:27.112940      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:28.113030      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:29.113121      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:30.113319      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:31.114078      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:32.114159      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:33.114977      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:34.115072      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:35.115793      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:36.116183      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:37.116984      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:38.117165      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:39.117593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:40.117760      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:41.118767      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:42.118900      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:43.119964      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:44.120216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:45.120570      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:46.121175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:47.121423      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:48.121566      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:49.122306      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:50.122482      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:51.123207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:52.124023      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:53.124614      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:54.124701      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:55.124869      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:56.125176      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:57.125456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:58.125638      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:51:59.125889      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:00.126146      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:01.126696      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:02.127410      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:03.127962      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:04.128139      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:05.128316      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:06.129174      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:07.129558      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:08.130105      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:09.130581      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:10.130771      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:11.131288      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:12.131966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:13.132680      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:14.132853      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:15.133808      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:16.134184      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:17.134759      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:18.134905      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:19.135745      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:20.135820      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:21.135950      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:22.136118      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:23.136534      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:24.137097      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:25.137834      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:26.138156      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:27.138966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:28.139960      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:29.140278      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:30.140384      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:31.140826      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:32.140917      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:33.141436      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:34.141600      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:35.142469      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:36.143163      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:37.143858      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:38.144781      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:39.144932      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:40.145114      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:41.145296      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:42.145496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:43.146142      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:44.146228      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:45.146697      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:46.147442      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:47.147577      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:48.148037      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:49.149102      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:50.149322      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:51.149898      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:52.150083      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:53.150640      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:54.150809      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:55.150903      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:56.151167      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:57.151958      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:58.152052      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:52:59.153042      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:00.153126      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:01.154059      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:02.154153      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:03.154241      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:04.154466      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:05.154553      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:06.155172      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:07.155796      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:08.156349      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:09.157080      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:10.157179      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:11.157921      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:12.157992      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:13.159077      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:14.159968      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:15.160721      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:16.161170      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:17.161732      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:18.161884      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:19.162510      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:20.162704      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:21.163371      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:22.163461      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:23.163964      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:24.164120      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:25.164204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:26.165195      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:27.165927      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:28.166091      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:29.166812      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:30.166904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:31.167245      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:32.167342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:33.168350      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:34.168601      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:35.169231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:36.170197      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:37.171122      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:38.171216      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:53:38.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 13:53:38.894
  STEP: Destroying namespace "container-probe-6803" for this suite. @ 07/29/23 13:53:38.907
• [242.585 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 07/29/23 13:53:38.92
  Jul 29 13:53:38.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 13:53:38.92
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:53:38.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:53:38.938
  STEP: Creating configMap with name configmap-test-volume-map-b2fa22b6-b508-4cc2-8cda-e382415b05fc @ 07/29/23 13:53:38.941
  STEP: Creating a pod to test consume configMaps @ 07/29/23 13:53:38.945
  E0729 13:53:39.171623      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:40.171783      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:41.171740      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:42.171848      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:53:42.969
  Jul 29 13:53:42.971: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-configmaps-0526c5ac-9495-4761-b12f-a34c96f57875 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 13:53:42.99
  Jul 29 13:53:43.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3374" for this suite. @ 07/29/23 13:53:43.011
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 07/29/23 13:53:43.02
  Jul 29 13:53:43.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 13:53:43.021
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:53:43.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:53:43.041
  STEP: Creating replication controller my-hostname-basic-9f378917-f280-4fcb-b876-8c10e04aa8a7 @ 07/29/23 13:53:43.043
  Jul 29 13:53:43.052: INFO: Pod name my-hostname-basic-9f378917-f280-4fcb-b876-8c10e04aa8a7: Found 0 pods out of 1
  E0729 13:53:43.172656      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:44.172772      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:45.172880      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:46.172904      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:47.172989      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:53:48.056: INFO: Pod name my-hostname-basic-9f378917-f280-4fcb-b876-8c10e04aa8a7: Found 1 pods out of 1
  Jul 29 13:53:48.056: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9f378917-f280-4fcb-b876-8c10e04aa8a7" are running
  Jul 29 13:53:48.059: INFO: Pod "my-hostname-basic-9f378917-f280-4fcb-b876-8c10e04aa8a7-gwwnv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 13:53:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 13:53:44 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 13:53:44 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 13:53:43 +0000 UTC Reason: Message:}])
  Jul 29 13:53:48.059: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/29/23 13:53:48.059
  Jul 29 13:53:48.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4766" for this suite. @ 07/29/23 13:53:48.073
• [5.060 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 07/29/23 13:53:48.08
  Jul 29 13:53:48.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename subjectreview @ 07/29/23 13:53:48.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:53:48.096
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:53:48.099
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-1526" @ 07/29/23 13:53:48.101
  Jul 29 13:53:48.105: INFO: saUsername: "system:serviceaccount:subjectreview-1526:e2e"
  Jul 29 13:53:48.105: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-1526"}
  Jul 29 13:53:48.105: INFO: saUID: "b33c5c3d-392a-4dc7-9d15-c9eaa37ab39d"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-1526:e2e" @ 07/29/23 13:53:48.105
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-1526:e2e" @ 07/29/23 13:53:48.106
  Jul 29 13:53:48.107: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-1526:e2e" api 'list' configmaps in "subjectreview-1526" namespace @ 07/29/23 13:53:48.107
  Jul 29 13:53:48.108: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-1526:e2e" @ 07/29/23 13:53:48.108
  Jul 29 13:53:48.111: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jul 29 13:53:48.111: INFO: LocalSubjectAccessReview has been verified
  Jul 29 13:53:48.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-1526" for this suite. @ 07/29/23 13:53:48.115
• [0.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 07/29/23 13:53:48.124
  Jul 29 13:53:48.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 13:53:48.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:53:48.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:53:48.146
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/29/23 13:53:48.148
  E0729 13:53:48.173393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:49.173537      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:50.174028      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:51.174784      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:53:52.169
  Jul 29 13:53:52.172: INFO: Trying to get logs from node ip-172-31-33-37 pod pod-24cb7a3e-07c4-449f-989a-9e0243594b1d container test-container: <nil>
  E0729 13:53:52.175607      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 07/29/23 13:53:52.178
  Jul 29 13:53:52.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8160" for this suite. @ 07/29/23 13:53:52.202
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 07/29/23 13:53:52.211
  Jul 29 13:53:52.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 13:53:52.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:53:52.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:53:52.23
  Jul 29 13:53:52.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  E0729 13:53:53.175718      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:54.175808      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:55.176764      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:53:55.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5636" for this suite. @ 07/29/23 13:53:55.483
• [3.282 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 07/29/23 13:53:55.495
  Jul 29 13:53:55.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:53:55.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:53:55.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:53:55.518
  STEP: Creating configMap with name configmap-projected-all-test-volume-b3bbe297-8b3b-4832-bf28-80934b878ec5 @ 07/29/23 13:53:55.522
  STEP: Creating secret with name secret-projected-all-test-volume-4e1ab8fd-3a4e-4070-922d-8d094ae0535d @ 07/29/23 13:53:55.532
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 07/29/23 13:53:55.536
  E0729 13:53:56.177222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:57.177309      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:58.177845      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:53:59.177938      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:53:59.561
  Jul 29 13:53:59.565: INFO: Trying to get logs from node ip-172-31-33-37 pod projected-volume-7c33658f-0e24-494c-9d0e-9d35d92cdd1c container projected-all-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 13:53:59.572
  Jul 29 13:53:59.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4031" for this suite. @ 07/29/23 13:53:59.597
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 07/29/23 13:53:59.608
  Jul 29 13:53:59.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 13:53:59.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:53:59.628
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:53:59.632
  Jul 29 13:53:59.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  E0729 13:54:00.178488      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:01.178912      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:02.178939      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0729 13:54:02.227613      18 warnings.go:70] unknown field "alpha"
  W0729 13:54:02.227632      18 warnings.go:70] unknown field "beta"
  W0729 13:54:02.227638      18 warnings.go:70] unknown field "delta"
  W0729 13:54:02.227644      18 warnings.go:70] unknown field "epsilon"
  W0729 13:54:02.227651      18 warnings.go:70] unknown field "gamma"
  Jul 29 13:54:02.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7629" for this suite. @ 07/29/23 13:54:02.776
• [3.172 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 07/29/23 13:54:02.782
  Jul 29 13:54:02.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 13:54:02.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:54:02.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:54:02.804
  STEP: Creating a ReplaceConcurrent cronjob @ 07/29/23 13:54:02.807
  STEP: Ensuring a job is scheduled @ 07/29/23 13:54:02.813
  E0729 13:54:03.179027      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:04.179969      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:05.180798      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:06.181204      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:07.181585      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:08.182236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:09.182232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:10.182417      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:11.183071      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:12.183962      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:13.184663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:14.184833      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:15.185451      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:16.186206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:17.186888      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:18.186911      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:19.187010      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:20.187966      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:21.188673      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:22.188842      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:23.189442      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:24.190227      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:25.190892      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:26.191224      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:27.191802      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:28.191958      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:29.192222      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:30.192412      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:31.192733      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:32.192884      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:33.193876      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:34.194894      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:35.195965      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:36.196207      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:37.196496      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:38.197269      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:39.197808      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:40.198117      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:41.198688      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:42.198783      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:43.198897      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:44.199962      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:45.200158      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:46.201231      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:47.202158      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:48.202341      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:49.203232      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:50.203325      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:51.203908      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:52.204064      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:53.204731      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:54.204844      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:55.205724      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:56.206250      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:57.206847      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:58.206919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:54:59.207928      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:00.208015      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 07/29/23 13:55:00.817
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/29/23 13:55:00.82
  STEP: Ensuring the job is replaced with a new one @ 07/29/23 13:55:00.824
  E0729 13:55:01.208051      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:02.208229      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:03.209141      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:04.209236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:05.209769      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:06.210206      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:07.210663      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:08.210870      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:09.211588      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:10.211990      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:11.212723      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:12.213690      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:13.214393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:14.214486      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:15.215134      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:16.215208      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:17.215972      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:18.216393      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:19.216458      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:20.217456      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:21.217948      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:22.218053      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:23.218702      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:24.219470      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:25.219557      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:26.220259      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:27.220822      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:28.221287      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:29.221950      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:30.222111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:31.223019      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:32.223950      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:33.224593      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:34.224803      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:35.225457      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:36.226263      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:37.226342      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:38.227154      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:39.227968      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:40.228076      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:41.228323      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:42.228430      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:43.229407      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:44.229505      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:45.229590      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:46.230236      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:47.230775      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:48.231135      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:49.231881      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:50.232944      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:51.233292      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:52.234131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:53.234919      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:54.235001      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:55.235103      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:56.235533      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:57.236272      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:58.237286      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:55:59.237526      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:00.237612      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 07/29/23 13:56:00.829
  Jul 29 13:56:00.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-591" for this suite. @ 07/29/23 13:56:00.841
• [118.066 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 07/29/23 13:56:00.849
  Jul 29 13:56:00.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:56:00.85
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:56:00.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:56:00.869
  STEP: Creating projection with secret that has name projected-secret-test-map-c1efe30c-affe-4426-9ffb-1bb911e545b5 @ 07/29/23 13:56:00.877
  STEP: Creating a pod to test consume secrets @ 07/29/23 13:56:00.883
  E0729 13:56:01.238662      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:02.238928      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:03.239698      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:04.239780      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:56:04.9
  Jul 29 13:56:04.903: INFO: Trying to get logs from node ip-172-31-19-67 pod pod-projected-secrets-b2e13e26-d871-421c-8ef1-4fe1e03506ad container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 13:56:04.922
  Jul 29 13:56:04.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7450" for this suite. @ 07/29/23 13:56:04.942
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 07/29/23 13:56:04.949
  Jul 29 13:56:04.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 13:56:04.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:56:04.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:56:04.97
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/29/23 13:56:04.973
  E0729 13:56:05.240568      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:06.241293      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:07.241549      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:08.241621      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:56:08.992
  Jul 29 13:56:08.995: INFO: Trying to get logs from node ip-172-31-19-67 pod pod-690ad1aa-7b22-4af2-acc0-9e7bac4bc449 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 13:56:09.002
  Jul 29 13:56:09.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5807" for this suite. @ 07/29/23 13:56:09.022
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 07/29/23 13:56:09.032
  Jul 29 13:56:09.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename projected @ 07/29/23 13:56:09.033
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:56:09.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:56:09.053
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 13:56:09.056
  E0729 13:56:09.241999      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:10.242626      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:11.243148      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:12.243175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 13:56:13.079
  Jul 29 13:56:13.083: INFO: Trying to get logs from node ip-172-31-19-67 pod downwardapi-volume-811ecfdd-9887-4038-961d-4e4815c27702 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 13:56:13.09
  Jul 29 13:56:13.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7357" for this suite. @ 07/29/23 13:56:13.11
• [4.084 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 07/29/23 13:56:13.117
  Jul 29 13:56:13.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 13:56:13.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:56:13.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:56:13.137
  STEP: creating all guestbook components @ 07/29/23 13:56:13.141
  Jul 29 13:56:13.141: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jul 29 13:56:13.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8831 create -f -'
  E0729 13:56:13.244131      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:56:13.593: INFO: stderr: ""
  Jul 29 13:56:13.593: INFO: stdout: "service/agnhost-replica created\n"
  Jul 29 13:56:13.593: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jul 29 13:56:13.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8831 create -f -'
  Jul 29 13:56:13.958: INFO: stderr: ""
  Jul 29 13:56:13.958: INFO: stdout: "service/agnhost-primary created\n"
  Jul 29 13:56:13.958: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jul 29 13:56:13.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8831 create -f -'
  E0729 13:56:14.244476      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:56:14.312: INFO: stderr: ""
  Jul 29 13:56:14.312: INFO: stdout: "service/frontend created\n"
  Jul 29 13:56:14.312: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jul 29 13:56:14.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8831 create -f -'
  Jul 29 13:56:14.634: INFO: stderr: ""
  Jul 29 13:56:14.634: INFO: stdout: "deployment.apps/frontend created\n"
  Jul 29 13:56:14.634: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul 29 13:56:14.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8831 create -f -'
  Jul 29 13:56:14.961: INFO: stderr: ""
  Jul 29 13:56:14.961: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jul 29 13:56:14.961: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul 29 13:56:14.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8831 create -f -'
  Jul 29 13:56:15.233: INFO: stderr: ""
  Jul 29 13:56:15.233: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 07/29/23 13:56:15.233
  Jul 29 13:56:15.233: INFO: Waiting for all frontend pods to be Running.
  E0729 13:56:15.245491      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:16.246089      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:17.246175      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:18.247136      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:19.247247      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:20.247986      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:56:20.285: INFO: Waiting for frontend to serve content.
  Jul 29 13:56:20.296: INFO: Trying to add a new entry to the guestbook.
  Jul 29 13:56:20.309: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 07/29/23 13:56:20.318
  Jul 29 13:56:20.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8831 delete --grace-period=0 --force -f -'
  Jul 29 13:56:20.391: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 13:56:20.391: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 13:56:20.391
  Jul 29 13:56:20.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8831 delete --grace-period=0 --force -f -'
  Jul 29 13:56:20.467: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 13:56:20.467: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 13:56:20.467
  Jul 29 13:56:20.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8831 delete --grace-period=0 --force -f -'
  Jul 29 13:56:20.539: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 13:56:20.539: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 13:56:20.54
  Jul 29 13:56:20.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8831 delete --grace-period=0 --force -f -'
  Jul 29 13:56:20.602: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 13:56:20.602: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 13:56:20.602
  Jul 29 13:56:20.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8831 delete --grace-period=0 --force -f -'
  Jul 29 13:56:20.692: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 13:56:20.692: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 13:56:20.693
  Jul 29 13:56:20.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-8831 delete --grace-period=0 --force -f -'
  Jul 29 13:56:20.762: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 13:56:20.762: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jul 29 13:56:20.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8831" for this suite. @ 07/29/23 13:56:20.768
• [7.657 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 07/29/23 13:56:20.774
  Jul 29 13:56:20.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename watch @ 07/29/23 13:56:20.775
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:56:20.798
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:56:20.801
  STEP: creating a watch on configmaps with a certain label @ 07/29/23 13:56:20.805
  STEP: creating a new configmap @ 07/29/23 13:56:20.806
  STEP: modifying the configmap once @ 07/29/23 13:56:20.81
  STEP: changing the label value of the configmap @ 07/29/23 13:56:20.816
  STEP: Expecting to observe a delete notification for the watched object @ 07/29/23 13:56:20.824
  Jul 29 13:56:20.824: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9159  d22a3fed-a1ee-4834-8fec-c5fd7d463f53 44765 0 2023-07-29 13:56:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 13:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:56:20.825: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9159  d22a3fed-a1ee-4834-8fec-c5fd7d463f53 44766 0 2023-07-29 13:56:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 13:56:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:56:20.825: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9159  d22a3fed-a1ee-4834-8fec-c5fd7d463f53 44767 0 2023-07-29 13:56:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 13:56:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 07/29/23 13:56:20.825
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 07/29/23 13:56:20.833
  E0729 13:56:21.248368      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:22.248395      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:23.248587      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:24.248767      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:25.248935      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:26.249308      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:27.249405      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:28.249603      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:29.249686      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:30.250633      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 07/29/23 13:56:30.834
  STEP: modifying the configmap a third time @ 07/29/23 13:56:30.843
  STEP: deleting the configmap @ 07/29/23 13:56:30.851
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 07/29/23 13:56:30.857
  Jul 29 13:56:30.857: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9159  d22a3fed-a1ee-4834-8fec-c5fd7d463f53 44879 0 2023-07-29 13:56:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 13:56:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:56:30.857: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9159  d22a3fed-a1ee-4834-8fec-c5fd7d463f53 44880 0 2023-07-29 13:56:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 13:56:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:56:30.857: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9159  d22a3fed-a1ee-4834-8fec-c5fd7d463f53 44881 0 2023-07-29 13:56:20 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 13:56:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 13:56:30.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9159" for this suite. @ 07/29/23 13:56:30.862
• [10.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 07/29/23 13:56:30.871
  Jul 29 13:56:30.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename endpointslice @ 07/29/23 13:56:30.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:56:30.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:56:30.893
  STEP: getting /apis @ 07/29/23 13:56:30.895
  STEP: getting /apis/discovery.k8s.io @ 07/29/23 13:56:30.899
  STEP: getting /apis/discovery.k8s.iov1 @ 07/29/23 13:56:30.9
  STEP: creating @ 07/29/23 13:56:30.901
  STEP: getting @ 07/29/23 13:56:30.917
  STEP: listing @ 07/29/23 13:56:30.919
  STEP: watching @ 07/29/23 13:56:30.922
  Jul 29 13:56:30.922: INFO: starting watch
  STEP: cluster-wide listing @ 07/29/23 13:56:30.923
  STEP: cluster-wide watching @ 07/29/23 13:56:30.928
  Jul 29 13:56:30.928: INFO: starting watch
  STEP: patching @ 07/29/23 13:56:30.929
  STEP: updating @ 07/29/23 13:56:30.935
  Jul 29 13:56:30.943: INFO: waiting for watch events with expected annotations
  Jul 29 13:56:30.943: INFO: saw patched and updated annotations
  STEP: deleting @ 07/29/23 13:56:30.944
  STEP: deleting a collection @ 07/29/23 13:56:30.956
  Jul 29 13:56:30.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6437" for this suite. @ 07/29/23 13:56:30.975
• [0.111 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 07/29/23 13:56:30.983
  Jul 29 13:56:30.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename configmap @ 07/29/23 13:56:30.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:56:31.002
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:56:31.005
  STEP: creating a ConfigMap @ 07/29/23 13:56:31.009
  STEP: fetching the ConfigMap @ 07/29/23 13:56:31.013
  STEP: patching the ConfigMap @ 07/29/23 13:56:31.016
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 07/29/23 13:56:31.022
  STEP: deleting the ConfigMap by collection with a label selector @ 07/29/23 13:56:31.026
  STEP: listing all ConfigMaps in test namespace @ 07/29/23 13:56:31.034
  Jul 29 13:56:31.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5446" for this suite. @ 07/29/23 13:56:31.042
• [0.067 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 07/29/23 13:56:31.052
  Jul 29 13:56:31.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 13:56:31.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:56:31.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:56:31.073
  STEP: set up a multi version CRD @ 07/29/23 13:56:31.077
  Jul 29 13:56:31.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  E0729 13:56:31.251288      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:32.251578      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:33.252501      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:34.253111      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 07/29/23 13:56:34.513
  STEP: check the new version name is served @ 07/29/23 13:56:34.526
  E0729 13:56:35.253180      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 07/29/23 13:56:35.74
  E0729 13:56:36.253426      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 07/29/23 13:56:36.482
  E0729 13:56:37.253791      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:38.254599      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:56:39.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8397" for this suite. @ 07/29/23 13:56:39.124
• [8.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 07/29/23 13:56:39.134
  Jul 29 13:56:39.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename endpointslice @ 07/29/23 13:56:39.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:56:39.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:56:39.153
  E0729 13:56:39.255201      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 13:56:40.255976      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:56:41.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6851" for this suite. @ 07/29/23 13:56:41.202
• [2.075 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 07/29/23 13:56:41.209
  Jul 29 13:56:41.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3686456206
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 13:56:41.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 13:56:41.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 13:56:41.231
  Jul 29 13:56:41.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3686456206 --namespace=kubectl-2916 version'
  E0729 13:56:41.256230      18 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 13:56:41.289: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jul 29 13:56:41.289: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:20:54Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-20T02:05:23Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jul 29 13:56:41.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2916" for this suite. @ 07/29/23 13:56:41.293
• [0.091 seconds]
------------------------------
SSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jul 29 13:56:41.301: INFO: Running AfterSuite actions on node 1
  Jul 29 13:56:41.301: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.043 seconds]
------------------------------

Ran 378 of 7207 Specs in 6254.962 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h44m15.317294323s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

